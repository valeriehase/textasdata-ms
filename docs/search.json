[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Automatisierte Inhaltsanalyse",
    "section": "",
    "text": "Quelle: Foto von AltumCode auf Unsplash"
  },
  {
    "objectID": "index.html#infos-zum-workshop",
    "href": "index.html#infos-zum-workshop",
    "title": "Automatisierte Inhaltsanalyse",
    "section": "Infos zum Workshop",
    "text": "Infos zum Workshop\n\nMethodenworkshop am Institut f√ºr Kommunikationswissenschaft, Universit√§t M√ºnster\nüìÖ 24-25. Juli 2024\nWorkshop-Leitung:\n\nValerie Hase (Ludwig-Maximilians-Universit√§t M√ºnchen). Mehr Infos: github.com/valeriehase & valerie-hase.com\nUnterst√ºtzung durch Teaching Assistant Luisa Kutlar (Ludwig-Maximilians-Universit√§t M√ºnchen). Mehr Infos: github.com/luisakutlar"
  },
  {
    "objectID": "index.html#materialien",
    "href": "index.html#materialien",
    "title": "Automatisierte Inhaltsanalyse",
    "section": "Materialien",
    "text": "Materialien\n\nDaten\n\nDatensatz 1: IMDb Top-Rated TV Series Dataset. Verf√ºgbar unter MIT Lizenz via Kaggle.\n\n\n\n Hier geht es zum Download des TV Datasets\n\n\n\n\nFolien & R-Code\nSitzung 1Ô∏è‚É£: Einf√ºhrung in die automatisierte Inhaltsanalyse & Preprocessing\n\nFolien\nTutorial\n\nSitzung 2Ô∏è‚É£: Co-Occurence-Analysen\n\nFolien\nTutorial\n\nSitzung 3Ô∏è‚É£: Diktion√§re\n\nFolien\nTutorial\n\nSitzung 4Ô∏è‚É£: Topic Modeling\n\nFolien\nTutorial\n\nSitzung 5Ô∏è‚É£: Qualit√§tskriterien\n\nFolien\nTutorial\n\nSitzung 6Ô∏è‚É£: Ausblick\n\nFolien\nTutorial"
  },
  {
    "objectID": "index.html#weiterf√ºhrende-tutorials",
    "href": "index.html#weiterf√ºhrende-tutorials",
    "title": "Automatisierte Inhaltsanalyse",
    "section": "Weiterf√ºhrende Tutorials",
    "text": "Weiterf√ºhrende Tutorials\n\nBail, C. Day 3: Automated Text Analysis. Link\nBernauer J, & Traber D. Quantitative Analysis of Political Text. Link\nHase, V. (2022). Text as Data Methods in R. Link\nHase, V. (2023). Advanced Text Analysis. Link\nSanchez, G. (2014). Handling Strings with R. Link\nSilge, J., & Robinson, D. Text mining with R: A tidy approach. Link\nPuschmann, C., & Haim, R. Automated Content Analysis with R. Link\nUnkel, J. (2020). Methodische Vertiefung: Computational Methods mit R und R Studio. Link\nWatanabe, K., & M√ºller, S (2023). Quanteda Tutorials. Link"
  },
  {
    "objectID": "index.html#weiterf√ºhrende-literatur",
    "href": "index.html#weiterf√ºhrende-literatur",
    "title": "Automatisierte Inhaltsanalyse",
    "section": "Weiterf√ºhrende Literatur",
    "text": "Weiterf√ºhrende Literatur\n\nBaden, C., Pipal, C., Schoonvelde, M., & Van Der Velden, M. A. C. G. (2022). Three Gaps in Computational Text Analysis Methods for Social Sciences: A Research Agenda. Communication Methods and Measures, 16(1), 1‚Äì18. https://doi.org/10.1080/19312458.2021.2015574\nBenoit, K. (2019). Text as data: An overview. In Cuirini, L., & Franzese, R. (Eds.), Handbook of Research Methods in Political Science and International Relations. Preprint\nBoumans, J. W., & Trilling, D. (2016). Taking Stock of the Toolkit: An overview of relevant automated content analysis approaches and techniques for digital journalism scholars. Digital Journalism, 4(1), 8‚Äì23. https://doi.org/10.1080/21670811.2015.1096598\nGrimmer, J., Roberts, M. E., & Stewart, B. M. (2022). Text as data: A new framework for machine learning and the social sciences. Princeton University Press.\nG√ºnther, E., & Quandt, T. (2016). Word Counts and Topic Models: Automated text analysis methods for digital journalism research. Digital Journalism, 4(1), 75‚Äì88. https://doi.org/10.1080/21670811.2015.1093270\nHaim, M. (2023). Computational Communication Science: Eine Einf√ºhrung. Springer Fachmedien Wiesbaden. https://doi.org/10.1007/978-3-658-40171-9\nHase, V. (2023). Automated Content Analysis. In F. Oehmer, S. H. Kessler, E. Humprecht, K. Sommer, & L. Castro Herrero (eds.),¬†Handbook of Standardized Content Analysis: Applied Designs to Research Fields of Communication Science. VS Springer (pp.¬†23‚Äì36).¬†https://doi.org/10.1007/978-3-658-36179-2_3\nJ√ºnger, J., & G√§rtner, C. (2023). Computational Methods f√ºr die Sozial- und Geisteswissenschaften. Springer Fachmedien Wiesbaden. https://doi.org/10.1007/978-3-658-37747-2\nQuinn, K. M., Monroe, B. L., Colaresi, M., Crespin, M. H., & Radev, D. R. (2010). How to Analyze Political Attention with Minimal Assumptions and Costs. American Journal of Political Science, 54(1), 209‚Äì228. https://doi.org/10.1111/j.1540-5907.2009.00427.x\nAtteveldt, W. van, Trilling, D., & Arc√≠la Calder√≥n, C. (2022). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in Python and R. Wiley Blackwell.\nWilkerson, J., & Casas, A. (2017). Large-Scale Computerized Text Analysis in Political Science: Opportunities and Challenges. Annual Review of Political Science, 20(1), 529‚Äì544. https://www.annualreviews.org/doi/10.1146/annurev-polisci-052615-025542"
  },
  {
    "objectID": "01-begr√ºssung.html",
    "href": "01-begr√ºssung.html",
    "title": "\nAutomatisierte Inhaltsanalyse\n",
    "section": "",
    "text": "Automatisierte Inhaltsanalyse\nSitzung 1Ô∏è‚É£: Begr√º√üung\nLeitung: Valerie Hase (Ludwig-Maximilians-Universit√§t M√ºnchen)\nüëâ github.com/valeriehase & valerie-hase.com\nTeaching Assistant: Luisa Kutlar (Ludwig-Maximilians-Universit√§t M√ºnchen)\nüëâ github.com/luisakutlar"
  },
  {
    "objectID": "01-begr√ºssung.html#wer-seid-ihr",
    "href": "01-begr√ºssung.html#wer-seid-ihr",
    "title": "Automatisierte Inhaltsanalyse_ Workshop",
    "section": "Wer seid ihr?",
    "text": "Wer seid ihr?\nBitte die Hand heben ü§ö, wenn ihr ‚Ä¶.\n\n\nmit automatisierter Inhaltsanalyse gearbeitet habt\nR regelm√§√üig nutzt\nandere Programmiersprachen (z. B. Python) regelm√§√üig nutzt"
  },
  {
    "objectID": "01-begr√ºssung.html#wer-sind-wir",
    "href": "01-begr√ºssung.html#wer-sind-wir",
    "title": "Automatisierte Inhaltsanalyse_ Workshop",
    "section": "Wer sind wir?",
    "text": "Wer sind wir?"
  },
  {
    "objectID": "01-begr√ºssung.html#vielen-dank-f√ºr-die-organisation",
    "href": "01-begr√ºssung.html#vielen-dank-f√ºr-die-organisation",
    "title": "Automatisierte Inhaltsanalyse_ Workshop",
    "section": "Vielen Dank f√ºr die Organisation üôå",
    "text": "Vielen Dank f√ºr die Organisation üôå\nShoutout an: Mittelbaunetzwerk Journalismusforschung & Mittelbaunetzwerk Wissenschaftskommunikation\n\nJulia Metag\nFranca Singh\nJakon J√ºnger"
  },
  {
    "objectID": "01-begr√ºssung.html#worum-geht-es-heute",
    "href": "01-begr√ºssung.html#worum-geht-es-heute",
    "title": "Automatisierte Inhaltsanalyse_ Workshop",
    "section": "Worum geht es heute?",
    "text": "Worum geht es heute?\n\n\n‚úÖ A\n‚ùå D"
  },
  {
    "objectID": "01-begr√ºssung.html#ablauf",
    "href": "01-begr√ºssung.html#ablauf",
    "title": "Automatisierte Inhaltsanalyse_ Workshop",
    "section": "Ablauf",
    "text": "Ablauf\n\nFragen? ü§î\n\n\n\n\nAutomatisierte Inhaltsanalyse - Workshop Universit√§t M√ºnster, Juli 2024"
  },
  {
    "objectID": "01-begr√ºssung.html#section",
    "href": "01-begr√ºssung.html#section",
    "title": "Automatisierte Inhaltsanalyse_ Workshop",
    "section": "",
    "text": "Fragen? ü§î\n\n\n\n\nAutomatisierte Inhaltsanalyse - Workshop Universit√§t M√ºnster, Juli 2024"
  },
  {
    "objectID": "tutorial/sitzung2.html",
    "href": "tutorial/sitzung2.html",
    "title": "Sitzung 2",
    "section": "",
    "text": "Paket ‚Äúreadtext‚Äù installieren\nWenn ihr beim Workshop kein Internet habt, k√∂nnt ihr das Paket auch weglassen.\n\ninstall.packages(\"readtext\")\n\n\n\nPakete ‚Äúaktivieren‚Äù\nAls n√§chstes laden wir √ºber die folgenden Befehle die Pakete, mit denen wir Daten einlesen und bereinigen werden.\n\nlibrary(\"readtext\")"
  },
  {
    "objectID": "tutorial/Sitzung2_EinlesenBereinigung.html",
    "href": "tutorial/Sitzung2_EinlesenBereinigung.html",
    "title": "Session 2: Einlesen & Bereinigen von Text",
    "section": "",
    "text": "Paket ‚Äúreadtext‚Äù installieren\nWenn ihr beim Workshop kein Internet habt, k√∂nnt ihr das Paket auch weglassen.\n\ninstall.packages(\"readtext\")\n\n\n\nPakete ‚Äúaktivieren‚Äù\nAls n√§chstes laden wir √ºber die folgenden Befehle die Pakete, mit denen wir Daten einlesen und bereinigen werden.\n\nlibrary(\"readtext\")"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html",
    "href": "tutorial/02-einlesen-bereinigen.html",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "",
    "text": "Zun√§chst installieren und laden wir alle Packete, die wir heute und morgen brauchen.\n\n#install.packages(\"RCurl\")\n#install.packages(\"quanteda\")\n#install.packages(\"tidyverse)\n#install.packages(\"dplyr\")\n#install.packages(\"quanteda.textplots\")\n#install.packages(\"quanteda.textstats\")\n#install.packages(\"udpipe\")\n\nlibrary(\"RCurl\")\nlibrary(\"quanteda\")\nlibrary(\"tidyverse\")\nlibrary(\"dplyr\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"quanteda.textstats\")\nlibrary(\"udpipe\")\n\nDann laden wir die Datei die hinter dem Link liegt mit der Funktion getURL() aus dem package Rcurl herunter. Mit einem Blick in das Environment sehen wir, dass die einzelnen W√∂rter mit einem ; getrennt werden . Daher brauchen wir die Funktion read.csv2() aus dem utils Packet - das Packet ist vorinstalliert und immer geladen - um die Daten in R einzulesen. Der Datensatz wird im Objekt daten_df gespeichert.\n\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv\")\ndaten_df &lt;-  read.csv2(text = url)\n\nAlternativ k√∂nnen Daten in einer .csv Datei auch mit einem , voneinander abgetrennt sein. Hier br√§uchte es dann die Funktion read.csv() zum Einlesen.\nNach dem Einlesen der Daten ist es √ºblich sich zun√§chst einen √úberblick √ºber die Daten zu verschaffen und zu kontrollieren, ob alles korrekt eingelesen wurde.\n\nhead(daten_df)\n\n                Title      Year Parental.Rating Rating Number.of.Votes\n1  1. Game of Thrones 2011‚Äì2019           TV-MA    9.2            2.3M\n2     2. Breaking Bad 2008‚Äì2013           TV-MA    9.5            2.1M\n3  3. Stranger Things 2016‚Äì2025           TV-14    8.7            1.3M\n4          4. Friends 1994‚Äì2004           TV-14    8.9            1.1M\n5 5. The Walking Dead 2010‚Äì2022           TV-MA    8.1            1.1M\n6         6. Sherlock 2010‚Äì2017           TV-14    9.1              1M\n                                                                                                                                                                                                                                    Description\n1                                                                                                           Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\n2                                                                    A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine with a former student in order to secure his family's future.\n3                                                                                          When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatural forces and one strange little girl.\n4                                                                                                        Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan borough of New York City.\n5                                                                                                              Sheriff Deputy Rick Grimes wakes up from a coma to learn the world is in ruins and must lead a group of survivors to stay alive.\n6 The quirky spin on Conan Doyle's iconic sleuth pitches him as a \"high-functioning sociopath\" in modern-day London. Assisting him in his investigations: Afghanistan War vet John Watson, who's introduced to Holmes by a mutual acquaintance.\n\nstr(daten_df)\n\n'data.frame':   900 obs. of  6 variables:\n $ Title          : chr  \"1. Game of Thrones\" \"2. Breaking Bad\" \"3. Stranger Things\" \"4. Friends\" ...\n $ Year           : chr  \"2011‚Äì2019\" \"2008‚Äì2013\" \"2016‚Äì2025\" \"1994‚Äì2004\" ...\n $ Parental.Rating: chr  \"TV-MA\" \"TV-MA\" \"TV-14\" \"TV-14\" ...\n $ Rating         : num  9.2 9.5 8.7 8.9 8.1 9.1 8.1 8.6 8.3 9 ...\n $ Number.of.Votes: chr  \"2.3M\" \"2.1M\" \"1.3M\" \"1.1M\" ...\n $ Description    : chr  \"Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being do\"| __truncated__ \"A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine wi\"| __truncated__ \"When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatura\"| __truncated__ \"Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan bo\"| __truncated__ ...\n\nView(daten_df)"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#preprocessing-1",
    "href": "tutorial/02-einlesen-bereinigen.html#preprocessing-1",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "Preprocessing",
    "text": "Preprocessing"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#encoding-issues-checken",
    "href": "tutorial/02-einlesen-bereinigen.html#encoding-issues-checken",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.1 Encoding issues checken",
    "text": "2.1 Encoding issues checken\nNach dem Einlesen haben wir bereits einen Blick in den Datensatz geworfen. Nun schauen wir uns gezielt die Textvariable description an, um zu √ºberpr√ºfen, ob alle Zeichen richtig dargestellt werden.\n\ndaten_df %&gt;% \n   select(Description) %&gt;% \n  head()\n\n                                                                                                                                                                                                                                    Description\n1                                                                                                           Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\n2                                                                    A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine with a former student in order to secure his family's future.\n3                                                                                          When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatural forces and one strange little girl.\n4                                                                                                        Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan borough of New York City.\n5                                                                                                              Sheriff Deputy Rick Grimes wakes up from a coma to learn the world is in ruins and must lead a group of survivors to stay alive.\n6 The quirky spin on Conan Doyle's iconic sleuth pitches him as a \"high-functioning sociopath\" in modern-day London. Assisting him in his investigations: Afghanistan War vet John Watson, who's introduced to Holmes by a mutual acquaintance.\n\n\nIn diesem Fall gibt es keine Encoding issues.\nWas tun falls doch?\n\n\nBeim Einlesen das richtige Encoding mitgeben\nManuell bereinigen\n\n\nBeim manuellen Bereinigen kann die Funktion gsub() helfen, die Zeichenketten ersetzen kann. Zum Beispiel so:\n\n#string mit encoding issues\nstring &lt;- \"Sch√É¬∂ne Gr√É¬º√É¬üe aus M√É¬ºnchen!\"\nprint(string)\n\n[1] \"Sch√É¬∂ne Gr√É¬º√É\\u009fe aus M√É¬ºnchen!\"\n\n#√úberpr√ºfen, ob \"M√ºnchen\" in string vorhanden ist\ncontains_m√ºnchen &lt;- grepl(\"M√ºnchen\", string)\nprint(contains_m√ºnchen)\n\n[1] FALSE\n\n#Zeichen manuell ersetzen\nstring_bereinigt &lt;- string %&gt;% \n  gsub(pattern = \"√É¬∂\", replacement =\"√∂\") %&gt;% \n  gsub(pattern = \"√É¬º\", replacement = \"√º\") %&gt;% \n  gsub(pattern = \"√É\\u009f\", replacement = \"√ü\") \nprint(string_bereinigt)\n\n[1] \"Sch√∂ne Gr√º√üe aus M√ºnchen!\"\n\n#√úberpr√ºfen, ob \"M√ºnchen\" in string_bereinigt vorhanden ist\ncontains_m√ºnchen &lt;- grepl(\"aus\", string_bereinigt)\nprint(contains_m√ºnchen)\n\n[1] TRUE"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#datenbereinigung",
    "href": "tutorial/02-einlesen-bereinigen.html#datenbereinigung",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "Datenbereinigung",
    "text": "Datenbereinigung"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#normalisierung",
    "href": "tutorial/02-einlesen-bereinigen.html#normalisierung",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.3 Normalisierung",
    "text": "2.3 Normalisierung\nMit der Funktion tokens_tolower()aus dem quanteda Packet k√∂nnen alle Buchstaben in Kleinbuchstaben umgeformt werden.\n\ndaten_tokens &lt;- tokens_tolower(daten_tokens)\n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"     \"noble\"    \"families\" \"fight\"    \"for\"      \"control\" \n [7] \"over\"     \"the\"      \"lands\"    \"of\"       \"westeros\" \"while\"   \n[ ... and 9 more ]\n\ntext2 :\n [1] \"a\"             \"chemistry\"     \"teacher\"       \"diagnosed\"    \n [5] \"with\"          \"inoperable\"    \"lung\"          \"cancer\"       \n [9] \"turns\"         \"to\"            \"manufacturing\" \"and\"          \n[ ... and 13 more ]\n\ntext3 :\n [1] \"when\"      \"a\"         \"young\"     \"boy\"       \"vanishes\"  \"a\"        \n [7] \"small\"     \"town\"      \"uncovers\"  \"a\"         \"mystery\"   \"involving\"\n[ ... and 10 more ]"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#tokenisierung-zahlen-urls-etc.-entfernen",
    "href": "tutorial/02-einlesen-bereinigen.html#tokenisierung-zahlen-urls-etc.-entfernen",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.2 Tokenisierung & Zahlen, URLs, etc. entfernen",
    "text": "2.2 Tokenisierung & Zahlen, URLs, etc. entfernen\nDie Funktion tokens()von quanteda erm√∂glicht es uns bei der Aufteilung von Text in tokens direkt bestimmte Zeichen zu entfernen. Hier entfernen wir Punkte, Zahlen, URLs und Symbole.\n\ndaten_tokens &lt;- tokens(daten_df$Description, what = \"word\", remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_symbols = TRUE) #wollen wir das alles entfernen?\n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"Nine\"     \"noble\"    \"families\" \"fight\"    \"for\"      \"control\" \n [7] \"over\"     \"the\"      \"lands\"    \"of\"       \"Westeros\" \"while\"   \n[ ... and 9 more ]\n\ntext2 :\n [1] \"A\"             \"chemistry\"     \"teacher\"       \"diagnosed\"    \n [5] \"with\"          \"inoperable\"    \"lung\"          \"cancer\"       \n [9] \"turns\"         \"to\"            \"manufacturing\" \"and\"          \n[ ... and 13 more ]\n\ntext3 :\n [1] \"When\"      \"a\"         \"young\"     \"boy\"       \"vanishes\"  \"a\"        \n [7] \"small\"     \"town\"      \"uncovers\"  \"a\"         \"mystery\"   \"involving\"\n[ ... and 10 more ]"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#stopw√∂rter-entfernen",
    "href": "tutorial/02-einlesen-bereinigen.html#stopw√∂rter-entfernen",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.4 Stopw√∂rter entfernen",
    "text": "2.4 Stopw√∂rter entfernen"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#vereinheitlichung",
    "href": "tutorial/02-einlesen-bereinigen.html#vereinheitlichung",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.5 Vereinheitlichung",
    "text": "2.5 Vereinheitlichung\nOft gibt es W√∂rter, die unterschiedliche Abk√ºrzungen oder Schreibweisen haben. Nehmen wir das Beispiel der Europ√§ischen Union, die auch mit EU oder E.U. abgek√ºrzt wird. Mit Hilfe der Funktion gsub() k√∂nnen wir strings mit anderen strings ersetzen.\n\nstring &lt;- \"Bei den EU Wahlen k√∂nnen alle B√ºrger*innen der Europ√§ischen Union w√§hlen gehen.\"\nstring &lt;- gsub(\"Europ√§ischen Union\", \"EU\", string)\nprint(string)\n\n[1] \"Bei den EU Wahlen k√∂nnen alle B√ºrger*innen der EU w√§hlen gehen.\""
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#lemmatizingstemming",
    "href": "tutorial/02-einlesen-bereinigen.html#lemmatizingstemming",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.6 Lemmatizing/Stemming",
    "text": "2.6 Lemmatizing/Stemming"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#selteneh√§ufige-features-entfernen",
    "href": "tutorial/02-einlesen-bereinigen.html#selteneh√§ufige-features-entfernen",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.8 Seltene/h√§ufige features entfernen",
    "text": "2.8 Seltene/h√§ufige features entfernen\nIm letzten Schritt des Preprocessings entfernen wir h√§ufig und selten vorkommende features aus der dfm. Das geht mit der Funktion dfm_trim()aus dem quanteda Packet.\nEs k√∂nnen unterschiedliche thresholds gesetzt werden - hier lassen wir nur features in der dfm die mindestens in 0.5% und h√∂chstens in 99% der Dokumente vorkommen. Das Argument docfreq_type = \"prop\"berechnet den Anteil der Dokumente, die ein bestimmtes feature beinhalten relativ zur Gesamtzahl der Dokumente. verbose = TRUEprinted w√§hrend der Ausf√ºhrung der Funktion Informationen √ºber den Rechenvorgang in die Konsole.\n\ndaten_dfm &lt;- daten_dfm %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE)"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#stoppw√∂rter-entfernen",
    "href": "tutorial/02-einlesen-bereinigen.html#stoppw√∂rter-entfernen",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.4 Stoppw√∂rter entfernen",
    "text": "2.4 Stoppw√∂rter entfernen\nEs gibt verschiedene M√∂glichkeiten, Stoppw√∂rter zu entfernen. Am einfachsten ist dies mithilfe der in quanteda integrierten Stoppwortlisten m√∂glich. Diese sind in mehreren Sprachen verf√ºgbar, darunter auch Deutsch.\n\nstopwords(\"english\")\n\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n\ndaten_tokens &lt;- tokens_remove(daten_tokens, stopwords(\"english\"))\n\nJe nach Forschungsfrage k√∂nnen Stoppwortlisten angepasst werden, indem W√∂rter entfernt oder hinzugef√ºgt werden. Es ist aber auch m√∂glich eine eigene Liste zu erstellen.\n\n#W√∂rter aus der quanteda Stoppwortliste entfernen\nstoppw√∂rter &lt;- stopwords(\"english\")\nstoppw√∂rter &lt;- stoppw√∂rter[!stoppw√∂rter %in% c(\"i\", \"me\")]\n\n#W√∂rter der quanteda Stoppwortliste hinzuf√ºgen\nstoppw√∂rter &lt;- c(stoppw√∂rter, \"i\", \"me\")\n\n#Eigene Liste erstellen\neigene_stoppw√∂rter &lt;- c(\"hier\", \"eigene\", \"stoppw√∂rter\")"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#lemmatizing",
    "href": "tutorial/02-einlesen-bereinigen.html#lemmatizing",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.6 Lemmatizing",
    "text": "2.6 Lemmatizing\nMit der Funktion tokens_wordstem()aus quanteda reduzieren wir alle tokens auf ihren Wortstamm.\n\ndaten_tokens &lt;- daten_tokens %&gt;% \n  tokens_wordstem() \n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"      \"nobl\"      \"famili\"    \"fight\"     \"control\"   \"land\"     \n [7] \"westero\"   \"ancient\"   \"enemi\"     \"return\"    \"dormant\"   \"millennia\"\n\ntext2 :\n [1] \"chemistri\"      \"teacher\"        \"diagnos\"        \"inoper\"        \n [5] \"lung\"           \"cancer\"         \"turn\"           \"manufactur\"    \n [9] \"sell\"           \"methamphetamin\" \"former\"         \"student\"       \n[ ... and 4 more ]\n\ntext3 :\n [1] \"young\"      \"boy\"        \"vanish\"     \"small\"      \"town\"      \n [6] \"uncov\"      \"mysteri\"    \"involv\"     \"secret\"     \"experi\"    \n[11] \"terrifi\"    \"supernatur\"\n[ ... and 5 more ]"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#document-feature-matrix",
    "href": "tutorial/02-einlesen-bereinigen.html#document-feature-matrix",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.7 Document-Feature-Matrix",
    "text": "2.7 Document-Feature-Matrix\nUm aus unseren tokens eine dfm zu machen nutzen wir die dfm()Funktion aus dem quanteda package.\n\ndaten_dfm &lt;- daten_tokens %&gt;% \n  dfm()"
  },
  {
    "objectID": "index.html#zeitplan",
    "href": "index.html#zeitplan",
    "title": "Automatisierte Inhaltsanalyse",
    "section": "Zeitplan",
    "text": "Zeitplan\nüìÖ Mi, 24. Juli\n\n09:00 - 12:00: 1Ô∏è‚É£ Einf√ºhrung & Preprocessing\n12:00 - 13:00: ü•ó Mittagspause\n13:00 - 15:00: 2Ô∏è‚É£ Co-Occurence-Analysen\n15:00 - 17:00: 3Ô∏è‚É£ Diktion√§re\n\nüìÖ Do, 25. Juli\n\n09:00 - 12:00: 4Ô∏è‚É£ Topic Modeling\n12:00 - 13:00: ü•ó Mittagspause\n13:00 - 15:00: 5Ô∏è‚É£ Qualit√§tskriterien\n15:00 - 16:00: 6Ô∏è‚É£ Ausblick"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#off-the-shelf-diktion√§re",
    "href": "tutorial/02-einlesen-bereinigen.html#off-the-shelf-diktion√§re",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "7.1 Off-the-shelf Diktion√§re",
    "text": "7.1 Off-the-shelf Diktion√§re\nEs gibt viele off-the-shelf Diktion√§re. Der Einfachkeit halber nutzen wir hier zur Demonstation den data_dictionary_LSD2015aus dem quanteda Packet (Young & Soroka, 2012). #Soll hier noch ein Disclaimer wegen Validit√§t und Auswahl Diktion√§r etc. hin?\n\ndiktion√§r &lt;- data_dictionary_LSD2015\n\ndiktion√§r %&gt;% \n  head()\n\nDictionary object with 4 key entries.\n- [negative]:\n  - a lie, abandon*, abas*, abattoir*, abdicat*, aberra*, abhor*, abject*, abnormal*, abolish*, abominab*, abominat*, abrasiv*, absent*, abstrus*, absurd*, abus*, accident*, accost*, accursed* [ ... and 2,838 more ]\n- [positive]:\n  - ability*, abound*, absolv*, absorbent*, absorption*, abundanc*, abundant*, acced*, accentuat*, accept*, accessib*, acclaim*, acclamation*, accolad*, accommodat*, accomplish*, accord, accordan*, accorded*, accords [ ... and 1,689 more ]\n- [neg_positive]:\n  - best not, better not, no damag*, no no, not ability*, not able, not abound*, not absolv*, not absorbent*, not absorption*, not abundanc*, not abundant*, not acced*, not accentuat*, not accept*, not accessib*, not acclaim*, not acclamation*, not accolad*, not accommodat* [ ... and 1,701 more ]\n- [neg_negative]:\n  - not a lie, not abandon*, not abas*, not abattoir*, not abdicat*, not aberra*, not abhor*, not abject*, not abnormal*, not abolish*, not abominab*, not abominat*, not abrasiv*, not absent*, not abstrus*, not absurd*, not abus*, not accident*, not accost*, not accursed* [ ... and 2,840 more ]\n\n?data_dictionary_LSD2015\n\nNun wollen wir den dictionary auf unsere Daten anwenden. Diese m√ºssen daf√ºr im dfm-Format sein. Mit der Funktion dfm_lookup() aus dem quanteda Packet wird f√ºr jeden Text, also in diesem Fall f√ºr jede TV Show, geschaut, wie viele W√∂rter aus den ersten zwei Spalten des Diktion√§rs vorkommen. Die Funktion dfm_weight(scheme = \"prop\")setzt die Anzahl der dictionary W√∂rter ins Verh√§ltnis mit der L√§nge des Textes.\n\nsentiment_tvshows &lt;-  daten_dfm %&gt;% \n  dfm_weight(scheme = \"prop\") %&gt;% \n  dfm_lookup(dictionary = data_dictionary_LSD2015[1:2])\n\nsentiment_tvshows %&gt;% \n  head()\n\nDocument-feature matrix of: 6 documents, 2 features (50.00% sparse) and 0 docvars.\n       features\ndocs      negative   positive\n  text1 0.28571429 0         \n  text2 0          0         \n  text3 0.06666667 0         \n  text4 0          0.09090909\n  text5 0          0.22222222\n  text6 0.16666667 0.16666667"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#eigene-diktion√§re",
    "href": "tutorial/02-einlesen-bereinigen.html#eigene-diktion√§re",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "7.2 Eigene Diktion√§re",
    "text": "7.2 Eigene Diktion√§re"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#anpassung-auf-kleinschreibung",
    "href": "tutorial/02-einlesen-bereinigen.html#anpassung-auf-kleinschreibung",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.3 Anpassung auf Kleinschreibung",
    "text": "2.3 Anpassung auf Kleinschreibung\nMit der Funktion tokens_tolower()aus dem quanteda Packet k√∂nnen alle Buchstaben in Kleinbuchstaben umgeformt werden.\n\ndaten_tokens &lt;- tokens_tolower(daten_tokens)\n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"     \"noble\"    \"families\" \"fight\"    \"for\"      \"control\" \n [7] \"over\"     \"the\"      \"lands\"    \"of\"       \"westeros\" \"while\"   \n[ ... and 9 more ]\n\ntext2 :\n [1] \"a\"             \"chemistry\"     \"teacher\"       \"diagnosed\"    \n [5] \"with\"          \"inoperable\"    \"lung\"          \"cancer\"       \n [9] \"turns\"         \"to\"            \"manufacturing\" \"and\"          \n[ ... and 13 more ]\n\ntext3 :\n [1] \"when\"      \"a\"         \"young\"     \"boy\"       \"vanishes\"  \"a\"        \n [7] \"small\"     \"town\"      \"uncovers\"  \"a\"         \"mystery\"   \"involving\"\n[ ... and 10 more ]"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#stemming",
    "href": "tutorial/02-einlesen-bereinigen.html#stemming",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.6 Stemming",
    "text": "2.6 Stemming\nMit der Funktion tokens_wordstem()aus quanteda reduzieren wir alle tokens auf ihren Wortstamm.\n\ndaten_tokens &lt;- daten_tokens %&gt;% \n  tokens_wordstem() \n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"      \"nobl\"      \"famili\"    \"fight\"     \"control\"   \"land\"     \n [7] \"westero\"   \"ancient\"   \"enemi\"     \"return\"    \"dormant\"   \"millennia\"\n\ntext2 :\n [1] \"chemistri\"      \"teacher\"        \"diagnos\"        \"inoper\"        \n [5] \"lung\"           \"cancer\"         \"turn\"           \"manufactur\"    \n [9] \"sell\"           \"methamphetamin\" \"former\"         \"student\"       \n[ ... and 4 more ]\n\ntext3 :\n [1] \"young\"      \"boy\"        \"vanish\"     \"small\"      \"town\"      \n [6] \"uncov\"      \"mysteri\"    \"involv\"     \"secret\"     \"experi\"    \n[11] \"terrifi\"    \"supernatur\"\n[ ... and 5 more ]"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#word-cloud-erster-blick-in-die-daten",
    "href": "tutorial/02-einlesen-bereinigen.html#word-cloud-erster-blick-in-die-daten",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.9 Word cloud: Erster Blick in die Daten",
    "text": "2.9 Word cloud: Erster Blick in die Daten\nF√ºr einen ersten Einblick in die Daten lassen wir uns mit der topfeatures()Funktion aus dem quanteda Packet die 10 am h√§ufigsten vorkommenden features ausgeben.\n\ndaten_dfm %&gt;% \n  topfeatures(n = 10)\n\n  live   life famili    new  world  young follow friend   find   seri \n   108    108    107    103     75     74     74     70     69     65 \n\n\nDas Ergebnis k√∂nnen wir mit einer word cloud visualisieren. Hierf√ºr nutzen wir die textplot_wordcloud()Funktion aus dem quanteda.textplots Packet.\n\nword_cloud &lt;- daten_dfm %&gt;% \n  textplot_wordcloud(max_words = 100)\n\n\n\n\n√úbung: mit emoji einleiten ‚Äútest your knowledge‚Äù mit anderem Datensatz, der nicht zu gro√ü ist"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html",
    "href": "tutorial/Tutorial_gesamt_alt.html",
    "title": "Gesamter Code Stand 15.07",
    "section": "",
    "text": "Zun√§chst installieren und laden wir alle Packete, die wir heute und morgen brauchen.\n\n#install.packages(\"RCurl\")\n#install.packages(\"quanteda\")\n#install.packages(\"tidyverse)\n#install.packages(\"dplyr\")\n#install.packages(\"quanteda.textplots\")\n#install.packages(\"quanteda.textstats\")\n#install.packages(\"udpipe\")\n\nlibrary(\"RCurl\")\nlibrary(\"quanteda\")\nlibrary(\"tidyverse\")\nlibrary(\"dplyr\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"quanteda.textstats\")\nlibrary(\"udpipe\")\n\nDann laden wir die Datei die hinter dem Link liegt mit der Funktion getURL() aus dem package Rcurl herunter. Mit einem Blick in das Environment sehen wir, dass die einzelnen W√∂rter mit einem ; getrennt werden . Daher brauchen wir die Funktion read.csv2() aus dem utils Packet - das Packet ist vorinstalliert und immer geladen - um die Daten in R einzulesen. Der Datensatz wird im Objekt daten_df gespeichert.\n\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv\")\ndaten_df &lt;-  read.csv2(text = url)\n\nAlternativ k√∂nnen Daten in einer .csv Datei auch mit einem , voneinander abgetrennt sein. Hier br√§uchte es dann die Funktion read.csv() zum Einlesen.\nNach dem Einlesen der Daten ist es √ºblich sich zun√§chst einen √úberblick √ºber die Daten zu verschaffen und zu kontrollieren, ob alles korrekt eingelesen wurde.\n\nhead(daten_df)\n\n                Title      Year Parental.Rating Rating Number.of.Votes\n1  1. Game of Thrones 2011‚Äì2019           TV-MA    9.2            2.3M\n2     2. Breaking Bad 2008‚Äì2013           TV-MA    9.5            2.1M\n3  3. Stranger Things 2016‚Äì2025           TV-14    8.7            1.3M\n4          4. Friends 1994‚Äì2004           TV-14    8.9            1.1M\n5 5. The Walking Dead 2010‚Äì2022           TV-MA    8.1            1.1M\n6         6. Sherlock 2010‚Äì2017           TV-14    9.1              1M\n                                                                                                                                                                                                                                    Description\n1                                                                                                           Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\n2                                                                    A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine with a former student in order to secure his family's future.\n3                                                                                          When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatural forces and one strange little girl.\n4                                                                                                        Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan borough of New York City.\n5                                                                                                              Sheriff Deputy Rick Grimes wakes up from a coma to learn the world is in ruins and must lead a group of survivors to stay alive.\n6 The quirky spin on Conan Doyle's iconic sleuth pitches him as a \"high-functioning sociopath\" in modern-day London. Assisting him in his investigations: Afghanistan War vet John Watson, who's introduced to Holmes by a mutual acquaintance.\n\nstr(daten_df)\n\n'data.frame':   900 obs. of  6 variables:\n $ Title          : chr  \"1. Game of Thrones\" \"2. Breaking Bad\" \"3. Stranger Things\" \"4. Friends\" ...\n $ Year           : chr  \"2011‚Äì2019\" \"2008‚Äì2013\" \"2016‚Äì2025\" \"1994‚Äì2004\" ...\n $ Parental.Rating: chr  \"TV-MA\" \"TV-MA\" \"TV-14\" \"TV-14\" ...\n $ Rating         : num  9.2 9.5 8.7 8.9 8.1 9.1 8.1 8.6 8.3 9 ...\n $ Number.of.Votes: chr  \"2.3M\" \"2.1M\" \"1.3M\" \"1.1M\" ...\n $ Description    : chr  \"Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being do\"| __truncated__ \"A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine wi\"| __truncated__ \"When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatura\"| __truncated__ \"Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan bo\"| __truncated__ ...\n\nView(daten_df)"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#encoding-issues-checken",
    "href": "tutorial/Tutorial_gesamt_alt.html#encoding-issues-checken",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.1 Encoding issues checken",
    "text": "2.1 Encoding issues checken\nNach dem Einlesen haben wir bereits einen Blick in den Datensatz geworfen. Nun schauen wir uns gezielt die Textvariable description an, um zu √ºberpr√ºfen, ob alle Zeichen richtig dargestellt werden.\n\ndaten_df %&gt;% \n   select(Description) %&gt;% \n  head()\n\n                                                                                                                                                                                                                                    Description\n1                                                                                                           Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\n2                                                                    A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine with a former student in order to secure his family's future.\n3                                                                                          When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatural forces and one strange little girl.\n4                                                                                                        Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan borough of New York City.\n5                                                                                                              Sheriff Deputy Rick Grimes wakes up from a coma to learn the world is in ruins and must lead a group of survivors to stay alive.\n6 The quirky spin on Conan Doyle's iconic sleuth pitches him as a \"high-functioning sociopath\" in modern-day London. Assisting him in his investigations: Afghanistan War vet John Watson, who's introduced to Holmes by a mutual acquaintance.\n\n\nIn diesem Fall gibt es keine Encoding issues.\nWas tun falls doch?\n\n\nBeim Einlesen das richtige Encoding mitgeben\nManuell bereinigen\n\n\nBeim manuellen Bereinigen kann die Funktion gsub() helfen, die Zeichenketten ersetzen kann. Zum Beispiel so:\n\n#string mit encoding issues\nstring &lt;- \"Sch√É¬∂ne Gr√É¬º√É¬üe aus M√É¬ºnchen!\"\nprint(string)\n\n[1] \"Sch√É¬∂ne Gr√É¬º√É\\u009fe aus M√É¬ºnchen!\"\n\n#√úberpr√ºfen, ob \"M√ºnchen\" in string vorhanden ist\ncontains_m√ºnchen &lt;- grepl(\"M√ºnchen\", string)\nprint(contains_m√ºnchen)\n\n[1] FALSE\n\n#Zeichen manuell ersetzen\nstring_bereinigt &lt;- string %&gt;% \n  gsub(pattern = \"√É¬∂\", replacement =\"√∂\") %&gt;% \n  gsub(pattern = \"√É¬º\", replacement = \"√º\") %&gt;% \n  gsub(pattern = \"√É\\u009f\", replacement = \"√ü\") \nprint(string_bereinigt)\n\n[1] \"Sch√∂ne Gr√º√üe aus M√ºnchen!\"\n\n#√úberpr√ºfen, ob \"M√ºnchen\" in string_bereinigt vorhanden ist\ncontains_m√ºnchen &lt;- grepl(\"aus\", string_bereinigt)\nprint(contains_m√ºnchen)\n\n[1] TRUE"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#tokenisierung-zahlen-urls-etc.-entfernen",
    "href": "tutorial/Tutorial_gesamt_alt.html#tokenisierung-zahlen-urls-etc.-entfernen",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.2 Tokenisierung & Zahlen, URLs, etc. entfernen",
    "text": "2.2 Tokenisierung & Zahlen, URLs, etc. entfernen\nDie Funktion tokens()von quanteda erm√∂glicht es uns bei der Aufteilung von Text in tokens direkt bestimmte Zeichen zu entfernen. Hier entfernen wir Punkte, Zahlen, URLs und Symbole.\n\ndaten_tokens &lt;- tokens(daten_df$Description, what = \"word\", remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_symbols = TRUE) #wollen wir das alles entfernen?\n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"Nine\"     \"noble\"    \"families\" \"fight\"    \"for\"      \"control\" \n [7] \"over\"     \"the\"      \"lands\"    \"of\"       \"Westeros\" \"while\"   \n[ ... and 9 more ]\n\ntext2 :\n [1] \"A\"             \"chemistry\"     \"teacher\"       \"diagnosed\"    \n [5] \"with\"          \"inoperable\"    \"lung\"          \"cancer\"       \n [9] \"turns\"         \"to\"            \"manufacturing\" \"and\"          \n[ ... and 13 more ]\n\ntext3 :\n [1] \"When\"      \"a\"         \"young\"     \"boy\"       \"vanishes\"  \"a\"        \n [7] \"small\"     \"town\"      \"uncovers\"  \"a\"         \"mystery\"   \"involving\"\n[ ... and 10 more ]"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#anpassung-auf-kleinschreibung",
    "href": "tutorial/Tutorial_gesamt_alt.html#anpassung-auf-kleinschreibung",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.3 Anpassung auf Kleinschreibung",
    "text": "2.3 Anpassung auf Kleinschreibung\nMit der Funktion tokens_tolower()aus dem quanteda Packet k√∂nnen alle Buchstaben in Kleinbuchstaben umgeformt werden.\n\ndaten_tokens &lt;- tokens_tolower(daten_tokens)\n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"     \"noble\"    \"families\" \"fight\"    \"for\"      \"control\" \n [7] \"over\"     \"the\"      \"lands\"    \"of\"       \"westeros\" \"while\"   \n[ ... and 9 more ]\n\ntext2 :\n [1] \"a\"             \"chemistry\"     \"teacher\"       \"diagnosed\"    \n [5] \"with\"          \"inoperable\"    \"lung\"          \"cancer\"       \n [9] \"turns\"         \"to\"            \"manufacturing\" \"and\"          \n[ ... and 13 more ]\n\ntext3 :\n [1] \"when\"      \"a\"         \"young\"     \"boy\"       \"vanishes\"  \"a\"        \n [7] \"small\"     \"town\"      \"uncovers\"  \"a\"         \"mystery\"   \"involving\"\n[ ... and 10 more ]"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#stoppw√∂rter-entfernen",
    "href": "tutorial/Tutorial_gesamt_alt.html#stoppw√∂rter-entfernen",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.4 Stoppw√∂rter entfernen",
    "text": "2.4 Stoppw√∂rter entfernen\nEs gibt verschiedene M√∂glichkeiten, Stoppw√∂rter zu entfernen. Am einfachsten ist dies mithilfe der in quanteda integrierten Stoppwortlisten m√∂glich. Diese sind in mehreren Sprachen verf√ºgbar, darunter auch Deutsch.\n\nstopwords(\"english\")\n\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n\ndaten_tokens &lt;- tokens_remove(daten_tokens, stopwords(\"english\"))\n\nJe nach Forschungsfrage k√∂nnen Stoppwortlisten angepasst werden, indem W√∂rter entfernt oder hinzugef√ºgt werden. Es ist aber auch m√∂glich eine eigene Liste zu erstellen.\n\n#W√∂rter aus der quanteda Stoppwortliste entfernen\nstoppw√∂rter &lt;- stopwords(\"english\")\nstoppw√∂rter &lt;- stoppw√∂rter[!stoppw√∂rter %in% c(\"i\", \"me\")]\n\n#W√∂rter der quanteda Stoppwortliste hinzuf√ºgen\nstoppw√∂rter &lt;- c(stoppw√∂rter, \"i\", \"me\")\n\n#Eigene Liste erstellen\neigene_stoppw√∂rter &lt;- c(\"hier\", \"eigene\", \"stoppw√∂rter\")"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#vereinheitlichung",
    "href": "tutorial/Tutorial_gesamt_alt.html#vereinheitlichung",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.5 Vereinheitlichung",
    "text": "2.5 Vereinheitlichung\nOft gibt es W√∂rter, die unterschiedliche Abk√ºrzungen oder Schreibweisen haben. Nehmen wir das Beispiel der Europ√§ischen Union, die auch mit EU oder E.U. abgek√ºrzt wird. Mit Hilfe der Funktion gsub() k√∂nnen wir strings mit anderen strings ersetzen.\n\nstring &lt;- \"Bei den EU Wahlen k√∂nnen alle B√ºrger*innen der Europ√§ischen Union w√§hlen gehen.\"\nstring &lt;- gsub(\"Europ√§ischen Union\", \"EU\", string)\nprint(string)\n\n[1] \"Bei den EU Wahlen k√∂nnen alle B√ºrger*innen der EU w√§hlen gehen.\""
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#stemming",
    "href": "tutorial/Tutorial_gesamt_alt.html#stemming",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.6 Stemming",
    "text": "2.6 Stemming\nMit der Funktion tokens_wordstem()aus quanteda reduzieren wir alle tokens auf ihren Wortstamm.\n\ndaten_tokens &lt;- daten_tokens %&gt;% \n  tokens_wordstem() \n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"      \"nobl\"      \"famili\"    \"fight\"     \"control\"   \"land\"     \n [7] \"westero\"   \"ancient\"   \"enemi\"     \"return\"    \"dormant\"   \"millennia\"\n\ntext2 :\n [1] \"chemistri\"      \"teacher\"        \"diagnos\"        \"inoper\"        \n [5] \"lung\"           \"cancer\"         \"turn\"           \"manufactur\"    \n [9] \"sell\"           \"methamphetamin\" \"former\"         \"student\"       \n[ ... and 4 more ]\n\ntext3 :\n [1] \"young\"      \"boy\"        \"vanish\"     \"small\"      \"town\"      \n [6] \"uncov\"      \"mysteri\"    \"involv\"     \"secret\"     \"experi\"    \n[11] \"terrifi\"    \"supernatur\"\n[ ... and 5 more ]"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#document-feature-matrix",
    "href": "tutorial/Tutorial_gesamt_alt.html#document-feature-matrix",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.7 Document-Feature-Matrix",
    "text": "2.7 Document-Feature-Matrix\nUm aus unseren tokens eine dfm zu machen nutzen wir die dfm()Funktion aus dem quanteda package.\n\ndaten_dfm &lt;- daten_tokens %&gt;% \n  dfm()"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#selteneh√§ufige-features-entfernen",
    "href": "tutorial/Tutorial_gesamt_alt.html#selteneh√§ufige-features-entfernen",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.8 Seltene/h√§ufige features entfernen",
    "text": "2.8 Seltene/h√§ufige features entfernen\nIm letzten Schritt des Preprocessings entfernen wir h√§ufig und selten vorkommende features aus der dfm. Das geht mit der Funktion dfm_trim()aus dem quanteda Packet.\nEs k√∂nnen unterschiedliche thresholds gesetzt werden - hier lassen wir nur features in der dfm die mindestens in 0.5% und h√∂chstens in 99% der Dokumente vorkommen. Das Argument docfreq_type = \"prop\"berechnet den Anteil der Dokumente, die ein bestimmtes feature beinhalten relativ zur Gesamtzahl der Dokumente. verbose = TRUEprinted w√§hrend der Ausf√ºhrung der Funktion Informationen √ºber den Rechenvorgang in die Konsole.\n\ndaten_dfm &lt;- daten_dfm %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE)"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#word-cloud-erster-blick-in-die-daten",
    "href": "tutorial/Tutorial_gesamt_alt.html#word-cloud-erster-blick-in-die-daten",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.9 Word cloud: Erster Blick in die Daten",
    "text": "2.9 Word cloud: Erster Blick in die Daten\nF√ºr einen ersten Einblick in die Daten lassen wir uns mit der topfeatures()Funktion aus dem quanteda Packet die 10 am h√§ufigsten vorkommenden features ausgeben.\n\ndaten_dfm %&gt;% \n  topfeatures(n = 10)\n\n  live   life famili    new  world  young follow friend   find   seri \n   108    108    107    103     75     74     74     70     69     65 \n\n\nDas Ergebnis k√∂nnen wir mit einer word cloud visualisieren. Hierf√ºr nutzen wir die textplot_wordcloud()Funktion aus dem quanteda.textplots Packet.\n\nword_cloud &lt;- daten_dfm %&gt;% \n  textplot_wordcloud(max_words = 100)\n\n\n\n\n√úbung: mit emoji einleiten ‚Äútest your knowledge‚Äù mit anderem Datensatz, der nicht zu gro√ü ist"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#off-the-shelf-diktion√§re",
    "href": "tutorial/Tutorial_gesamt_alt.html#off-the-shelf-diktion√§re",
    "title": "Gesamter Code Stand 15.07",
    "section": "7.1 Off-the-shelf Diktion√§re",
    "text": "7.1 Off-the-shelf Diktion√§re\nEs gibt viele off-the-shelf Diktion√§re. Der Einfachkeit halber nutzen wir hier zur Demonstation den data_dictionary_LSD2015aus dem quanteda Packet (Young & Soroka, 2012).\n‚Äì Soll hier noch ein Disclaimer wegen Validit√§t und Auswahl Diktion√§r etc. hin?\n\ndiktion√§r &lt;- data_dictionary_LSD2015\n\ndiktion√§r %&gt;% \n  head()\n\nDictionary object with 4 key entries.\n- [negative]:\n  - a lie, abandon*, abas*, abattoir*, abdicat*, aberra*, abhor*, abject*, abnormal*, abolish*, abominab*, abominat*, abrasiv*, absent*, abstrus*, absurd*, abus*, accident*, accost*, accursed* [ ... and 2,838 more ]\n- [positive]:\n  - ability*, abound*, absolv*, absorbent*, absorption*, abundanc*, abundant*, acced*, accentuat*, accept*, accessib*, acclaim*, acclamation*, accolad*, accommodat*, accomplish*, accord, accordan*, accorded*, accords [ ... and 1,689 more ]\n- [neg_positive]:\n  - best not, better not, no damag*, no no, not ability*, not able, not abound*, not absolv*, not absorbent*, not absorption*, not abundanc*, not abundant*, not acced*, not accentuat*, not accept*, not accessib*, not acclaim*, not acclamation*, not accolad*, not accommodat* [ ... and 1,701 more ]\n- [neg_negative]:\n  - not a lie, not abandon*, not abas*, not abattoir*, not abdicat*, not aberra*, not abhor*, not abject*, not abnormal*, not abolish*, not abominab*, not abominat*, not abrasiv*, not absent*, not abstrus*, not absurd*, not abus*, not accident*, not accost*, not accursed* [ ... and 2,840 more ]\n\n?data_dictionary_LSD2015\n\nNun wollen wir den Diktion√§r auf unsere Daten anwenden. Diese m√ºssen daf√ºr im dfm-Format sein. Mit der Funktion dfm_lookup() aus dem quanteda Packet wird f√ºr jeden Text, also in diesem Fall f√ºr jede TV Show, geschaut, wie viele W√∂rter aus den ersten zwei Spalten des Diktion√§rs vorkommen. Die Funktion dfm_weight(scheme = \"prop\")setzt die Anzahl der dictionary W√∂rter ins Verh√§ltnis mit der L√§nge des Textes.\n\nsentiment_tvshows &lt;-  daten_dfm %&gt;% \n  dfm_weight(scheme = \"prop\") %&gt;% \n  dfm_lookup(dictionary = data_dictionary_LSD2015[1:2])\n\nsentiment_tvshows %&gt;% \n  head()\n\nDocument-feature matrix of: 6 documents, 2 features (50.00% sparse) and 0 docvars.\n       features\ndocs      negative   positive\n  text1 0.28571429 0         \n  text2 0          0         \n  text3 0.06666667 0         \n  text4 0          0.09090909\n  text5 0          0.22222222\n  text6 0.16666667 0.16666667"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#eigene-diktion√§re",
    "href": "tutorial/Tutorial_gesamt_alt.html#eigene-diktion√§re",
    "title": "Gesamter Code Stand 15.07",
    "section": "7.2 Eigene Diktion√§re",
    "text": "7.2 Eigene Diktion√§re"
  },
  {
    "objectID": "tutorial/02_cooccurence_kollokationen_postagging_ner.html",
    "href": "tutorial/02_cooccurence_kollokationen_postagging_ner.html",
    "title": "Sitzung 2: Co-Occurence Analyse, Kollokationen, POS-tagging und named entity recognition",
    "section": "",
    "text": "#Packages laden und Objekte erstellen\n\n#install.packages(\"RCurl\")\n#install.packages(\"quanteda\")\n#install.packages(\"tidyverse)\n#install.packages(\"dplyr\")\n#install.packages(\"quanteda.textplots\")\n#install.packages(\"quanteda.textstats\")\n#install.packages(\"udpipe\")\n\nlibrary(\"RCurl\")\nlibrary(\"quanteda\")\nlibrary(\"tidyverse\")\nlibrary(\"dplyr\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"quanteda.textstats\")\nlibrary(\"udpipe\")\n\n\n#Daten laden\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv\")\ndaten_df &lt;-  read.csv2(text = url)\n\n#Tokens\nstopwords(\"english\")\n\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n\ndaten_tokens &lt;- daten_df$Description %&gt;% \n  tokens(what = \"word\",\n         remove_punct = TRUE, \n         remove_numbers = TRUE, \n         remove_url = TRUE, \n         remove_symbols = TRUE) %&gt;% \n  tokens_tolower() %&gt;% \n  tokens_remove(stopwords(\"english\")) %&gt;% \n  tokens_wordstem()\n\n#dfm\ndaten_dfm &lt;- daten_tokens %&gt;% \n  dfm() %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE) \n\n\n3 Co-Occurrence-Analysen\nBevor wir zur Co-Occurence-Analyse kommen, gibt es auch eine Funktion, die es einem schnell und unkompliziert erm√∂glicht herauszufinden, in welchem Kontext ein Wort benutzt wird. Die hier gemeinte Funktion ist die kwic()Funktion aus dem quanteda Packet. Mit folgendem Code k√∂nnen wir beispielsweise herausfinden, in welchem Kontext das Wort hero vorkommt. In der Ausgabe werden jeweils die f√ºnf W√∂rter vor und nach dem Wort hero angezeigt.\n\ndaten_tokens %&gt;% \n  kwic(pattern = \"hero\", window = 5)\n\nKeyword-in-context with 12 matches.                                                          \n   [text75, 4]                famili former child | hero |\n  [text124, 3]                      stori saitama | hero |\n [text140, 18]      embark path destin turn rebel | hero |\n [text230, 24]        beast boy togeth becom team | hero |\n  [text241, 9]    hunter must recruit ragtag team | hero |\n [text292, 15]        know super power compound v | hero |\n  [text336, 5] superhero-admir boy enrol prestigi | hero |\n [text336, 10]     hero academi learn realli mean | hero |\n [text391, 10]      take challeng whole new level | hero |\n  [text498, 3]                       seven formid | hero |\n [text646, 11]   unit state america team everyday | hero |\n  [text756, 4]           hercul half-man half-god | hero |\n                                       \n now grown apart must reunit           \n just fun can defeat enemi             \n                                       \n                                       \n villain help prevent apocalyps impact \n put physic moral boundari test        \n academi learn realli mean hero        \n strongest superhero grant power       \n fallen apart desol dhananjay rajpoot  \n form arguabl power team ever          \n must transport known survivor plagu   \n fantast strength malevol stepmoth hera\n\n\nNun zur Co-Occurence-Analyse. Hierf√ºr m√ºssen wir zun√§chst die dfm in eine Feature Co-occurrence Matrix (fcm) umwandeln. Das machen wir mit der Funktion fcm() aus dem quanteda Packet.\n\ndaten_fcm &lt;- daten_dfm %&gt;% \n  fcm()\n\ndaten_fcm %&gt;% \n  head()\n\nFeature co-occurrence matrix of: 6 by 605 features.\n         features\nfeatures  famili fight control land ancient enemi return turn former student\n  famili       8     6       2    3       1     4      6    3      4       2\n  fight        0     1       2    2       2     2      2    1      0       2\n  control      0     0       0    1       1     1      1    0      0       0\n  land         0     0       0    0       1     1      1    0      0       1\n  ancient      0     0       0    0       0     2      1    0      0       0\n  enemi        0     0       0    0       0     0      2    0      0       0\n[ reached max_nfeat ... 595 more features ]\n\n\nBeim n√§chsten Schritt w√§hlen wir die features aus, die uns in unserer Analyse interessieren. Das machen wir mit der Funktion fcm_select()aus dem quanteda Packet.\n\ndaten_fcm &lt;- daten_fcm %&gt;% \n  fcm_select(pattern = c(\"famili\", \"crime\", \"america\", \"school\"), #hier noch bessere Begriffe vielleicht √ºber topfeatures oder topic modeling\n             selection = \"keep\")\n\nDann visualisieren wir die fcm mit der textplot_network()Funktion aus demquanteda.textplots() Paket. Wenn features im selben Dokument vorkommen, werden sie mit einer Linie verbunden. Umso dicker die Linie, desto √∂fter kommen die features miteinander vor.\n\ntextplot_network(daten_fcm)\n\n\n\n\nDiese Visualisierung gibt uns nun aber noch keine genauen Angaben dazu, wie oft ein feature mit einem anderen vorkommt. Um das herauszufinden, m√ºssen wir die fcm mit der convert()Funktion aus dem quanteda Packet in einen data frame umwandeln.\n\ndaten_fcm_df &lt;- daten_fcm %&gt;% \n  convert(to = \"data.frame\")\n\ndaten_fcm_df %&gt;% \n  head()\n\n   doc_id famili america crime school\n1  famili      8       3     3      7\n2 america      0       0     2      0\n3   crime      0       0     2      0\n4  school      0       0     0      2\n\n\nMit Hilfe von select()k√∂nnen wir uns nun einzelne H√§ufigkeiten, wie oft ein feature mit einem anderen feature vorkommt, ausgeben lassen.\n\ndaten_fcm_df %&gt;%\n  filter(doc_id == \"crime\") %&gt;% #Zeile\n  select(america) #Spalte #Frage: Wie kann es sein, dass crime america 0 ist und america crime 2?\n\n  america\n1       0\n\n\n\n\n4 Kollokationen und N-gramme\nUm herauszufinden, welche tokens oft hintereinander vorkommen, k√∂nnen wir die Funktion textstat_collocationsaus dem quanteda.textstats Packet verwenden.\n\ndaten_tokens %&gt;%\n  textstat_collocations(min_count = 10) %&gt;%\n  arrange(-lambda) %&gt;%\n  head(10)\n\n        collocation count count_nested length    lambda         z\n8         los angel    22            0      2 11.993246  7.856635\n9          new york    39            0      2  9.635906  6.744996\n5     serial killer    10            0      2  8.666634 11.850129\n4 person profession    13            0      2  7.818063 12.192676\n7     antholog seri    10            0      2  7.633523  8.612874\n1       high school    22            0      2  7.042098 16.492644\n3       best friend    25            0      2  7.006749 15.089627\n2         york citi    19            0      2  5.811625 16.073334\n6       seri follow    10            0      2  4.324217 11.364619\n\n\nIn einem weiteren Schritt kann es manchmal sinnvoll sein, Kollokationen f√ºr die Analyse zu einem token zusammenzufassen (dieser Schritt w√§re dann Teil des Preprocessings). Das l√§sst sich mit der tokens_compound()Funktion aus dem quanteda.textstats Packet umsetzen. Die Funktion verbindet die gegebenen tokens mit einem Unterstrich zu einem token.\n\nngramme &lt;- c(\"los angel\",\"new york citi\", \"serial killer\", \"high school\", \"best friend\")\ndaten_tokens_ngramme &lt;- tokens_compound(daten_tokens, pattern = phrase(ngramme))\n\nWie das nun in den Daten aussieht k√∂nnen wir mit Hilfe der kwic()Funktion aus dem quanteda Packet herausfinden (kwic steht f√ºr keywords in context).\n\ndaten_tokens_ngramme %&gt;% \n  kwic(pattern = c(\"los angel\",\"new york citi\", \"serial killer\", \"high school\", \"best friend\")) %&gt;% \n  head(n=30)\n\nKeyword-in-context with 0 matches.\n\n\n\n\n5 Parts-of-speech tagging\nF√ºr das parts of speech tagging nutzen wir das Packet UDPipe. Bevor wir unseren data frame in die udipie()Funktion geben, m√ºssen wir ihn ins tibble Format umformen und eine ID Variable erstellen. Des Weiteren bennen wir die Spalte value in text um. Das ist notwendig, weil as_tibble()die urspr√ºngliche Spalte Description in value umbenannt hat. Am Ende lassen wir uns nur einen Teil der Variablen ausgeben, damit das Ergebnis √ºbersichtlich bleibt.\n\ndaten_df_udpipe &lt;- daten_df$Description %&gt;%\n\n#Format f√ºr das udpipe Packet anpassen\n  as_tibble() %&gt;%\n  mutate(doc_id = paste0(\"text\", 1:n())) %&gt;% \n  rename(text = value) %&gt;%\n\n#Der Einfachheit halber nur f√ºr einen Text\n  slice(1) %&gt;%\n\n#part-of-speech tagging\n  udpipe(\"english\") \n\nWenn wir jetzt beispielsweise herausfinden wollen, mit welchen Adjektiven das Wort ‚Äúfamily‚Äù beschrieben wird, sieht der Code daf√ºr wie folgt aus:\n\n#Dataframe nach Nomen mit dem lemma \"family\" filtern\nadjectives_describing_family &lt;- daten_df_udpipe %&gt;%\n  filter(upos == \"NOUN\" & lemma == \"family\") %&gt;%\n\n#Den gefilterten Dataframe mit dem ungefilterten Dataframe joinen\n  inner_join(daten_df_udpipe, by = c(\"doc_id\", \"sentence_id\")) %&gt;%\n  \n#Gro√üen Dataframe nach Adjektiven filtern, die bei ihrer head_token Variable die family token_id haben\n  filter(upos.y == \"ADJ\" & head_token_id.y == token_id.x) %&gt;%\n\n#relevanten Variablen ausw√§hlen\n  select(doc_id, sentence_id, token_id = token_id.y, token = token.y, upos = upos.y)\n\nprint(adjectives_describing_family)\n\n  doc_id sentence_id token_id token upos\n1  text1           1        2 noble  ADJ\n\n\n\n\n6 Named entitiy recognition"
  },
  {
    "objectID": "tutorial/05_qualit√§tskriterien.html",
    "href": "tutorial/05_qualit√§tskriterien.html",
    "title": "Sitzung 5: Qualit√§tskriterien",
    "section": "",
    "text": "#Packages laden und Objekte erstellen\n\n#install.packages(\"RCurl\")\n#install.packages(\"quanteda\")\n#install.packages(\"tidyverse)\n#install.packages(\"dplyr\")\n#install.packages(\"quanteda.textplots\")\n#install.packages(\"quanteda.textstats\")\n#install.packages(\"udpipe\")\n\nlibrary(\"RCurl\")\nlibrary(\"quanteda\")\nlibrary(\"tidyverse\")\nlibrary(\"dplyr\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"quanteda.textstats\")\nlibrary(\"udpipe\")\n\n\n#Daten laden\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv\")\ndaten_df &lt;-  read.csv2(text = url)\n\n#Tokens\nstopwords(\"english\")\n\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n\ndaten_tokens &lt;- daten_df$Description %&gt;% \n  tokens(what = \"word\",\n         remove_punct = TRUE, \n         remove_numbers = TRUE, \n         remove_url = TRUE, \n         remove_symbols = TRUE) %&gt;% \n  tokens_tolower() %&gt;% \n  tokens_remove(stopwords(\"english\")) %&gt;% \n  tokens_wordstem()\n\n#dfm\ndaten_dfm &lt;- daten_tokens %&gt;% \n  dfm() %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE) \n\n\n9 Qualit√§tskriterien"
  },
  {
    "objectID": "tutorial/03_diktion√§re.html",
    "href": "tutorial/03_diktion√§re.html",
    "title": "Sitzung 3: Diktion√§re",
    "section": "",
    "text": "#Packages laden und Objekte erstellen\n#install.packages(\"RCurl\")\n#install.packages(\"quanteda\")\n#install.packages(\"tidyverse)\n#install.packages(\"dplyr\")\n#install.packages(\"quanteda.textplots\")\n#install.packages(\"quanteda.textstats\")\n#install.packages(\"udpipe\")\n\nlibrary(\"RCurl\")\nlibrary(\"quanteda\")\nlibrary(\"tidyverse\")\nlibrary(\"dplyr\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"quanteda.textstats\")\nlibrary(\"udpipe\")\n#Daten laden\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv\")\ndaten_df &lt;-  read.csv2(text = url)\n\n#Tokens\nstopwords(\"english\")\n\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n\ndaten_tokens &lt;- daten_df$Description %&gt;% \n  tokens(what = \"word\",\n         remove_punct = TRUE, \n         remove_numbers = TRUE, \n         remove_url = TRUE, \n         remove_symbols = TRUE) %&gt;% \n  tokens_tolower() %&gt;% \n  tokens_remove(stopwords(\"english\")) %&gt;% \n  tokens_wordstem()\n\n#dfm\ndaten_dfm &lt;- daten_tokens %&gt;% \n  dfm() %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE)"
  },
  {
    "objectID": "tutorial/03_diktion√§re.html#off-the-shelf-diktion√§re",
    "href": "tutorial/03_diktion√§re.html#off-the-shelf-diktion√§re",
    "title": "Sitzung 3: Diktion√§re",
    "section": "7.1 Off-the-shelf Diktion√§re",
    "text": "7.1 Off-the-shelf Diktion√§re\nEs gibt viele off-the-shelf Diktion√§re. Der Einfachkeit halber nutzen wir hier zur Demonstation den data_dictionary_LSD2015aus dem quanteda Packet (Young & Soroka, 2012).\n\ndiktion√§r &lt;- data_dictionary_LSD2015\n\ndiktion√§r %&gt;% \n  head()\n\nDictionary object with 4 key entries.\n- [negative]:\n  - a lie, abandon*, abas*, abattoir*, abdicat*, aberra*, abhor*, abject*, abnormal*, abolish*, abominab*, abominat*, abrasiv*, absent*, abstrus*, absurd*, abus*, accident*, accost*, accursed* [ ... and 2,838 more ]\n- [positive]:\n  - ability*, abound*, absolv*, absorbent*, absorption*, abundanc*, abundant*, acced*, accentuat*, accept*, accessib*, acclaim*, acclamation*, accolad*, accommodat*, accomplish*, accord, accordan*, accorded*, accords [ ... and 1,689 more ]\n- [neg_positive]:\n  - best not, better not, no damag*, no no, not ability*, not able, not abound*, not absolv*, not absorbent*, not absorption*, not abundanc*, not abundant*, not acced*, not accentuat*, not accept*, not accessib*, not acclaim*, not acclamation*, not accolad*, not accommodat* [ ... and 1,701 more ]\n- [neg_negative]:\n  - not a lie, not abandon*, not abas*, not abattoir*, not abdicat*, not aberra*, not abhor*, not abject*, not abnormal*, not abolish*, not abominab*, not abominat*, not abrasiv*, not absent*, not abstrus*, not absurd*, not abus*, not accident*, not accost*, not accursed* [ ... and 2,840 more ]\n\n?data_dictionary_LSD2015\n\nNun wollen wir den Diktion√§r auf unsere Daten anwenden. Diese m√ºssen daf√ºr im dfm-Format sein. Mit der Funktion dfm_lookup() aus dem quanteda Packet wird f√ºr jeden Text, also in diesem Fall f√ºr jede TV Show, geschaut, wie viele W√∂rter aus den ersten zwei Spalten des Diktion√§rs vorkommen. Die Funktion dfm_weight(scheme = \"prop\")setzt die Anzahl der dictionary W√∂rter ins Verh√§ltnis mit der L√§nge des Textes.\n\nsentiment_tvshows &lt;-  daten_dfm %&gt;% \n  dfm_weight(scheme = \"prop\") %&gt;% \n  dfm_lookup(dictionary = data_dictionary_LSD2015[1:2])\n\nsentiment_tvshows %&gt;% \n  head()\n\nDocument-feature matrix of: 6 documents, 2 features (50.00% sparse) and 0 docvars.\n       features\ndocs      negative   positive\n  text1 0.28571429 0         \n  text2 0          0         \n  text3 0.06666667 0         \n  text4 0          0.09090909\n  text5 0          0.22222222\n  text6 0.16666667 0.16666667\n\n\nEine erste Auswertung kann dann beispielsweise so aussehen:\n\n#S4 Objekt in einen data frame umwandeln\nsentiment_tvshows &lt;- convert(sentiment_tvshows, to = \"data.frame\")\n\n#Anzahl Texte, die mindestens ein negatives Wort haben\nsentiment_tvshows %&gt;% \n  filter(negative != 0) %&gt;% \n  count()\n\n    n\n1 436\n\n#Negativit√§t in Summe\nsentiment_tvshows$negative %&gt;% \n  sum()\n\n[1] 73.02149\n\n#Durschnittlicher negativ-Wert bei negativen Texten\n(sentiment_tvshows$negative %&gt;% \n  sum())/(sentiment_tvshows %&gt;% \n  filter(negative != 0) %&gt;% \n  count())\n\n          n\n1 0.1674805\n\n#Negativster Text\nsentiment_tvshows %&gt;% \n  filter(negative != 0) %&gt;% \n  arrange(desc(negative))\n\n     doc_id   negative   positive\n1   text329 0.60000000 0.10000000\n2   text687 0.50000000 0.00000000\n3   text888 0.50000000 0.25000000\n4   text253 0.44444444 0.22222222\n5   text444 0.42857143 0.14285714\n6   text723 0.42857143 0.00000000\n7   text151 0.40000000 0.00000000\n8   text193 0.40000000 0.00000000\n9   text284 0.40000000 0.10000000\n10  text458 0.40000000 0.00000000\n11  text411 0.38461538 0.00000000\n12  text259 0.37500000 0.00000000\n13  text735 0.36363636 0.00000000\n14   text86 0.33333333 0.00000000\n15  text106 0.33333333 0.00000000\n16  text131 0.33333333 0.00000000\n17  text153 0.33333333 0.11111111\n18  text190 0.33333333 0.00000000\n19  text205 0.33333333 0.00000000\n20  text219 0.33333333 0.16666667\n21  text244 0.33333333 0.00000000\n22  text314 0.33333333 0.00000000\n23  text316 0.33333333 0.00000000\n24  text397 0.33333333 0.00000000\n25  text402 0.33333333 0.00000000\n26  text490 0.33333333 0.00000000\n27  text656 0.33333333 0.00000000\n28  text668 0.33333333 0.16666667\n29  text671 0.33333333 0.00000000\n30  text782 0.33333333 0.00000000\n31  text823 0.33333333 0.33333333\n32  text824 0.33333333 0.00000000\n33  text869 0.33333333 0.00000000\n34  text890 0.33333333 0.00000000\n35  text159 0.31250000 0.06250000\n36  text154 0.30769231 0.07692308\n37   text50 0.30000000 0.00000000\n38  text865 0.30000000 0.00000000\n39    text1 0.28571429 0.00000000\n40   text20 0.28571429 0.14285714\n41   text26 0.28571429 0.00000000\n42   text30 0.28571429 0.00000000\n43   text46 0.28571429 0.00000000\n44  text192 0.28571429 0.00000000\n45  text196 0.28571429 0.00000000\n46  text328 0.28571429 0.00000000\n47  text361 0.28571429 0.00000000\n48  text435 0.28571429 0.14285714\n49  text457 0.28571429 0.28571429\n50  text466 0.28571429 0.07142857\n51  text798 0.28571429 0.00000000\n52  text856 0.28571429 0.00000000\n53  text148 0.27272727 0.18181818\n54  text482 0.27272727 0.09090909\n55  text756 0.27272727 0.36363636\n56    text8 0.25000000 0.00000000\n57   text10 0.25000000 0.25000000\n58   text21 0.25000000 0.00000000\n59   text27 0.25000000 0.00000000\n60   text32 0.25000000 0.12500000\n61   text62 0.25000000 0.00000000\n62  text178 0.25000000 0.25000000\n63  text181 0.25000000 0.12500000\n64  text218 0.25000000 0.00000000\n65  text277 0.25000000 0.00000000\n66  text342 0.25000000 0.00000000\n67  text352 0.25000000 0.00000000\n68  text406 0.25000000 0.12500000\n69  text474 0.25000000 0.12500000\n70  text542 0.25000000 0.00000000\n71  text601 0.25000000 0.12500000\n72  text621 0.25000000 0.08333333\n73  text643 0.25000000 0.25000000\n74  text644 0.25000000 0.00000000\n75  text655 0.25000000 0.00000000\n76  text662 0.25000000 0.00000000\n77  text692 0.25000000 0.00000000\n78  text727 0.25000000 0.25000000\n79  text742 0.25000000 0.00000000\n80  text768 0.25000000 0.00000000\n81  text784 0.25000000 0.00000000\n82  text852 0.25000000 0.00000000\n83  text625 0.23529412 0.00000000\n84  text179 0.23076923 0.00000000\n85  text182 0.23076923 0.15384615\n86  text591 0.23076923 0.07692308\n87  text763 0.23076923 0.00000000\n88   text63 0.22222222 0.00000000\n89  text202 0.22222222 0.00000000\n90  text217 0.22222222 0.00000000\n91  text228 0.22222222 0.11111111\n92  text255 0.22222222 0.11111111\n93  text374 0.22222222 0.00000000\n94  text391 0.22222222 0.11111111\n95  text439 0.22222222 0.11111111\n96  text488 0.22222222 0.11111111\n97  text494 0.22222222 0.00000000\n98  text589 0.22222222 0.00000000\n99  text596 0.22222222 0.00000000\n100 text600 0.22222222 0.00000000\n101 text845 0.22222222 0.00000000\n102 text847 0.22222222 0.00000000\n103 text857 0.22222222 0.00000000\n104 text864 0.22222222 0.22222222\n105 text899 0.22222222 0.00000000\n106  text74 0.21428571 0.21428571\n107 text611 0.21428571 0.00000000\n108 text767 0.21428571 0.14285714\n109 text859 0.21428571 0.00000000\n110  text22 0.20000000 0.10000000\n111  text47 0.20000000 0.00000000\n112  text79 0.20000000 0.10000000\n113 text118 0.20000000 0.20000000\n114 text140 0.20000000 0.10000000\n115 text149 0.20000000 0.20000000\n116 text155 0.20000000 0.00000000\n117 text162 0.20000000 0.10000000\n118 text223 0.20000000 0.00000000\n119 text283 0.20000000 0.20000000\n120 text287 0.20000000 0.00000000\n121 text288 0.20000000 0.06666667\n122 text290 0.20000000 0.10000000\n123 text294 0.20000000 0.20000000\n124 text392 0.20000000 0.00000000\n125 text463 0.20000000 0.10000000\n126 text480 0.20000000 0.20000000\n127 text487 0.20000000 0.10000000\n128 text539 0.20000000 0.00000000\n129 text561 0.20000000 0.60000000\n130 text588 0.20000000 0.10000000\n131 text594 0.20000000 0.00000000\n132 text642 0.20000000 0.00000000\n133 text680 0.20000000 0.20000000\n134 text697 0.20000000 0.00000000\n135 text751 0.20000000 0.00000000\n136 text850 0.20000000 0.10000000\n137 text634 0.18750000 0.06250000\n138 text791 0.18750000 0.06250000\n139 text896 0.18750000 0.18750000\n140 text122 0.18181818 0.09090909\n141 text269 0.18181818 0.09090909\n142 text291 0.18181818 0.09090909\n143 text383 0.18181818 0.00000000\n144 text419 0.18181818 0.09090909\n145 text465 0.18181818 0.00000000\n146 text471 0.18181818 0.00000000\n147 text501 0.18181818 0.09090909\n148 text548 0.18181818 0.00000000\n149 text788 0.18181818 0.00000000\n150 text858 0.18181818 0.09090909\n151 text883 0.18181818 0.00000000\n152 text230 0.17647059 0.11764706\n153 text520 0.17647059 0.00000000\n154 text556 0.17647059 0.00000000\n155   text6 0.16666667 0.16666667\n156  text13 0.16666667 0.00000000\n157  text34 0.16666667 0.08333333\n158  text42 0.16666667 0.16666667\n159  text52 0.16666667 0.00000000\n160  text66 0.16666667 0.00000000\n161 text115 0.16666667 0.00000000\n162 text124 0.16666667 0.16666667\n163 text142 0.16666667 0.00000000\n164 text200 0.16666667 0.00000000\n165 text207 0.16666667 0.00000000\n166 text234 0.16666667 0.00000000\n167 text236 0.16666667 0.00000000\n168 text242 0.16666667 0.50000000\n169 text270 0.16666667 0.16666667\n170 text295 0.16666667 0.00000000\n171 text330 0.16666667 0.00000000\n172 text349 0.16666667 0.00000000\n173 text377 0.16666667 0.00000000\n174 text381 0.16666667 0.00000000\n175 text429 0.16666667 0.00000000\n176 text483 0.16666667 0.00000000\n177 text518 0.16666667 0.16666667\n178 text521 0.16666667 0.00000000\n179 text545 0.16666667 0.33333333\n180 text560 0.16666667 0.00000000\n181 text574 0.16666667 0.00000000\n182 text576 0.16666667 0.00000000\n183 text582 0.16666667 0.00000000\n184 text605 0.16666667 0.16666667\n185 text620 0.16666667 0.00000000\n186 text665 0.16666667 0.00000000\n187 text676 0.16666667 0.33333333\n188 text758 0.16666667 0.08333333\n189 text818 0.16666667 0.00000000\n190 text833 0.16666667 0.00000000\n191 text853 0.16666667 0.00000000\n192  text36 0.15384615 0.15384615\n193  text60 0.15384615 0.00000000\n194 text102 0.15384615 0.07692308\n195 text127 0.15384615 0.00000000\n196 text431 0.15384615 0.00000000\n197 text481 0.15384615 0.15384615\n198 text549 0.15384615 0.00000000\n199 text707 0.15384615 0.07692308\n200 text750 0.15384615 0.15384615\n201 text783 0.15384615 0.15384615\n202 text753 0.14814815 0.00000000\n203  text12 0.14285714 0.14285714\n204  text56 0.14285714 0.07142857\n205 text120 0.14285714 0.07142857\n206 text136 0.14285714 0.14285714\n207 text177 0.14285714 0.07142857\n208 text180 0.14285714 0.42857143\n209 text195 0.14285714 0.00000000\n210 text225 0.14285714 0.00000000\n211 text249 0.14285714 0.00000000\n212 text250 0.14285714 0.00000000\n213 text262 0.14285714 0.28571429\n214 text264 0.14285714 0.00000000\n215 text265 0.14285714 0.07142857\n216 text303 0.14285714 0.00000000\n217 text320 0.14285714 0.00000000\n218 text323 0.14285714 0.00000000\n219 text358 0.14285714 0.14285714\n220 text373 0.14285714 0.00000000\n221 text396 0.14285714 0.00000000\n222 text408 0.14285714 0.00000000\n223 text428 0.14285714 0.00000000\n224 text456 0.14285714 0.00000000\n225 text478 0.14285714 0.00000000\n226 text514 0.14285714 0.00000000\n227 text522 0.14285714 0.14285714\n228 text564 0.14285714 0.14285714\n229 text626 0.14285714 0.00000000\n230 text638 0.14285714 0.14285714\n231 text659 0.14285714 0.00000000\n232 text694 0.14285714 0.07142857\n233 text695 0.14285714 0.00000000\n234 text701 0.14285714 0.00000000\n235 text712 0.14285714 0.14285714\n236 text728 0.14285714 0.00000000\n237 text747 0.14285714 0.00000000\n238 text759 0.14285714 0.00000000\n239 text765 0.14285714 0.00000000\n240 text774 0.14285714 0.14285714\n241 text781 0.14285714 0.00000000\n242 text811 0.14285714 0.00000000\n243 text831 0.14285714 0.00000000\n244 text862 0.14285714 0.00000000\n245 text885 0.14285714 0.00000000\n246 text886 0.14285714 0.00000000\n247 text150 0.13333333 0.06666667\n248 text160 0.13333333 0.06666667\n249 text790 0.13333333 0.00000000\n250 text863 0.13333333 0.06666667\n251  text18 0.12500000 0.00000000\n252  text40 0.12500000 0.00000000\n253  text58 0.12500000 0.00000000\n254  text65 0.12500000 0.25000000\n255  text89 0.12500000 0.00000000\n256 text103 0.12500000 0.00000000\n257 text105 0.12500000 0.00000000\n258 text114 0.12500000 0.00000000\n259 text116 0.12500000 0.00000000\n260 text163 0.12500000 0.00000000\n261 text166 0.12500000 0.25000000\n262 text167 0.12500000 0.12500000\n263 text186 0.12500000 0.00000000\n264 text199 0.12500000 0.00000000\n265 text211 0.12500000 0.12500000\n266 text261 0.12500000 0.00000000\n267 text268 0.12500000 0.06250000\n268 text299 0.12500000 0.00000000\n269 text304 0.12500000 0.00000000\n270 text353 0.12500000 0.25000000\n271 text362 0.12500000 0.12500000\n272 text378 0.12500000 0.00000000\n273 text403 0.12500000 0.12500000\n274 text437 0.12500000 0.00000000\n275 text475 0.12500000 0.25000000\n276 text505 0.12500000 0.25000000\n277 text508 0.12500000 0.00000000\n278 text517 0.12500000 0.00000000\n279 text524 0.12500000 0.12500000\n280 text558 0.12500000 0.00000000\n281 text571 0.12500000 0.00000000\n282 text577 0.12500000 0.00000000\n283 text579 0.12500000 0.12500000\n284 text586 0.12500000 0.12500000\n285 text610 0.12500000 0.00000000\n286 text627 0.12500000 0.00000000\n287 text636 0.12500000 0.00000000\n288 text684 0.12500000 0.12500000\n289 text714 0.12500000 0.12500000\n290 text736 0.12500000 0.12500000\n291 text752 0.12500000 0.25000000\n292 text776 0.12500000 0.00000000\n293 text780 0.12500000 0.00000000\n294  text45 0.11111111 0.00000000\n295  text73 0.11111111 0.00000000\n296  text82 0.11111111 0.00000000\n297  text90 0.11111111 0.22222222\n298  text95 0.11111111 0.00000000\n299 text164 0.11111111 0.22222222\n300 text189 0.11111111 0.11111111\n301 text326 0.11111111 0.00000000\n302 text338 0.11111111 0.11111111\n303 text348 0.11111111 0.11111111\n304 text360 0.11111111 0.11111111\n305 text370 0.11111111 0.00000000\n306 text388 0.11111111 0.00000000\n307 text405 0.11111111 0.00000000\n308 text449 0.11111111 0.11111111\n309 text467 0.11111111 0.00000000\n310 text479 0.11111111 0.11111111\n311 text502 0.11111111 0.00000000\n312 text528 0.11111111 0.00000000\n313 text534 0.11111111 0.00000000\n314 text639 0.11111111 0.00000000\n315 text661 0.11111111 0.11111111\n316 text689 0.11111111 0.05555556\n317 text706 0.11111111 0.44444444\n318 text737 0.11111111 0.00000000\n319 text743 0.11111111 0.00000000\n320 text772 0.11111111 0.11111111\n321 text775 0.11111111 0.00000000\n322 text879 0.11111111 0.11111111\n323 text233 0.10526316 0.10526316\n324 text300 0.10526316 0.15789474\n325  text25 0.10000000 0.30000000\n326  text55 0.10000000 0.00000000\n327 text132 0.10000000 0.10000000\n328 text135 0.10000000 0.00000000\n329 text252 0.10000000 0.10000000\n330 text273 0.10000000 0.10000000\n331 text285 0.10000000 0.00000000\n332 text331 0.10000000 0.00000000\n333 text333 0.10000000 0.20000000\n334 text340 0.10000000 0.00000000\n335 text344 0.10000000 0.00000000\n336 text368 0.10000000 0.00000000\n337 text380 0.10000000 0.00000000\n338 text413 0.10000000 0.10000000\n339 text432 0.10000000 0.00000000\n340 text443 0.10000000 0.00000000\n341 text448 0.10000000 0.00000000\n342 text451 0.10000000 0.00000000\n343 text462 0.10000000 0.00000000\n344 text622 0.10000000 0.00000000\n345 text624 0.10000000 0.30000000\n346 text648 0.10000000 0.00000000\n347 text658 0.10000000 0.00000000\n348 text698 0.10000000 0.10000000\n349 text738 0.10000000 0.00000000\n350 text760 0.10000000 0.10000000\n351 text812 0.10000000 0.00000000\n352 text855 0.10000000 0.00000000\n353 text867 0.10000000 0.00000000\n354 text873 0.10000000 0.10000000\n355 text874 0.10000000 0.00000000\n356 text146 0.09090909 0.18181818\n357 text214 0.09090909 0.00000000\n358 text220 0.09090909 0.00000000\n359 text280 0.09090909 0.18181818\n360 text324 0.09090909 0.18181818\n361 text341 0.09090909 0.18181818\n362 text359 0.09090909 0.00000000\n363 text418 0.09090909 0.09090909\n364 text424 0.09090909 0.00000000\n365 text460 0.09090909 0.00000000\n366 text493 0.09090909 0.09090909\n367 text607 0.09090909 0.18181818\n368 text675 0.09090909 0.09090909\n369 text705 0.09090909 0.09090909\n370 text721 0.09090909 0.00000000\n371 text809 0.09090909 0.09090909\n372 text810 0.09090909 0.00000000\n373 text819 0.09090909 0.09090909\n374 text860 0.09090909 0.09090909\n375 text872 0.09090909 0.00000000\n376  text49 0.08333333 0.00000000\n377  text99 0.08333333 0.08333333\n378 text111 0.08333333 0.08333333\n379 text128 0.08333333 0.25000000\n380 text169 0.08333333 0.16666667\n381 text197 0.08333333 0.16666667\n382 text224 0.08333333 0.00000000\n383 text240 0.08333333 0.00000000\n384 text384 0.08333333 0.00000000\n385 text410 0.08333333 0.08333333\n386 text436 0.08333333 0.00000000\n387 text492 0.08333333 0.08333333\n388 text510 0.08333333 0.50000000\n389 text513 0.08333333 0.00000000\n390 text590 0.08333333 0.25000000\n391 text606 0.08333333 0.00000000\n392 text674 0.08333333 0.00000000\n393 text682 0.08333333 0.00000000\n394 text719 0.08333333 0.00000000\n395 text726 0.08333333 0.08333333\n396 text766 0.08333333 0.00000000\n397  text38 0.07692308 0.00000000\n398  text39 0.07692308 0.23076923\n399  text43 0.07692308 0.00000000\n400 text112 0.07692308 0.07692308\n401 text139 0.07692308 0.07692308\n402 text229 0.07692308 0.07692308\n403 text327 0.07692308 0.07692308\n404 text452 0.07692308 0.07692308\n405 text708 0.07692308 0.00000000\n406 text711 0.07692308 0.15384615\n407 text722 0.07692308 0.23076923\n408 text849 0.07692308 0.00000000\n409 text133 0.07142857 0.07142857\n410 text307 0.07142857 0.14285714\n411 text541 0.07142857 0.21428571\n412 text587 0.07142857 0.00000000\n413 text710 0.07142857 0.07142857\n414 text730 0.07142857 0.07142857\n415 text732 0.07142857 0.07142857\n416 text796 0.07142857 0.00000000\n417 text801 0.07142857 0.14285714\n418 text875 0.07142857 0.14285714\n419   text3 0.06666667 0.00000000\n420  text70 0.06666667 0.13333333\n421 text147 0.06666667 0.06666667\n422 text243 0.06666667 0.00000000\n423 text267 0.06666667 0.00000000\n424 text446 0.06666667 0.06666667\n425 text573 0.06666667 0.00000000\n426 text602 0.06666667 0.26666667\n427 text204 0.06250000 0.12500000\n428 text266 0.06250000 0.12500000\n429 text315 0.06250000 0.06250000\n430 text567 0.06250000 0.06250000\n431 text654 0.06250000 0.00000000\n432 text232 0.05882353 0.05882353\n433 text570 0.05882353 0.05882353\n434 text834 0.05882353 0.00000000\n435 text183 0.05555556 0.00000000\n436 text646 0.05555556 0.11111111\n\ndaten_df[329,] %&gt;% \n  select(Description) %&gt;% \n  print() %&gt;% \n  head()\n\n                                                                                                                                                                                                Description\n329 Ash has spent the last thirty years avoiding responsibility, maturity, and the terrors of the Evil Dead until a Deadite plague threatens to destroy all of mankind and Ash becomes mankind's only hope.\n\n\n                                                                                                                                                                                                Description\n329 Ash has spent the last thirty years avoiding responsibility, maturity, and the terrors of the Evil Dead until a Deadite plague threatens to destroy all of mankind and Ash becomes mankind's only hope."
  },
  {
    "objectID": "tutorial/03_diktion√§re.html#eigene-diktion√§re",
    "href": "tutorial/03_diktion√§re.html#eigene-diktion√§re",
    "title": "Sitzung 3: Diktion√§re",
    "section": "7.2 Eigene Diktion√§re",
    "text": "7.2 Eigene Diktion√§re\nZum Teil kann es sinnvoller sein einen eigenen Diktion√§r zu nutzen. So eine Wortliste zu erstellen geht sehr einfach:\n\ndiktion√§r_crime &lt;- dictionary(list(crime = c(\"crime\", \"police\", \"gun\", \"shot\", \"dead\", \"murder\", \"kill\", \"court\", \"suspect\", \"witness\", \"arrest\", \"officer\", \"verdict\")))\n\nDie Anwendung und Auswertung bleibt gleich:\n\n#Diktion√§r anwenden\ncrime_tvshows &lt;-  daten_dfm %&gt;% \n  dfm_weight(scheme = \"prop\") %&gt;% \n  dfm_lookup(dictionary = diktion√§r_crime)\n\n#S4 Objekt in einen data frame umwandeln \ncrime_tvshows &lt;- convert(crime_tvshows, to = \"data.frame\")\n\n#Text, der am meisten dem Genre crime entspricht\ncrime_tvshows %&gt;% \n  arrange(desc(crime)) %&gt;% \n  head()\n\n   doc_id     crime\n1 text723 0.2857143\n2 text259 0.2500000\n3 text314 0.2500000\n4 text352 0.2500000\n5 text494 0.2222222\n6 text656 0.2222222\n\ndaten_df[314,] %&gt;% \n  select(Description) %&gt;% \n  print()\n\n                                                                                                                                               Description\n314 An L.A.P.D. homicide detective works to solve the murder of a 13-year-old boy while standing trial in federal court for the murder of a serial killer."
  },
  {
    "objectID": "tutorial/04_topicmodeling.html",
    "href": "tutorial/04_topicmodeling.html",
    "title": "Sitzung 4: Topic Modeling",
    "section": "",
    "text": "#Packages laden und Objekte erstellen\n\n#install.packages(\"RCurl\")\n#install.packages(\"quanteda\")\n#install.packages(\"tidyverse)\n#install.packages(\"dplyr\")\n#install.packages(\"quanteda.textplots\")\n#install.packages(\"quanteda.textstats\")\n#install.packages(\"udpipe\")\n\nlibrary(\"RCurl\")\nlibrary(\"quanteda\")\nlibrary(\"tidyverse\")\nlibrary(\"dplyr\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"quanteda.textstats\")\nlibrary(\"udpipe\")\n\n\n#Daten laden\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv\")\ndaten_df &lt;-  read.csv2(text = url)\n\n#Tokens\nstopwords(\"english\")\n\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n\ndaten_tokens &lt;- daten_df$Description %&gt;% \n  tokens(what = \"word\",\n         remove_punct = TRUE, \n         remove_numbers = TRUE, \n         remove_url = TRUE, \n         remove_symbols = TRUE) %&gt;% \n  tokens_tolower() %&gt;% \n  tokens_remove(stopwords(\"english\")) %&gt;% \n  tokens_wordstem()\n\n#dfm\ndaten_dfm &lt;- daten_tokens %&gt;% \n  dfm() %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE) \n\n\n8 Topic Modeling"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html",
    "href": "tutorial/01_einlesen_bereinigen.html",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "",
    "text": "Zun√§chst installieren alle Packete, die wir f√ºr diese Sitzung brauchten (z.B. tidyverse). Ihr braucht install.packages() nur, wenn ihr die Pakete im Methodencafe noch nicht installiert hattet-\n\n#install.packages(\"tidyverse)\n#install.packages(\"quanteda\")\n#install.packages(\"quanteda.textplots\")\n#install.packages(\"RCurl\")\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"RCurl\")\n\nDann laden wir die Datei die hinter dem Link liegt mit der Funktion getURL() aus dem package Rcurl herunter. Mit einem Blick in das Environment sehen wir, dass die einzelnen W√∂rter mit einem ; getrennt werden . Daher brauchen wir die Funktion read.csv2() aus dem utils Packet - das Packet ist vorinstalliert und immer geladen - um die Daten in R einzulesen. Der Datensatz wird im Objekt daten_df gespeichert.\n\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv\")\ndaten_df &lt;-  read.csv2(text = url)\n\nAlternativ k√∂nnen Daten in einer .csv Datei auch mit einem , voneinander abgetrennt sein. Hier br√§uchte es dann die Funktion read.csv() zum Einlesen.\nNach dem Einlesen der Daten ist es √ºblich sich zun√§chst einen √úberblick √ºber die Daten zu verschaffen und zu kontrollieren, ob alles korrekt eingelesen wurde.\n\nhead(daten_df)\n\n                Title      Year Parental.Rating Rating Number.of.Votes\n1  1. Game of Thrones 2011‚Äì2019           TV-MA    9.2            2.3M\n2     2. Breaking Bad 2008‚Äì2013           TV-MA    9.5            2.1M\n3  3. Stranger Things 2016‚Äì2025           TV-14    8.7            1.3M\n4          4. Friends 1994‚Äì2004           TV-14    8.9            1.1M\n5 5. The Walking Dead 2010‚Äì2022           TV-MA    8.1            1.1M\n6         6. Sherlock 2010‚Äì2017           TV-14    9.1              1M\n                                                                                                                                                                                                                                    Description\n1                                                                                                           Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\n2                                                                    A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine with a former student in order to secure his family's future.\n3                                                                                          When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatural forces and one strange little girl.\n4                                                                                                        Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan borough of New York City.\n5                                                                                                              Sheriff Deputy Rick Grimes wakes up from a coma to learn the world is in ruins and must lead a group of survivors to stay alive.\n6 The quirky spin on Conan Doyle's iconic sleuth pitches him as a \"high-functioning sociopath\" in modern-day London. Assisting him in his investigations: Afghanistan War vet John Watson, who's introduced to Holmes by a mutual acquaintance.\n\nstr(daten_df)\n\n'data.frame':   900 obs. of  6 variables:\n $ Title          : chr  \"1. Game of Thrones\" \"2. Breaking Bad\" \"3. Stranger Things\" \"4. Friends\" ...\n $ Year           : chr  \"2011‚Äì2019\" \"2008‚Äì2013\" \"2016‚Äì2025\" \"1994‚Äì2004\" ...\n $ Parental.Rating: chr  \"TV-MA\" \"TV-MA\" \"TV-14\" \"TV-14\" ...\n $ Rating         : num  9.2 9.5 8.7 8.9 8.1 9.1 8.1 8.6 8.3 9 ...\n $ Number.of.Votes: chr  \"2.3M\" \"2.1M\" \"1.3M\" \"1.1M\" ...\n $ Description    : chr  \"Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being do\"| __truncated__ \"A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine wi\"| __truncated__ \"When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatura\"| __truncated__ \"Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan bo\"| __truncated__ ...\n\nView(daten_df)"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#encoding-issues-checken",
    "href": "tutorial/01_einlesen_bereinigen.html#encoding-issues-checken",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.1 Encoding issues checken",
    "text": "2.1 Encoding issues checken\nNach dem Einlesen haben wir bereits einen Blick in den Datensatz geworfen. Nun schauen wir uns gezielt die Textvariable description an, um zu √ºberpr√ºfen, ob alle Zeichen richtig dargestellt werden.\n\ndaten_df %&gt;% \n   select(Description) %&gt;% \n  head()\n\n                                                                                                                                                                                                                                    Description\n1                                                                                                           Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\n2                                                                    A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine with a former student in order to secure his family's future.\n3                                                                                          When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatural forces and one strange little girl.\n4                                                                                                        Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan borough of New York City.\n5                                                                                                              Sheriff Deputy Rick Grimes wakes up from a coma to learn the world is in ruins and must lead a group of survivors to stay alive.\n6 The quirky spin on Conan Doyle's iconic sleuth pitches him as a \"high-functioning sociopath\" in modern-day London. Assisting him in his investigations: Afghanistan War vet John Watson, who's introduced to Holmes by a mutual acquaintance.\n\n\nIn diesem Fall gibt es keine Encoding issues.\nWas tun falls doch?\n\n\nBeim Einlesen das richtige Encoding mitgeben\nManuell bereinigen\n\n\nBeim manuellen Bereinigen kann die Funktion gsub() helfen, die Zeichenketten ersetzen kann. Zum Beispiel so:\n\n#string mit encoding issues\nstring &lt;- \"Sch√É¬∂ne Gr√É¬º√É¬üe aus M√É¬ºnchen!\"\nprint(string)\n\n[1] \"Sch√É¬∂ne Gr√É¬º√É\\u009fe aus M√É¬ºnchen!\"\n\n#√úberpr√ºfen, ob \"M√ºnchen\" in string vorhanden ist\ncontains_m√ºnchen &lt;- grepl(\"M√ºnchen\", string)\nprint(contains_m√ºnchen)\n\n[1] FALSE\n\n#Zeichen manuell ersetzen\nstring_bereinigt &lt;- string %&gt;% \n  gsub(pattern = \"√É¬∂\", replacement =\"√∂\") %&gt;% \n  gsub(pattern = \"√É¬º\", replacement = \"√º\") %&gt;% \n  gsub(pattern = \"√É\\u009f\", replacement = \"√ü\") \nprint(string_bereinigt)\n\n[1] \"Sch√∂ne Gr√º√üe aus M√ºnchen!\"\n\n#√úberpr√ºfen, ob \"M√ºnchen\" in string_bereinigt vorhanden ist\ncontains_m√ºnchen &lt;- grepl(\"aus\", string_bereinigt)\nprint(contains_m√ºnchen)\n\n[1] TRUE"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#tokenisierung-zahlen-urls-etc.-entfernen",
    "href": "tutorial/01_einlesen_bereinigen.html#tokenisierung-zahlen-urls-etc.-entfernen",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.2 Tokenisierung & Zahlen, URLs, etc. entfernen",
    "text": "2.2 Tokenisierung & Zahlen, URLs, etc. entfernen\nDie Funktion tokens()von quanteda erm√∂glicht es uns bei der Aufteilung von Text in tokens direkt bestimmte Zeichen zu entfernen. Hier entfernen wir Punkte, Zahlen, URLs und Symbole.\n\ndaten_tokens &lt;- tokens(daten_df$Description, what = \"word\", remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_symbols = TRUE) #wollen wir das alles entfernen?\n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"Nine\"     \"noble\"    \"families\" \"fight\"    \"for\"      \"control\" \n [7] \"over\"     \"the\"      \"lands\"    \"of\"       \"Westeros\" \"while\"   \n[ ... and 9 more ]\n\ntext2 :\n [1] \"A\"             \"chemistry\"     \"teacher\"       \"diagnosed\"    \n [5] \"with\"          \"inoperable\"    \"lung\"          \"cancer\"       \n [9] \"turns\"         \"to\"            \"manufacturing\" \"and\"          \n[ ... and 13 more ]\n\ntext3 :\n [1] \"When\"      \"a\"         \"young\"     \"boy\"       \"vanishes\"  \"a\"        \n [7] \"small\"     \"town\"      \"uncovers\"  \"a\"         \"mystery\"   \"involving\"\n[ ... and 10 more ]"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#anpassung-auf-kleinschreibung",
    "href": "tutorial/01_einlesen_bereinigen.html#anpassung-auf-kleinschreibung",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.3 Anpassung auf Kleinschreibung",
    "text": "2.3 Anpassung auf Kleinschreibung\nMit der Funktion tokens_tolower()aus dem quanteda Packet k√∂nnen alle Buchstaben in Kleinbuchstaben umgeformt werden.\n\ndaten_tokens &lt;- tokens_tolower(daten_tokens)\n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"     \"noble\"    \"families\" \"fight\"    \"for\"      \"control\" \n [7] \"over\"     \"the\"      \"lands\"    \"of\"       \"westeros\" \"while\"   \n[ ... and 9 more ]\n\ntext2 :\n [1] \"a\"             \"chemistry\"     \"teacher\"       \"diagnosed\"    \n [5] \"with\"          \"inoperable\"    \"lung\"          \"cancer\"       \n [9] \"turns\"         \"to\"            \"manufacturing\" \"and\"          \n[ ... and 13 more ]\n\ntext3 :\n [1] \"when\"      \"a\"         \"young\"     \"boy\"       \"vanishes\"  \"a\"        \n [7] \"small\"     \"town\"      \"uncovers\"  \"a\"         \"mystery\"   \"involving\"\n[ ... and 10 more ]"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#stoppw√∂rter-entfernen",
    "href": "tutorial/01_einlesen_bereinigen.html#stoppw√∂rter-entfernen",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.4 Stoppw√∂rter entfernen",
    "text": "2.4 Stoppw√∂rter entfernen\nEs gibt verschiedene M√∂glichkeiten, Stoppw√∂rter zu entfernen. Am einfachsten ist dies mithilfe der in quanteda integrierten Stoppwortlisten m√∂glich. Diese sind in mehreren Sprachen verf√ºgbar, darunter auch Deutsch.\n\nstopwords(\"english\")\n\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n\ndaten_tokens &lt;- tokens_remove(daten_tokens, stopwords(\"english\"))\n\nJe nach Forschungsfrage k√∂nnen Stoppwortlisten angepasst werden, indem W√∂rter entfernt oder hinzugef√ºgt werden. Es ist aber auch m√∂glich eine eigene Liste zu erstellen.\n\n#W√∂rter aus der quanteda Stoppwortliste entfernen\nstoppw√∂rter &lt;- stopwords(\"english\")\nstoppw√∂rter &lt;- stoppw√∂rter[!stoppw√∂rter %in% c(\"i\", \"me\")]\n\n#W√∂rter der quanteda Stoppwortliste hinzuf√ºgen\nstoppw√∂rter &lt;- c(stoppw√∂rter, \"i\", \"me\")\n\n#Eigene Liste erstellen\neigene_stoppw√∂rter &lt;- c(\"hier\", \"eigene\", \"stoppw√∂rter\")"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#vereinheitlichung",
    "href": "tutorial/01_einlesen_bereinigen.html#vereinheitlichung",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.5 Vereinheitlichung",
    "text": "2.5 Vereinheitlichung\nOft gibt es W√∂rter, die unterschiedliche Abk√ºrzungen oder Schreibweisen haben. Nehmen wir das Beispiel der Europ√§ischen Union, die auch mit EU oder E.U. abgek√ºrzt wird. Mit Hilfe der Funktion gsub() k√∂nnen wir strings mit anderen strings ersetzen.\n\nstring &lt;- \"Bei den EU Wahlen k√∂nnen alle B√ºrger*innen der Europ√§ischen Union w√§hlen gehen.\"\nstring &lt;- gsub(\"Europ√§ischen Union\", \"EU\", string)\nprint(string)\n\n[1] \"Bei den EU Wahlen k√∂nnen alle B√ºrger*innen der EU w√§hlen gehen.\""
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#stemming",
    "href": "tutorial/01_einlesen_bereinigen.html#stemming",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.6 Stemming",
    "text": "2.6 Stemming\nMit der Funktion tokens_wordstem()aus quanteda reduzieren wir alle tokens auf ihren Wortstamm.\n\ndaten_tokens &lt;- daten_tokens %&gt;% \n  tokens_wordstem() \n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"      \"nobl\"      \"famili\"    \"fight\"     \"control\"   \"land\"     \n [7] \"westero\"   \"ancient\"   \"enemi\"     \"return\"    \"dormant\"   \"millennia\"\n\ntext2 :\n [1] \"chemistri\"      \"teacher\"        \"diagnos\"        \"inoper\"        \n [5] \"lung\"           \"cancer\"         \"turn\"           \"manufactur\"    \n [9] \"sell\"           \"methamphetamin\" \"former\"         \"student\"       \n[ ... and 4 more ]\n\ntext3 :\n [1] \"young\"      \"boy\"        \"vanish\"     \"small\"      \"town\"      \n [6] \"uncov\"      \"mysteri\"    \"involv\"     \"secret\"     \"experi\"    \n[11] \"terrifi\"    \"supernatur\"\n[ ... and 5 more ]"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#document-feature-matrix",
    "href": "tutorial/01_einlesen_bereinigen.html#document-feature-matrix",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.7 Document-Feature-Matrix",
    "text": "2.7 Document-Feature-Matrix\nUm aus unseren tokens eine dfm zu machen nutzen wir die dfm()Funktion aus dem quanteda package.\n\ndaten_dfm &lt;- daten_tokens %&gt;% \n  dfm()"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#selteneh√§ufige-features-entfernen",
    "href": "tutorial/01_einlesen_bereinigen.html#selteneh√§ufige-features-entfernen",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.8 Seltene/h√§ufige features entfernen",
    "text": "2.8 Seltene/h√§ufige features entfernen\nIm letzten Schritt des Preprocessings entfernen wir h√§ufig und selten vorkommende features aus der dfm. Das geht mit der Funktion dfm_trim()aus dem quanteda Packet.\nEs k√∂nnen unterschiedliche thresholds gesetzt werden - hier lassen wir nur features in der dfm die mindestens in 0.5% und h√∂chstens in 99% der Dokumente vorkommen. Das Argument docfreq_type = \"prop\"berechnet den Anteil der Dokumente, die ein bestimmtes feature beinhalten relativ zur Gesamtzahl der Dokumente. verbose = TRUEprinted w√§hrend der Ausf√ºhrung der Funktion Informationen √ºber den Rechenvorgang in die Konsole.\n\ndaten_dfm &lt;- daten_dfm %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE)"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#word-cloud-erster-blick-in-die-daten",
    "href": "tutorial/01_einlesen_bereinigen.html#word-cloud-erster-blick-in-die-daten",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.9 Word cloud: Erster Blick in die Daten",
    "text": "2.9 Word cloud: Erster Blick in die Daten\nF√ºr einen ersten Einblick in die Daten lassen wir uns mit der topfeatures()Funktion aus dem quanteda Packet die 10 am h√§ufigsten vorkommenden features ausgeben.\n\ndaten_dfm %&gt;% \n  topfeatures(n = 10)\n\n  live   life famili    new  world  young follow friend   find   seri \n   108    108    107    103     75     74     74     70     69     65 \n\n\nDas Ergebnis k√∂nnen wir mit einer word cloud visualisieren. Hierf√ºr nutzen wir die textplot_wordcloud()Funktion aus dem quanteda.textplots Packet.\n\nword_cloud &lt;- daten_dfm %&gt;% \n  textplot_wordcloud(max_words = 100)\n\n\n\n\n√úbung: mit emoji einleiten ‚Äútest your knowledge‚Äù mit anderem Datensatz, der nicht zu gro√ü ist"
  }
]