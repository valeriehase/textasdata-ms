[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Automatisierte Inhaltsanalyse",
    "section": "",
    "text": "Quelle: Foto von AltumCode auf Unsplash"
  },
  {
    "objectID": "index.html#infos-zum-workshop",
    "href": "index.html#infos-zum-workshop",
    "title": "Automatisierte Inhaltsanalyse",
    "section": "Infos zum Workshop",
    "text": "Infos zum Workshop\n\nMethodenworkshop am Institut f√ºr Kommunikationswissenschaft, Universit√§t M√ºnster\nüìÖ 24-25. Juli 2024\nWorkshop-Leitung:\n\nValerie Hase (Ludwig-Maximilians-Universit√§t M√ºnchen). Mehr Infos: github.com/valeriehase & valerie-hase.com\nUnterst√ºtzung durch Teaching Assistant Luisa Kutlar (Ludwig-Maximilians-Universit√§t M√ºnchen). Mehr Infos: github.com/luisakutlar"
  },
  {
    "objectID": "index.html#materialien",
    "href": "index.html#materialien",
    "title": "Automatisierte Inhaltsanalyse",
    "section": "Materialien",
    "text": "Materialien\n\nDaten\n\nDatensatz 1: IMDb Top-Rated TV Series Dataset. Verf√ºgbar unter MIT Lizenz via Kaggle.\n\n\n\n Hier geht es zum Download des TV-Datasets\n\n\n\nDatensatz 2: Dataset of Indian Newspaper Horoscopes. Verf√ºgbar unter CC BY-NC-SA 4.0 Lizenz via Kaggle, Autor: Aman Bhargava.\n\n\n\n Hier geht es zum Download des Horoskop-Datasets\n\n\n\n\nFolien & R-Code\nSitzung 1Ô∏è‚É£: Einf√ºhrung in die automatisierte Inhaltsanalyse & Preprocessing\n\n Folien\n Tutorial\n R-Code\n\n\nSitzung 2Ô∏è‚É£: Co-Occurence-Analysen\n\n Folien\n Tutorial\n R-Code\n\n\nSitzung 3Ô∏è‚É£: Diktion√§re\n\nFolien\nTutorial\n\nSitzung 4Ô∏è‚É£: Topic Modeling\n\nFolien\nTutorial\n\nSitzung 5Ô∏è‚É£: Qualit√§tskriterien\n\nFolien\nTutorial\n\nSitzung 6Ô∏è‚É£: Ausblick\n\nFolien\nTutorial"
  },
  {
    "objectID": "index.html#weiterf√ºhrende-tutorials",
    "href": "index.html#weiterf√ºhrende-tutorials",
    "title": "Automatisierte Inhaltsanalyse",
    "section": "Weiterf√ºhrende Tutorials",
    "text": "Weiterf√ºhrende Tutorials\n\nBail, C. Day 3: Automated Text Analysis. Link\nBernauer J, & Traber D. Quantitative Analysis of Political Text. Link\nHase, V. (2022). Text as Data Methods in R. Link\nHase, V. (2023). Advanced Text Analysis. Link\nSanchez, G. (2014). Handling Strings with R. Link\nSilge, J., & Robinson, D. Text mining with R: A tidy approach. Link\nPuschmann, C., & Haim, R. Automated Content Analysis with R. Link\nUnkel, J. (2020). Methodische Vertiefung: Computational Methods mit R und R Studio. Link\nWatanabe, K., & M√ºller, S (2023). Quanteda Tutorials. Link"
  },
  {
    "objectID": "index.html#weiterf√ºhrende-literatur",
    "href": "index.html#weiterf√ºhrende-literatur",
    "title": "Automatisierte Inhaltsanalyse",
    "section": "Weiterf√ºhrende Literatur",
    "text": "Weiterf√ºhrende Literatur\n\nBaden, C., Pipal, C., Schoonvelde, M., & Van Der Velden, M. A. C. G. (2022). Three Gaps in Computational Text Analysis Methods for Social Sciences: A Research Agenda. Communication Methods and Measures, 16(1), 1‚Äì18. https://doi.org/10.1080/19312458.2021.2015574\nBenoit, K. (2019). Text as data: An overview. In Cuirini, L., & Franzese, R. (Eds.), Handbook of Research Methods in Political Science and International Relations. Preprint\nBoumans, J. W., & Trilling, D. (2016). Taking Stock of the Toolkit: An overview of relevant automated content analysis approaches and techniques for digital journalism scholars. Digital Journalism, 4(1), 8‚Äì23. https://doi.org/10.1080/21670811.2015.1096598\nGrimmer, J., Roberts, M. E., & Stewart, B. M. (2022). Text as data: A new framework for machine learning and the social sciences. Princeton University Press.\nG√ºnther, E., & Quandt, T. (2016). Word Counts and Topic Models: Automated text analysis methods for digital journalism research. Digital Journalism, 4(1), 75‚Äì88. https://doi.org/10.1080/21670811.2015.1093270\nHaim, M. (2023). Computational Communication Science: Eine Einf√ºhrung. Springer Fachmedien Wiesbaden. https://doi.org/10.1007/978-3-658-40171-9\nHase, V. (2023). Automated Content Analysis. In F. Oehmer, S. H. Kessler, E. Humprecht, K. Sommer, & L. Castro Herrero (eds.),¬†Handbook of Standardized Content Analysis: Applied Designs to Research Fields of Communication Science. VS Springer (pp.¬†23‚Äì36).¬†https://doi.org/10.1007/978-3-658-36179-2_3\nJ√ºnger, J., & G√§rtner, C. (2023). Computational Methods f√ºr die Sozial- und Geisteswissenschaften. Springer Fachmedien Wiesbaden. https://doi.org/10.1007/978-3-658-37747-2\nQuinn, K. M., Monroe, B. L., Colaresi, M., Crespin, M. H., & Radev, D. R. (2010). How to Analyze Political Attention with Minimal Assumptions and Costs. American Journal of Political Science, 54(1), 209‚Äì228. https://doi.org/10.1111/j.1540-5907.2009.00427.x\nAtteveldt, W. van, Trilling, D., & Arc√≠la Calder√≥n, C. (2022). Computational analysis of communication: A practical introduction to the analysis of texts, networks, and images with code examples in Python and R. Wiley Blackwell.\nWilkerson, J., & Casas, A. (2017). Large-Scale Computerized Text Analysis in Political Science: Opportunities and Challenges. Annual Review of Political Science, 20(1), 529‚Äì544. https://www.annualreviews.org/doi/10.1146/annurev-polisci-052615-025542"
  },
  {
    "objectID": "01-begr√ºssung.html",
    "href": "01-begr√ºssung.html",
    "title": "\nAutomatisierte Inhaltsanalyse\n",
    "section": "",
    "text": "Automatisierte Inhaltsanalyse\nSitzung 1Ô∏è‚É£: Begr√º√üung\nLeitung: Valerie Hase (Ludwig-Maximilians-Universit√§t M√ºnchen)\nüëâ github.com/valeriehase & valerie-hase.com\nTeaching Assistant: Luisa Kutlar (Ludwig-Maximilians-Universit√§t M√ºnchen)\nüëâ github.com/luisakutlar"
  },
  {
    "objectID": "01-begr√ºssung.html#wer-seid-ihr",
    "href": "01-begr√ºssung.html#wer-seid-ihr",
    "title": "Automatisierte Inhaltsanalyse_ Workshop",
    "section": "Wer seid ihr?",
    "text": "Wer seid ihr?\nBitte die Hand heben ü§ö, wenn ihr ‚Ä¶.\n\n\nmit automatisierter Inhaltsanalyse gearbeitet habt\nR regelm√§√üig nutzt\nandere Programmiersprachen (z. B. Python) regelm√§√üig nutzt"
  },
  {
    "objectID": "01-begr√ºssung.html#wer-sind-wir",
    "href": "01-begr√ºssung.html#wer-sind-wir",
    "title": "Automatisierte Inhaltsanalyse_ Workshop",
    "section": "Wer sind wir?",
    "text": "Wer sind wir?"
  },
  {
    "objectID": "01-begr√ºssung.html#vielen-dank-f√ºr-die-organisation",
    "href": "01-begr√ºssung.html#vielen-dank-f√ºr-die-organisation",
    "title": "Automatisierte Inhaltsanalyse_ Workshop",
    "section": "Vielen Dank f√ºr die Organisation üôå",
    "text": "Vielen Dank f√ºr die Organisation üôå\nShoutout an: Mittelbaunetzwerk Journalismusforschung & Mittelbaunetzwerk Wissenschaftskommunikation\n\nJulia Metag\nFranca Singh\nJakon J√ºnger"
  },
  {
    "objectID": "01-begr√ºssung.html#worum-geht-es-heute",
    "href": "01-begr√ºssung.html#worum-geht-es-heute",
    "title": "Automatisierte Inhaltsanalyse_ Workshop",
    "section": "Worum geht es heute?",
    "text": "Worum geht es heute?\n\n\n‚úÖ A\n‚ùå D"
  },
  {
    "objectID": "01-begr√ºssung.html#ablauf",
    "href": "01-begr√ºssung.html#ablauf",
    "title": "Automatisierte Inhaltsanalyse_ Workshop",
    "section": "Ablauf",
    "text": "Ablauf\n\nFragen? ü§î\n\n\n\n\nAutomatisierte Inhaltsanalyse - Workshop Universit√§t M√ºnster, Juli 2024"
  },
  {
    "objectID": "01-begr√ºssung.html#section",
    "href": "01-begr√ºssung.html#section",
    "title": "Automatisierte Inhaltsanalyse_ Workshop",
    "section": "",
    "text": "Fragen? ü§î\n\n\n\n\nAutomatisierte Inhaltsanalyse - Workshop Universit√§t M√ºnster, Juli 2024"
  },
  {
    "objectID": "tutorial/sitzung2.html",
    "href": "tutorial/sitzung2.html",
    "title": "Sitzung 2",
    "section": "",
    "text": "Paket ‚Äúreadtext‚Äù installieren\nWenn ihr beim Workshop kein Internet habt, k√∂nnt ihr das Paket auch weglassen.\n\ninstall.packages(\"readtext\")\n\n\n\nPakete ‚Äúaktivieren‚Äù\nAls n√§chstes laden wir √ºber die folgenden Befehle die Pakete, mit denen wir Daten einlesen und bereinigen werden.\n\nlibrary(\"readtext\")"
  },
  {
    "objectID": "tutorial/Sitzung2_EinlesenBereinigung.html",
    "href": "tutorial/Sitzung2_EinlesenBereinigung.html",
    "title": "Session 2: Einlesen & Bereinigen von Text",
    "section": "",
    "text": "Paket ‚Äúreadtext‚Äù installieren\nWenn ihr beim Workshop kein Internet habt, k√∂nnt ihr das Paket auch weglassen.\n\ninstall.packages(\"readtext\")\n\n\n\nPakete ‚Äúaktivieren‚Äù\nAls n√§chstes laden wir √ºber die folgenden Befehle die Pakete, mit denen wir Daten einlesen und bereinigen werden.\n\nlibrary(\"readtext\")"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html",
    "href": "tutorial/02-einlesen-bereinigen.html",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "",
    "text": "Zun√§chst installieren und laden wir alle Packete, die wir heute und morgen brauchen.\n\n#install.packages(\"RCurl\")\n#install.packages(\"quanteda\")\n#install.packages(\"tidyverse)\n#install.packages(\"dplyr\")\n#install.packages(\"quanteda.textplots\")\n#install.packages(\"quanteda.textstats\")\n#install.packages(\"udpipe\")\n\nlibrary(\"RCurl\")\nlibrary(\"quanteda\")\nlibrary(\"tidyverse\")\nlibrary(\"dplyr\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"quanteda.textstats\")\nlibrary(\"udpipe\")\n\nDann laden wir die Datei die hinter dem Link liegt mit der Funktion getURL() aus dem package Rcurl herunter. Mit einem Blick in das Environment sehen wir, dass die einzelnen W√∂rter mit einem ; getrennt werden . Daher brauchen wir die Funktion read.csv2() aus dem utils Packet - das Packet ist vorinstalliert und immer geladen - um die Daten in R einzulesen. Der Datensatz wird im Objekt daten_df gespeichert.\n\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv\")\ndaten_df &lt;-  read.csv2(text = url)\n\nAlternativ k√∂nnen Daten in einer .csv Datei auch mit einem , voneinander abgetrennt sein. Hier br√§uchte es dann die Funktion read.csv() zum Einlesen.\nNach dem Einlesen der Daten ist es √ºblich sich zun√§chst einen √úberblick √ºber die Daten zu verschaffen und zu kontrollieren, ob alles korrekt eingelesen wurde.\n\nhead(daten_df)\n\n                Title      Year Parental.Rating Rating Number.of.Votes\n1  1. Game of Thrones 2011‚Äì2019           TV-MA    9.2            2.3M\n2     2. Breaking Bad 2008‚Äì2013           TV-MA    9.5            2.1M\n3  3. Stranger Things 2016‚Äì2025           TV-14    8.7            1.3M\n4          4. Friends 1994‚Äì2004           TV-14    8.9            1.1M\n5 5. The Walking Dead 2010‚Äì2022           TV-MA    8.1            1.1M\n6         6. Sherlock 2010‚Äì2017           TV-14    9.1              1M\n                                                                                                                                                                                                                                    Description\n1                                                                                                           Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\n2                                                                    A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine with a former student in order to secure his family's future.\n3                                                                                          When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatural forces and one strange little girl.\n4                                                                                                        Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan borough of New York City.\n5                                                                                                              Sheriff Deputy Rick Grimes wakes up from a coma to learn the world is in ruins and must lead a group of survivors to stay alive.\n6 The quirky spin on Conan Doyle's iconic sleuth pitches him as a \"high-functioning sociopath\" in modern-day London. Assisting him in his investigations: Afghanistan War vet John Watson, who's introduced to Holmes by a mutual acquaintance.\n\nstr(daten_df)\n\n'data.frame':   900 obs. of  6 variables:\n $ Title          : chr  \"1. Game of Thrones\" \"2. Breaking Bad\" \"3. Stranger Things\" \"4. Friends\" ...\n $ Year           : chr  \"2011‚Äì2019\" \"2008‚Äì2013\" \"2016‚Äì2025\" \"1994‚Äì2004\" ...\n $ Parental.Rating: chr  \"TV-MA\" \"TV-MA\" \"TV-14\" \"TV-14\" ...\n $ Rating         : num  9.2 9.5 8.7 8.9 8.1 9.1 8.1 8.6 8.3 9 ...\n $ Number.of.Votes: chr  \"2.3M\" \"2.1M\" \"1.3M\" \"1.1M\" ...\n $ Description    : chr  \"Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being do\"| __truncated__ \"A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine wi\"| __truncated__ \"When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatura\"| __truncated__ \"Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan bo\"| __truncated__ ...\n\nView(daten_df)"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#preprocessing-1",
    "href": "tutorial/02-einlesen-bereinigen.html#preprocessing-1",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "Preprocessing",
    "text": "Preprocessing"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#encoding-issues-checken",
    "href": "tutorial/02-einlesen-bereinigen.html#encoding-issues-checken",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.1 Encoding issues checken",
    "text": "2.1 Encoding issues checken\nNach dem Einlesen haben wir bereits einen Blick in den Datensatz geworfen. Nun schauen wir uns gezielt die Textvariable description an, um zu √ºberpr√ºfen, ob alle Zeichen richtig dargestellt werden.\n\ndaten_df %&gt;% \n   select(Description) %&gt;% \n  head()\n\n                                                                                                                                                                                                                                    Description\n1                                                                                                           Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\n2                                                                    A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine with a former student in order to secure his family's future.\n3                                                                                          When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatural forces and one strange little girl.\n4                                                                                                        Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan borough of New York City.\n5                                                                                                              Sheriff Deputy Rick Grimes wakes up from a coma to learn the world is in ruins and must lead a group of survivors to stay alive.\n6 The quirky spin on Conan Doyle's iconic sleuth pitches him as a \"high-functioning sociopath\" in modern-day London. Assisting him in his investigations: Afghanistan War vet John Watson, who's introduced to Holmes by a mutual acquaintance.\n\n\nIn diesem Fall gibt es keine Encoding issues.\nWas tun falls doch?\n\n\nBeim Einlesen das richtige Encoding mitgeben\nManuell bereinigen\n\n\nBeim manuellen Bereinigen kann die Funktion gsub() helfen, die Zeichenketten ersetzen kann. Zum Beispiel so:\n\n#string mit encoding issues\nstring &lt;- \"Sch√É¬∂ne Gr√É¬º√É¬üe aus M√É¬ºnchen!\"\nprint(string)\n\n[1] \"Sch√É¬∂ne Gr√É¬º√É\\u009fe aus M√É¬ºnchen!\"\n\n#√úberpr√ºfen, ob \"M√ºnchen\" in string vorhanden ist\ncontains_m√ºnchen &lt;- grepl(\"M√ºnchen\", string)\nprint(contains_m√ºnchen)\n\n[1] FALSE\n\n#Zeichen manuell ersetzen\nstring_bereinigt &lt;- string %&gt;% \n  gsub(pattern = \"√É¬∂\", replacement =\"√∂\") %&gt;% \n  gsub(pattern = \"√É¬º\", replacement = \"√º\") %&gt;% \n  gsub(pattern = \"√É\\u009f\", replacement = \"√ü\") \nprint(string_bereinigt)\n\n[1] \"Sch√∂ne Gr√º√üe aus M√ºnchen!\"\n\n#√úberpr√ºfen, ob \"M√ºnchen\" in string_bereinigt vorhanden ist\ncontains_m√ºnchen &lt;- grepl(\"aus\", string_bereinigt)\nprint(contains_m√ºnchen)\n\n[1] TRUE"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#datenbereinigung",
    "href": "tutorial/02-einlesen-bereinigen.html#datenbereinigung",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "Datenbereinigung",
    "text": "Datenbereinigung"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#normalisierung",
    "href": "tutorial/02-einlesen-bereinigen.html#normalisierung",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.3 Normalisierung",
    "text": "2.3 Normalisierung\nMit der Funktion tokens_tolower()aus dem quanteda Packet k√∂nnen alle Buchstaben in Kleinbuchstaben umgeformt werden.\n\ndaten_tokens &lt;- tokens_tolower(daten_tokens)\n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"     \"noble\"    \"families\" \"fight\"    \"for\"      \"control\" \n [7] \"over\"     \"the\"      \"lands\"    \"of\"       \"westeros\" \"while\"   \n[ ... and 9 more ]\n\ntext2 :\n [1] \"a\"             \"chemistry\"     \"teacher\"       \"diagnosed\"    \n [5] \"with\"          \"inoperable\"    \"lung\"          \"cancer\"       \n [9] \"turns\"         \"to\"            \"manufacturing\" \"and\"          \n[ ... and 13 more ]\n\ntext3 :\n [1] \"when\"      \"a\"         \"young\"     \"boy\"       \"vanishes\"  \"a\"        \n [7] \"small\"     \"town\"      \"uncovers\"  \"a\"         \"mystery\"   \"involving\"\n[ ... and 10 more ]"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#tokenisierung-zahlen-urls-etc.-entfernen",
    "href": "tutorial/02-einlesen-bereinigen.html#tokenisierung-zahlen-urls-etc.-entfernen",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.2 Tokenisierung & Zahlen, URLs, etc. entfernen",
    "text": "2.2 Tokenisierung & Zahlen, URLs, etc. entfernen\nDie Funktion tokens()von quanteda erm√∂glicht es uns bei der Aufteilung von Text in tokens direkt bestimmte Zeichen zu entfernen. Hier entfernen wir Punkte, Zahlen, URLs und Symbole.\n\ndaten_tokens &lt;- tokens(daten_df$Description, what = \"word\", remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_symbols = TRUE) #wollen wir das alles entfernen?\n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"Nine\"     \"noble\"    \"families\" \"fight\"    \"for\"      \"control\" \n [7] \"over\"     \"the\"      \"lands\"    \"of\"       \"Westeros\" \"while\"   \n[ ... and 9 more ]\n\ntext2 :\n [1] \"A\"             \"chemistry\"     \"teacher\"       \"diagnosed\"    \n [5] \"with\"          \"inoperable\"    \"lung\"          \"cancer\"       \n [9] \"turns\"         \"to\"            \"manufacturing\" \"and\"          \n[ ... and 13 more ]\n\ntext3 :\n [1] \"When\"      \"a\"         \"young\"     \"boy\"       \"vanishes\"  \"a\"        \n [7] \"small\"     \"town\"      \"uncovers\"  \"a\"         \"mystery\"   \"involving\"\n[ ... and 10 more ]"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#stopw√∂rter-entfernen",
    "href": "tutorial/02-einlesen-bereinigen.html#stopw√∂rter-entfernen",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.4 Stopw√∂rter entfernen",
    "text": "2.4 Stopw√∂rter entfernen"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#vereinheitlichung",
    "href": "tutorial/02-einlesen-bereinigen.html#vereinheitlichung",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.5 Vereinheitlichung",
    "text": "2.5 Vereinheitlichung\nOft gibt es W√∂rter, die unterschiedliche Abk√ºrzungen oder Schreibweisen haben. Nehmen wir das Beispiel der Europ√§ischen Union, die auch mit EU oder E.U. abgek√ºrzt wird. Mit Hilfe der Funktion gsub() k√∂nnen wir strings mit anderen strings ersetzen.\n\nstring &lt;- \"Bei den EU Wahlen k√∂nnen alle B√ºrger*innen der Europ√§ischen Union w√§hlen gehen.\"\nstring &lt;- gsub(\"Europ√§ischen Union\", \"EU\", string)\nprint(string)\n\n[1] \"Bei den EU Wahlen k√∂nnen alle B√ºrger*innen der EU w√§hlen gehen.\""
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#lemmatizingstemming",
    "href": "tutorial/02-einlesen-bereinigen.html#lemmatizingstemming",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.6 Lemmatizing/Stemming",
    "text": "2.6 Lemmatizing/Stemming"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#selteneh√§ufige-features-entfernen",
    "href": "tutorial/02-einlesen-bereinigen.html#selteneh√§ufige-features-entfernen",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.8 Seltene/h√§ufige features entfernen",
    "text": "2.8 Seltene/h√§ufige features entfernen\nIm letzten Schritt des Preprocessings entfernen wir h√§ufig und selten vorkommende features aus der dfm. Das geht mit der Funktion dfm_trim()aus dem quanteda Packet.\nEs k√∂nnen unterschiedliche thresholds gesetzt werden - hier lassen wir nur features in der dfm die mindestens in 0.5% und h√∂chstens in 99% der Dokumente vorkommen. Das Argument docfreq_type = \"prop\"berechnet den Anteil der Dokumente, die ein bestimmtes feature beinhalten relativ zur Gesamtzahl der Dokumente. verbose = TRUEprinted w√§hrend der Ausf√ºhrung der Funktion Informationen √ºber den Rechenvorgang in die Konsole.\n\ndaten_dfm &lt;- daten_dfm %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE)"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#stoppw√∂rter-entfernen",
    "href": "tutorial/02-einlesen-bereinigen.html#stoppw√∂rter-entfernen",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.4 Stoppw√∂rter entfernen",
    "text": "2.4 Stoppw√∂rter entfernen\nEs gibt verschiedene M√∂glichkeiten, Stoppw√∂rter zu entfernen. Am einfachsten ist dies mithilfe der in quanteda integrierten Stoppwortlisten m√∂glich. Diese sind in mehreren Sprachen verf√ºgbar, darunter auch Deutsch.\n\nstopwords(\"english\")\n\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n\ndaten_tokens &lt;- tokens_remove(daten_tokens, stopwords(\"english\"))\n\nJe nach Forschungsfrage k√∂nnen Stoppwortlisten angepasst werden, indem W√∂rter entfernt oder hinzugef√ºgt werden. Es ist aber auch m√∂glich eine eigene Liste zu erstellen.\n\n#W√∂rter aus der quanteda Stoppwortliste entfernen\nstoppw√∂rter &lt;- stopwords(\"english\")\nstoppw√∂rter &lt;- stoppw√∂rter[!stoppw√∂rter %in% c(\"i\", \"me\")]\n\n#W√∂rter der quanteda Stoppwortliste hinzuf√ºgen\nstoppw√∂rter &lt;- c(stoppw√∂rter, \"i\", \"me\")\n\n#Eigene Liste erstellen\neigene_stoppw√∂rter &lt;- c(\"hier\", \"eigene\", \"stoppw√∂rter\")"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#lemmatizing",
    "href": "tutorial/02-einlesen-bereinigen.html#lemmatizing",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.6 Lemmatizing",
    "text": "2.6 Lemmatizing\nMit der Funktion tokens_wordstem()aus quanteda reduzieren wir alle tokens auf ihren Wortstamm.\n\ndaten_tokens &lt;- daten_tokens %&gt;% \n  tokens_wordstem() \n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"      \"nobl\"      \"famili\"    \"fight\"     \"control\"   \"land\"     \n [7] \"westero\"   \"ancient\"   \"enemi\"     \"return\"    \"dormant\"   \"millennia\"\n\ntext2 :\n [1] \"chemistri\"      \"teacher\"        \"diagnos\"        \"inoper\"        \n [5] \"lung\"           \"cancer\"         \"turn\"           \"manufactur\"    \n [9] \"sell\"           \"methamphetamin\" \"former\"         \"student\"       \n[ ... and 4 more ]\n\ntext3 :\n [1] \"young\"      \"boy\"        \"vanish\"     \"small\"      \"town\"      \n [6] \"uncov\"      \"mysteri\"    \"involv\"     \"secret\"     \"experi\"    \n[11] \"terrifi\"    \"supernatur\"\n[ ... and 5 more ]"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#document-feature-matrix",
    "href": "tutorial/02-einlesen-bereinigen.html#document-feature-matrix",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.7 Document-Feature-Matrix",
    "text": "2.7 Document-Feature-Matrix\nUm aus unseren tokens eine dfm zu machen nutzen wir die dfm()Funktion aus dem quanteda package.\n\ndaten_dfm &lt;- daten_tokens %&gt;% \n  dfm()"
  },
  {
    "objectID": "index.html#zeitplan",
    "href": "index.html#zeitplan",
    "title": "Automatisierte Inhaltsanalyse",
    "section": "Zeitplan",
    "text": "Zeitplan\nüìÖ Mi, 24. Juli\n\n09:00 - 12:00: 1Ô∏è‚É£ Einf√ºhrung & Preprocessing\n12:00 - 13:00: ü•ó Mittagspause\n13:00 - 15:00: 2Ô∏è‚É£ Co-Occurence-Analysen\n15:00 - 17:00: 3Ô∏è‚É£ Diktion√§re\n\nüìÖ Do, 25. Juli\n\n09:00 - 12:00: 4Ô∏è‚É£ Topic Modeling\n12:00 - 13:00: ü•ó Mittagspause\n13:00 - 15:00: 5Ô∏è‚É£ Qualit√§tskriterien\n15:00 - 16:00: 6Ô∏è‚É£ Ausblick"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#off-the-shelf-diktion√§re",
    "href": "tutorial/02-einlesen-bereinigen.html#off-the-shelf-diktion√§re",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "7.1 Off-the-shelf Diktion√§re",
    "text": "7.1 Off-the-shelf Diktion√§re\nEs gibt viele off-the-shelf Diktion√§re. Der Einfachkeit halber nutzen wir hier zur Demonstation den data_dictionary_LSD2015aus dem quanteda Packet (Young & Soroka, 2012). #Soll hier noch ein Disclaimer wegen Validit√§t und Auswahl Diktion√§r etc. hin?\n\ndiktion√§r &lt;- data_dictionary_LSD2015\n\ndiktion√§r %&gt;% \n  head()\n\nDictionary object with 4 key entries.\n- [negative]:\n  - a lie, abandon*, abas*, abattoir*, abdicat*, aberra*, abhor*, abject*, abnormal*, abolish*, abominab*, abominat*, abrasiv*, absent*, abstrus*, absurd*, abus*, accident*, accost*, accursed* [ ... and 2,838 more ]\n- [positive]:\n  - ability*, abound*, absolv*, absorbent*, absorption*, abundanc*, abundant*, acced*, accentuat*, accept*, accessib*, acclaim*, acclamation*, accolad*, accommodat*, accomplish*, accord, accordan*, accorded*, accords [ ... and 1,689 more ]\n- [neg_positive]:\n  - best not, better not, no damag*, no no, not ability*, not able, not abound*, not absolv*, not absorbent*, not absorption*, not abundanc*, not abundant*, not acced*, not accentuat*, not accept*, not accessib*, not acclaim*, not acclamation*, not accolad*, not accommodat* [ ... and 1,701 more ]\n- [neg_negative]:\n  - not a lie, not abandon*, not abas*, not abattoir*, not abdicat*, not aberra*, not abhor*, not abject*, not abnormal*, not abolish*, not abominab*, not abominat*, not abrasiv*, not absent*, not abstrus*, not absurd*, not abus*, not accident*, not accost*, not accursed* [ ... and 2,840 more ]\n\n?data_dictionary_LSD2015\n\nNun wollen wir den dictionary auf unsere Daten anwenden. Diese m√ºssen daf√ºr im dfm-Format sein. Mit der Funktion dfm_lookup() aus dem quanteda Packet wird f√ºr jeden Text, also in diesem Fall f√ºr jede TV Show, geschaut, wie viele W√∂rter aus den ersten zwei Spalten des Diktion√§rs vorkommen. Die Funktion dfm_weight(scheme = \"prop\")setzt die Anzahl der dictionary W√∂rter ins Verh√§ltnis mit der L√§nge des Textes.\n\nsentiment_tvshows &lt;-  daten_dfm %&gt;% \n  dfm_weight(scheme = \"prop\") %&gt;% \n  dfm_lookup(dictionary = data_dictionary_LSD2015[1:2])\n\nsentiment_tvshows %&gt;% \n  head()\n\nDocument-feature matrix of: 6 documents, 2 features (50.00% sparse) and 0 docvars.\n       features\ndocs      negative   positive\n  text1 0.28571429 0         \n  text2 0          0         \n  text3 0.06666667 0         \n  text4 0          0.09090909\n  text5 0          0.22222222\n  text6 0.16666667 0.16666667"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#eigene-diktion√§re",
    "href": "tutorial/02-einlesen-bereinigen.html#eigene-diktion√§re",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "7.2 Eigene Diktion√§re",
    "text": "7.2 Eigene Diktion√§re"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#anpassung-auf-kleinschreibung",
    "href": "tutorial/02-einlesen-bereinigen.html#anpassung-auf-kleinschreibung",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.3 Anpassung auf Kleinschreibung",
    "text": "2.3 Anpassung auf Kleinschreibung\nMit der Funktion tokens_tolower()aus dem quanteda Packet k√∂nnen alle Buchstaben in Kleinbuchstaben umgeformt werden.\n\ndaten_tokens &lt;- tokens_tolower(daten_tokens)\n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"     \"noble\"    \"families\" \"fight\"    \"for\"      \"control\" \n [7] \"over\"     \"the\"      \"lands\"    \"of\"       \"westeros\" \"while\"   \n[ ... and 9 more ]\n\ntext2 :\n [1] \"a\"             \"chemistry\"     \"teacher\"       \"diagnosed\"    \n [5] \"with\"          \"inoperable\"    \"lung\"          \"cancer\"       \n [9] \"turns\"         \"to\"            \"manufacturing\" \"and\"          \n[ ... and 13 more ]\n\ntext3 :\n [1] \"when\"      \"a\"         \"young\"     \"boy\"       \"vanishes\"  \"a\"        \n [7] \"small\"     \"town\"      \"uncovers\"  \"a\"         \"mystery\"   \"involving\"\n[ ... and 10 more ]"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#stemming",
    "href": "tutorial/02-einlesen-bereinigen.html#stemming",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.6 Stemming",
    "text": "2.6 Stemming\nMit der Funktion tokens_wordstem()aus quanteda reduzieren wir alle tokens auf ihren Wortstamm.\n\ndaten_tokens &lt;- daten_tokens %&gt;% \n  tokens_wordstem() \n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"      \"nobl\"      \"famili\"    \"fight\"     \"control\"   \"land\"     \n [7] \"westero\"   \"ancient\"   \"enemi\"     \"return\"    \"dormant\"   \"millennia\"\n\ntext2 :\n [1] \"chemistri\"      \"teacher\"        \"diagnos\"        \"inoper\"        \n [5] \"lung\"           \"cancer\"         \"turn\"           \"manufactur\"    \n [9] \"sell\"           \"methamphetamin\" \"former\"         \"student\"       \n[ ... and 4 more ]\n\ntext3 :\n [1] \"young\"      \"boy\"        \"vanish\"     \"small\"      \"town\"      \n [6] \"uncov\"      \"mysteri\"    \"involv\"     \"secret\"     \"experi\"    \n[11] \"terrifi\"    \"supernatur\"\n[ ... and 5 more ]"
  },
  {
    "objectID": "tutorial/02-einlesen-bereinigen.html#word-cloud-erster-blick-in-die-daten",
    "href": "tutorial/02-einlesen-bereinigen.html#word-cloud-erster-blick-in-die-daten",
    "title": "Sitzung 2 - Einlesen & Bereinigen von Text",
    "section": "2.9 Word cloud: Erster Blick in die Daten",
    "text": "2.9 Word cloud: Erster Blick in die Daten\nF√ºr einen ersten Einblick in die Daten lassen wir uns mit der topfeatures()Funktion aus dem quanteda Packet die 10 am h√§ufigsten vorkommenden features ausgeben.\n\ndaten_dfm %&gt;% \n  topfeatures(n = 10)\n\n  live   life famili    new  world  young follow friend   find   seri \n   108    108    107    103     75     74     74     70     69     65 \n\n\nDas Ergebnis k√∂nnen wir mit einer word cloud visualisieren. Hierf√ºr nutzen wir die textplot_wordcloud()Funktion aus dem quanteda.textplots Packet.\n\nword_cloud &lt;- daten_dfm %&gt;% \n  textplot_wordcloud(max_words = 100)\n\n\n\n\n√úbung: mit emoji einleiten ‚Äútest your knowledge‚Äù mit anderem Datensatz, der nicht zu gro√ü ist"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html",
    "href": "tutorial/Tutorial_gesamt_alt.html",
    "title": "Gesamter Code Stand 15.07",
    "section": "",
    "text": "Zun√§chst installieren und laden wir alle Packete, die wir heute und morgen brauchen.\n\n#install.packages(\"RCurl\")\n#install.packages(\"quanteda\")\n#install.packages(\"tidyverse)\n#install.packages(\"dplyr\")\n#install.packages(\"quanteda.textplots\")\n#install.packages(\"quanteda.textstats\")\n#install.packages(\"udpipe\")\n\nlibrary(\"RCurl\")\nlibrary(\"quanteda\")\nlibrary(\"tidyverse\")\nlibrary(\"dplyr\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"quanteda.textstats\")\nlibrary(\"udpipe\")\n\nDann laden wir die Datei die hinter dem Link liegt mit der Funktion getURL() aus dem package Rcurl herunter. Mit einem Blick in das Environment sehen wir, dass die einzelnen W√∂rter mit einem ; getrennt werden . Daher brauchen wir die Funktion read.csv2() aus dem utils Packet - das Packet ist vorinstalliert und immer geladen - um die Daten in R einzulesen. Der Datensatz wird im Objekt daten_df gespeichert.\n\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv\")\ndaten_df &lt;-  read.csv2(text = url)\n\nAlternativ k√∂nnen Daten in einer .csv Datei auch mit einem , voneinander abgetrennt sein. Hier br√§uchte es dann die Funktion read.csv() zum Einlesen.\nNach dem Einlesen der Daten ist es √ºblich sich zun√§chst einen √úberblick √ºber die Daten zu verschaffen und zu kontrollieren, ob alles korrekt eingelesen wurde.\n\nhead(daten_df)\n\n                Title      Year Parental.Rating Rating Number.of.Votes\n1  1. Game of Thrones 2011‚Äì2019           TV-MA    9.2            2.3M\n2     2. Breaking Bad 2008‚Äì2013           TV-MA    9.5            2.1M\n3  3. Stranger Things 2016‚Äì2025           TV-14    8.7            1.3M\n4          4. Friends 1994‚Äì2004           TV-14    8.9            1.1M\n5 5. The Walking Dead 2010‚Äì2022           TV-MA    8.1            1.1M\n6         6. Sherlock 2010‚Äì2017           TV-14    9.1              1M\n                                                                                                                                                                                                                                    Description\n1                                                                                                           Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\n2                                                                    A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine with a former student in order to secure his family's future.\n3                                                                                          When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatural forces and one strange little girl.\n4                                                                                                        Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan borough of New York City.\n5                                                                                                              Sheriff Deputy Rick Grimes wakes up from a coma to learn the world is in ruins and must lead a group of survivors to stay alive.\n6 The quirky spin on Conan Doyle's iconic sleuth pitches him as a \"high-functioning sociopath\" in modern-day London. Assisting him in his investigations: Afghanistan War vet John Watson, who's introduced to Holmes by a mutual acquaintance.\n\nstr(daten_df)\n\n'data.frame':   900 obs. of  6 variables:\n $ Title          : chr  \"1. Game of Thrones\" \"2. Breaking Bad\" \"3. Stranger Things\" \"4. Friends\" ...\n $ Year           : chr  \"2011‚Äì2019\" \"2008‚Äì2013\" \"2016‚Äì2025\" \"1994‚Äì2004\" ...\n $ Parental.Rating: chr  \"TV-MA\" \"TV-MA\" \"TV-14\" \"TV-14\" ...\n $ Rating         : num  9.2 9.5 8.7 8.9 8.1 9.1 8.1 8.6 8.3 9 ...\n $ Number.of.Votes: chr  \"2.3M\" \"2.1M\" \"1.3M\" \"1.1M\" ...\n $ Description    : chr  \"Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being do\"| __truncated__ \"A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine wi\"| __truncated__ \"When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatura\"| __truncated__ \"Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan bo\"| __truncated__ ...\n\nView(daten_df)"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#encoding-issues-checken",
    "href": "tutorial/Tutorial_gesamt_alt.html#encoding-issues-checken",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.1 Encoding issues checken",
    "text": "2.1 Encoding issues checken\nNach dem Einlesen haben wir bereits einen Blick in den Datensatz geworfen. Nun schauen wir uns gezielt die Textvariable description an, um zu √ºberpr√ºfen, ob alle Zeichen richtig dargestellt werden.\n\ndaten_df %&gt;% \n   select(Description) %&gt;% \n  head()\n\n                                                                                                                                                                                                                                    Description\n1                                                                                                           Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\n2                                                                    A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine with a former student in order to secure his family's future.\n3                                                                                          When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatural forces and one strange little girl.\n4                                                                                                        Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan borough of New York City.\n5                                                                                                              Sheriff Deputy Rick Grimes wakes up from a coma to learn the world is in ruins and must lead a group of survivors to stay alive.\n6 The quirky spin on Conan Doyle's iconic sleuth pitches him as a \"high-functioning sociopath\" in modern-day London. Assisting him in his investigations: Afghanistan War vet John Watson, who's introduced to Holmes by a mutual acquaintance.\n\n\nIn diesem Fall gibt es keine Encoding issues.\nWas tun falls doch?\n\n\nBeim Einlesen das richtige Encoding mitgeben\nManuell bereinigen\n\n\nBeim manuellen Bereinigen kann die Funktion gsub() helfen, die Zeichenketten ersetzen kann. Zum Beispiel so:\n\n#string mit encoding issues\nstring &lt;- \"Sch√É¬∂ne Gr√É¬º√É¬üe aus M√É¬ºnchen!\"\nprint(string)\n\n[1] \"Sch√É¬∂ne Gr√É¬º√É\\u009fe aus M√É¬ºnchen!\"\n\n#√úberpr√ºfen, ob \"M√ºnchen\" in string vorhanden ist\ncontains_m√ºnchen &lt;- grepl(\"M√ºnchen\", string)\nprint(contains_m√ºnchen)\n\n[1] FALSE\n\n#Zeichen manuell ersetzen\nstring_bereinigt &lt;- string %&gt;% \n  gsub(pattern = \"√É¬∂\", replacement =\"√∂\") %&gt;% \n  gsub(pattern = \"√É¬º\", replacement = \"√º\") %&gt;% \n  gsub(pattern = \"√É\\u009f\", replacement = \"√ü\") \nprint(string_bereinigt)\n\n[1] \"Sch√∂ne Gr√º√üe aus M√ºnchen!\"\n\n#√úberpr√ºfen, ob \"M√ºnchen\" in string_bereinigt vorhanden ist\ncontains_m√ºnchen &lt;- grepl(\"aus\", string_bereinigt)\nprint(contains_m√ºnchen)\n\n[1] TRUE"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#tokenisierung-zahlen-urls-etc.-entfernen",
    "href": "tutorial/Tutorial_gesamt_alt.html#tokenisierung-zahlen-urls-etc.-entfernen",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.2 Tokenisierung & Zahlen, URLs, etc. entfernen",
    "text": "2.2 Tokenisierung & Zahlen, URLs, etc. entfernen\nDie Funktion tokens()von quanteda erm√∂glicht es uns bei der Aufteilung von Text in tokens direkt bestimmte Zeichen zu entfernen. Hier entfernen wir Punkte, Zahlen, URLs und Symbole.\n\ndaten_tokens &lt;- tokens(daten_df$Description, what = \"word\", remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_symbols = TRUE) #wollen wir das alles entfernen?\n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"Nine\"     \"noble\"    \"families\" \"fight\"    \"for\"      \"control\" \n [7] \"over\"     \"the\"      \"lands\"    \"of\"       \"Westeros\" \"while\"   \n[ ... and 9 more ]\n\ntext2 :\n [1] \"A\"             \"chemistry\"     \"teacher\"       \"diagnosed\"    \n [5] \"with\"          \"inoperable\"    \"lung\"          \"cancer\"       \n [9] \"turns\"         \"to\"            \"manufacturing\" \"and\"          \n[ ... and 13 more ]\n\ntext3 :\n [1] \"When\"      \"a\"         \"young\"     \"boy\"       \"vanishes\"  \"a\"        \n [7] \"small\"     \"town\"      \"uncovers\"  \"a\"         \"mystery\"   \"involving\"\n[ ... and 10 more ]"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#anpassung-auf-kleinschreibung",
    "href": "tutorial/Tutorial_gesamt_alt.html#anpassung-auf-kleinschreibung",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.3 Anpassung auf Kleinschreibung",
    "text": "2.3 Anpassung auf Kleinschreibung\nMit der Funktion tokens_tolower()aus dem quanteda Packet k√∂nnen alle Buchstaben in Kleinbuchstaben umgeformt werden.\n\ndaten_tokens &lt;- tokens_tolower(daten_tokens)\n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"     \"noble\"    \"families\" \"fight\"    \"for\"      \"control\" \n [7] \"over\"     \"the\"      \"lands\"    \"of\"       \"westeros\" \"while\"   \n[ ... and 9 more ]\n\ntext2 :\n [1] \"a\"             \"chemistry\"     \"teacher\"       \"diagnosed\"    \n [5] \"with\"          \"inoperable\"    \"lung\"          \"cancer\"       \n [9] \"turns\"         \"to\"            \"manufacturing\" \"and\"          \n[ ... and 13 more ]\n\ntext3 :\n [1] \"when\"      \"a\"         \"young\"     \"boy\"       \"vanishes\"  \"a\"        \n [7] \"small\"     \"town\"      \"uncovers\"  \"a\"         \"mystery\"   \"involving\"\n[ ... and 10 more ]"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#stoppw√∂rter-entfernen",
    "href": "tutorial/Tutorial_gesamt_alt.html#stoppw√∂rter-entfernen",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.4 Stoppw√∂rter entfernen",
    "text": "2.4 Stoppw√∂rter entfernen\nEs gibt verschiedene M√∂glichkeiten, Stoppw√∂rter zu entfernen. Am einfachsten ist dies mithilfe der in quanteda integrierten Stoppwortlisten m√∂glich. Diese sind in mehreren Sprachen verf√ºgbar, darunter auch Deutsch.\n\nstopwords(\"english\")\n\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n\ndaten_tokens &lt;- tokens_remove(daten_tokens, stopwords(\"english\"))\n\nJe nach Forschungsfrage k√∂nnen Stoppwortlisten angepasst werden, indem W√∂rter entfernt oder hinzugef√ºgt werden. Es ist aber auch m√∂glich eine eigene Liste zu erstellen.\n\n#W√∂rter aus der quanteda Stoppwortliste entfernen\nstoppw√∂rter &lt;- stopwords(\"english\")\nstoppw√∂rter &lt;- stoppw√∂rter[!stoppw√∂rter %in% c(\"i\", \"me\")]\n\n#W√∂rter der quanteda Stoppwortliste hinzuf√ºgen\nstoppw√∂rter &lt;- c(stoppw√∂rter, \"i\", \"me\")\n\n#Eigene Liste erstellen\neigene_stoppw√∂rter &lt;- c(\"hier\", \"eigene\", \"stoppw√∂rter\")"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#vereinheitlichung",
    "href": "tutorial/Tutorial_gesamt_alt.html#vereinheitlichung",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.5 Vereinheitlichung",
    "text": "2.5 Vereinheitlichung\nOft gibt es W√∂rter, die unterschiedliche Abk√ºrzungen oder Schreibweisen haben. Nehmen wir das Beispiel der Europ√§ischen Union, die auch mit EU oder E.U. abgek√ºrzt wird. Mit Hilfe der Funktion gsub() k√∂nnen wir strings mit anderen strings ersetzen.\n\nstring &lt;- \"Bei den EU Wahlen k√∂nnen alle B√ºrger*innen der Europ√§ischen Union w√§hlen gehen.\"\nstring &lt;- gsub(\"Europ√§ischen Union\", \"EU\", string)\nprint(string)\n\n[1] \"Bei den EU Wahlen k√∂nnen alle B√ºrger*innen der EU w√§hlen gehen.\""
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#stemming",
    "href": "tutorial/Tutorial_gesamt_alt.html#stemming",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.6 Stemming",
    "text": "2.6 Stemming\nMit der Funktion tokens_wordstem()aus quanteda reduzieren wir alle tokens auf ihren Wortstamm.\n\ndaten_tokens &lt;- daten_tokens %&gt;% \n  tokens_wordstem() \n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"      \"nobl\"      \"famili\"    \"fight\"     \"control\"   \"land\"     \n [7] \"westero\"   \"ancient\"   \"enemi\"     \"return\"    \"dormant\"   \"millennia\"\n\ntext2 :\n [1] \"chemistri\"      \"teacher\"        \"diagnos\"        \"inoper\"        \n [5] \"lung\"           \"cancer\"         \"turn\"           \"manufactur\"    \n [9] \"sell\"           \"methamphetamin\" \"former\"         \"student\"       \n[ ... and 4 more ]\n\ntext3 :\n [1] \"young\"      \"boy\"        \"vanish\"     \"small\"      \"town\"      \n [6] \"uncov\"      \"mysteri\"    \"involv\"     \"secret\"     \"experi\"    \n[11] \"terrifi\"    \"supernatur\"\n[ ... and 5 more ]"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#document-feature-matrix",
    "href": "tutorial/Tutorial_gesamt_alt.html#document-feature-matrix",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.7 Document-Feature-Matrix",
    "text": "2.7 Document-Feature-Matrix\nUm aus unseren tokens eine dfm zu machen nutzen wir die dfm()Funktion aus dem quanteda package.\n\ndaten_dfm &lt;- daten_tokens %&gt;% \n  dfm()"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#selteneh√§ufige-features-entfernen",
    "href": "tutorial/Tutorial_gesamt_alt.html#selteneh√§ufige-features-entfernen",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.8 Seltene/h√§ufige features entfernen",
    "text": "2.8 Seltene/h√§ufige features entfernen\nIm letzten Schritt des Preprocessings entfernen wir h√§ufig und selten vorkommende features aus der dfm. Das geht mit der Funktion dfm_trim()aus dem quanteda Packet.\nEs k√∂nnen unterschiedliche thresholds gesetzt werden - hier lassen wir nur features in der dfm die mindestens in 0.5% und h√∂chstens in 99% der Dokumente vorkommen. Das Argument docfreq_type = \"prop\"berechnet den Anteil der Dokumente, die ein bestimmtes feature beinhalten relativ zur Gesamtzahl der Dokumente. verbose = TRUEprinted w√§hrend der Ausf√ºhrung der Funktion Informationen √ºber den Rechenvorgang in die Konsole.\n\ndaten_dfm &lt;- daten_dfm %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE)"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#word-cloud-erster-blick-in-die-daten",
    "href": "tutorial/Tutorial_gesamt_alt.html#word-cloud-erster-blick-in-die-daten",
    "title": "Gesamter Code Stand 15.07",
    "section": "2.9 Word cloud: Erster Blick in die Daten",
    "text": "2.9 Word cloud: Erster Blick in die Daten\nF√ºr einen ersten Einblick in die Daten lassen wir uns mit der topfeatures()Funktion aus dem quanteda Packet die 10 am h√§ufigsten vorkommenden features ausgeben.\n\ndaten_dfm %&gt;% \n  topfeatures(n = 10)\n\n  live   life famili    new  young follow  world friend   find   seri \n   108    108    107    103     74     74     74     70     69     65 \n\n\nDas Ergebnis k√∂nnen wir mit einer word cloud visualisieren. Hierf√ºr nutzen wir die textplot_wordcloud()Funktion aus dem quanteda.textplots Packet.\n\nword_cloud &lt;- daten_dfm %&gt;% \n  textplot_wordcloud(max_words = 100)\n\n\n\n\n\n\n\n\n√úbung: mit emoji einleiten ‚Äútest your knowledge‚Äù mit anderem Datensatz, der nicht zu gro√ü ist"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#off-the-shelf-diktion√§re",
    "href": "tutorial/Tutorial_gesamt_alt.html#off-the-shelf-diktion√§re",
    "title": "Gesamter Code Stand 15.07",
    "section": "7.1 Off-the-shelf Diktion√§re",
    "text": "7.1 Off-the-shelf Diktion√§re\nEs gibt viele off-the-shelf Diktion√§re. Der Einfachkeit halber nutzen wir hier zur Demonstation den data_dictionary_LSD2015aus dem quanteda Packet (Young & Soroka, 2012).\n‚Äì Soll hier noch ein Disclaimer wegen Validit√§t und Auswahl Diktion√§r etc. hin?\n\ndiktion√§r &lt;- data_dictionary_LSD2015\n\ndiktion√§r %&gt;% \n  head()\n\nDictionary object with 4 key entries.\n- [negative]:\n  - a lie, abandon*, abas*, abattoir*, abdicat*, aberra*, abhor*, abject*, abnormal*, abolish*, abominab*, abominat*, abrasiv*, absent*, abstrus*, absurd*, abus*, accident*, accost*, accursed* [ ... and 2,838 more ]\n- [positive]:\n  - ability*, abound*, absolv*, absorbent*, absorption*, abundanc*, abundant*, acced*, accentuat*, accept*, accessib*, acclaim*, acclamation*, accolad*, accommodat*, accomplish*, accord, accordan*, accorded*, accords [ ... and 1,689 more ]\n- [neg_positive]:\n  - best not, better not, no damag*, no no, not ability*, not able, not abound*, not absolv*, not absorbent*, not absorption*, not abundanc*, not abundant*, not acced*, not accentuat*, not accept*, not accessib*, not acclaim*, not acclamation*, not accolad*, not accommodat* [ ... and 1,701 more ]\n- [neg_negative]:\n  - not a lie, not abandon*, not abas*, not abattoir*, not abdicat*, not aberra*, not abhor*, not abject*, not abnormal*, not abolish*, not abominab*, not abominat*, not abrasiv*, not absent*, not abstrus*, not absurd*, not abus*, not accident*, not accost*, not accursed* [ ... and 2,840 more ]\n\n?data_dictionary_LSD2015\n\nNun wollen wir den Diktion√§r auf unsere Daten anwenden. Diese m√ºssen daf√ºr im dfm-Format sein. Mit der Funktion dfm_lookup() aus dem quanteda Packet wird f√ºr jeden Text, also in diesem Fall f√ºr jede TV Show, geschaut, wie viele W√∂rter aus den ersten zwei Spalten des Diktion√§rs vorkommen. Die Funktion dfm_weight(scheme = \"prop\")setzt die Anzahl der dictionary W√∂rter ins Verh√§ltnis mit der L√§nge des Textes.\n\nsentiment_tvshows &lt;-  daten_dfm %&gt;% \n  dfm_weight(scheme = \"prop\") %&gt;% \n  dfm_lookup(dictionary = data_dictionary_LSD2015[1:2])\n\nsentiment_tvshows %&gt;% \n  head()\n\nDocument-feature matrix of: 6 documents, 2 features (50.00% sparse) and 0 docvars.\n       features\ndocs      negative   positive\n  text1 0.28571429 0         \n  text2 0          0         \n  text3 0.06666667 0         \n  text4 0          0.09090909\n  text5 0          0.22222222\n  text6 0.16666667 0.16666667"
  },
  {
    "objectID": "tutorial/Tutorial_gesamt_alt.html#eigene-diktion√§re",
    "href": "tutorial/Tutorial_gesamt_alt.html#eigene-diktion√§re",
    "title": "Gesamter Code Stand 15.07",
    "section": "7.2 Eigene Diktion√§re",
    "text": "7.2 Eigene Diktion√§re"
  },
  {
    "objectID": "tutorial/02_cooccurence_kollokationen_postagging_ner.html",
    "href": "tutorial/02_cooccurence_kollokationen_postagging_ner.html",
    "title": "Sitzung 2: Co-Occurence Analyse, Kollokationen, POS-tagging und named entity recognition",
    "section": "",
    "text": "#Packages laden und Objekte erstellen\n\n#install.packages(\"RCurl\")\n#install.packages(\"quanteda\")\n#install.packages(\"tidyverse)\n#install.packages(\"dplyr\")\n#install.packages(\"quanteda.textplots\")\n#install.packages(\"quanteda.textstats\")\n#install.packages(\"udpipe\")\n\nlibrary(\"RCurl\")\nlibrary(\"quanteda\")\nlibrary(\"tidyverse\")\nlibrary(\"dplyr\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"quanteda.textstats\")\nlibrary(\"udpipe\")\n\n\n#Daten laden\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv\")\ndaten_df &lt;-  read.csv2(text = url)\n\n#Tokens\nstopwords(\"english\")\n\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n\ndaten_tokens &lt;- daten_df$Description %&gt;% \n  tokens(what = \"word\",\n         remove_punct = TRUE, \n         remove_numbers = TRUE, \n         remove_url = TRUE, \n         remove_symbols = TRUE) %&gt;% \n  tokens_tolower() %&gt;% \n  tokens_remove(stopwords(\"english\")) %&gt;% \n  tokens_wordstem()\n\n#dfm\ndaten_dfm &lt;- daten_tokens %&gt;% \n  dfm() %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE) \n\n\n3 Co-Occurrence-Analysen\nBevor wir zur Co-Occurence-Analyse kommen, gibt es auch eine Funktion, die es einem schnell und unkompliziert erm√∂glicht herauszufinden, in welchem Kontext ein Wort benutzt wird. Die hier gemeinte Funktion ist die kwic()Funktion aus dem quanteda Packet. Mit folgendem Code k√∂nnen wir beispielsweise herausfinden, in welchem Kontext das Wort hero vorkommt. In der Ausgabe werden jeweils die f√ºnf W√∂rter vor und nach dem Wort hero angezeigt.\n\ndaten_tokens %&gt;% \n  kwic(pattern = \"hero\", window = 5)\n\nKeyword-in-context with 12 matches.                                                          \n   [text75, 4]                famili former child | hero |\n  [text124, 3]                      stori saitama | hero |\n [text140, 18]      embark path destin turn rebel | hero |\n [text230, 24]        beast boy togeth becom team | hero |\n  [text241, 9]    hunter must recruit ragtag team | hero |\n [text292, 15]        know super power compound v | hero |\n  [text336, 5] superhero-admir boy enrol prestigi | hero |\n [text336, 10]     hero academi learn realli mean | hero |\n [text391, 10]      take challeng whole new level | hero |\n  [text498, 3]                       seven formid | hero |\n [text646, 11]   unit state america team everyday | hero |\n  [text756, 4]           hercul half-man half-god | hero |\n                                       \n now grown apart must reunit           \n just fun can defeat enemi             \n                                       \n                                       \n villain help prevent apocalyps impact \n put physic moral boundari test        \n academi learn realli mean hero        \n strongest superhero grant power       \n fallen apart desol dhananjay rajpoot  \n form arguabl power team ever          \n must transport known survivor plagu   \n fantast strength malevol stepmoth hera\n\n\nNun zur Co-Occurence-Analyse. Hierf√ºr m√ºssen wir zun√§chst die dfm in eine Feature Co-occurrence Matrix (fcm) umwandeln. Das machen wir mit der Funktion fcm() aus dem quanteda Packet.\n\ndaten_fcm &lt;- daten_dfm %&gt;% \n  fcm()\n\ndaten_fcm %&gt;% \n  head()\n\nFeature co-occurrence matrix of: 6 by 605 features.\n         features\nfeatures  famili fight control land ancient enemi return turn former student\n  famili       8     6       2    3       1     4      6    3      4       2\n  fight        0     1       2    2       2     2      2    1      0       2\n  control      0     0       0    1       1     1      1    0      0       0\n  land         0     0       0    0       1     1      1    0      0       1\n  ancient      0     0       0    0       0     2      1    0      0       0\n  enemi        0     0       0    0       0     0      2    0      0       0\n[ reached max_nfeat ... 595 more features ]\n\n\nBeim n√§chsten Schritt w√§hlen wir die features aus, die uns in unserer Analyse interessieren. Das machen wir mit der Funktion fcm_select()aus dem quanteda Packet.\n\ndaten_fcm &lt;- daten_fcm %&gt;% \n  fcm_select(pattern = c(\"famili\", \"crime\", \"america\", \"school\"), #hier noch bessere Begriffe vielleicht √ºber topfeatures oder topic modeling\n             selection = \"keep\")\n\nDann visualisieren wir die fcm mit der textplot_network()Funktion aus demquanteda.textplots() Paket. Wenn features im selben Dokument vorkommen, werden sie mit einer Linie verbunden. Umso dicker die Linie, desto √∂fter kommen die features miteinander vor.\n\ntextplot_network(daten_fcm)\n\n\n\n\nDiese Visualisierung gibt uns nun aber noch keine genauen Angaben dazu, wie oft ein feature mit einem anderen vorkommt. Um das herauszufinden, m√ºssen wir die fcm mit der convert()Funktion aus dem quanteda Packet in einen data frame umwandeln.\n\ndaten_fcm_df &lt;- daten_fcm %&gt;% \n  convert(to = \"data.frame\")\n\ndaten_fcm_df %&gt;% \n  head()\n\n   doc_id famili america crime school\n1  famili      8       3     3      7\n2 america      0       0     2      0\n3   crime      0       0     2      0\n4  school      0       0     0      2\n\n\nMit Hilfe von select()k√∂nnen wir uns nun einzelne H√§ufigkeiten, wie oft ein feature mit einem anderen feature vorkommt, ausgeben lassen.\n\ndaten_fcm_df %&gt;%\n  filter(doc_id == \"crime\") %&gt;% #Zeile\n  select(america) #Spalte #Frage: Wie kann es sein, dass crime america 0 ist und america crime 2?\n\n  america\n1       0\n\n\n\n\n4 Kollokationen und N-gramme\nUm herauszufinden, welche tokens oft hintereinander vorkommen, k√∂nnen wir die Funktion textstat_collocationsaus dem quanteda.textstats Packet verwenden.\n\ndaten_tokens %&gt;%\n  textstat_collocations(min_count = 10) %&gt;%\n  arrange(-lambda) %&gt;%\n  head(10)\n\n        collocation count count_nested length    lambda         z\n8         los angel    22            0      2 11.993246  7.856635\n9          new york    39            0      2  9.635906  6.744996\n5     serial killer    10            0      2  8.666634 11.850129\n4 person profession    13            0      2  7.818063 12.192676\n7     antholog seri    10            0      2  7.633523  8.612874\n1       high school    22            0      2  7.042098 16.492644\n3       best friend    25            0      2  7.006749 15.089627\n2         york citi    19            0      2  5.811625 16.073334\n6       seri follow    10            0      2  4.324217 11.364619\n\n\nIn einem weiteren Schritt kann es manchmal sinnvoll sein, Kollokationen f√ºr die Analyse zu einem token zusammenzufassen (dieser Schritt w√§re dann Teil des Preprocessings). Das l√§sst sich mit der tokens_compound()Funktion aus dem quanteda.textstats Packet umsetzen. Die Funktion verbindet die gegebenen tokens mit einem Unterstrich zu einem token.\n\nngramme &lt;- c(\"los angel\",\"new york citi\", \"serial killer\", \"high school\", \"best friend\")\ndaten_tokens_ngramme &lt;- tokens_compound(daten_tokens, pattern = phrase(ngramme))\n\nWie das nun in den Daten aussieht k√∂nnen wir mit Hilfe der kwic()Funktion aus dem quanteda Packet herausfinden (kwic steht f√ºr keywords in context).\n\ndaten_tokens_ngramme %&gt;% \n  kwic(pattern = c(\"los angel\",\"new york citi\", \"serial killer\", \"high school\", \"best friend\")) %&gt;% \n  head(n=30)\n\nKeyword-in-context with 0 matches.\n\n\n\n\n5 Parts-of-speech tagging\nF√ºr das parts of speech tagging nutzen wir das Packet UDPipe. Bevor wir unseren data frame in die udipie()Funktion geben, m√ºssen wir ihn ins tibble Format umformen und eine ID Variable erstellen. Des Weiteren bennen wir die Spalte value in text um. Das ist notwendig, weil as_tibble()die urspr√ºngliche Spalte Description in value umbenannt hat. Am Ende lassen wir uns nur einen Teil der Variablen ausgeben, damit das Ergebnis √ºbersichtlich bleibt.\n\ndaten_df_udpipe &lt;- daten_df$Description %&gt;%\n\n#Format f√ºr das udpipe Packet anpassen\n  as_tibble() %&gt;%\n  mutate(doc_id = paste0(\"text\", 1:n())) %&gt;% \n  rename(text = value) %&gt;%\n\n#Der Einfachheit halber nur f√ºr einen Text\n  slice(1) %&gt;%\n\n#part-of-speech tagging\n  udpipe(\"english\") \n\nWenn wir jetzt beispielsweise herausfinden wollen, mit welchen Adjektiven das Wort ‚Äúfamily‚Äù beschrieben wird, sieht der Code daf√ºr wie folgt aus:\n\n#Dataframe nach Nomen mit dem lemma \"family\" filtern\nadjectives_describing_family &lt;- daten_df_udpipe %&gt;%\n  filter(upos == \"NOUN\" & lemma == \"family\") %&gt;%\n\n#Den gefilterten Dataframe mit dem ungefilterten Dataframe joinen\n  inner_join(daten_df_udpipe, by = c(\"doc_id\", \"sentence_id\")) %&gt;%\n  \n#Gro√üen Dataframe nach Adjektiven filtern, die bei ihrer head_token Variable die family token_id haben\n  filter(upos.y == \"ADJ\" & head_token_id.y == token_id.x) %&gt;%\n\n#relevanten Variablen ausw√§hlen\n  select(doc_id, sentence_id, token_id = token_id.y, token = token.y, upos = upos.y)\n\nprint(adjectives_describing_family)\n\n  doc_id sentence_id token_id token upos\n1  text1           1        2 noble  ADJ\n\n\n\n\n6 Named entitiy recognition\n\n\nüë©‚ÄçüíªTeste dein Wissen\nDie folgende √úbung fasst alles zusammen, was wir bisher gelernt haben. Gehe Schritt f√ºr Schritt die Teilaufgaben durch und schaue wie weit du kommst. Den Datensatz f√ºr die √úbung findest du hier: ‚Äúhttps://raw.githubusercontent.com/valeriehase/textasdata-ms/main/data/data_horoscope.csv‚Äù\n\nLade den Datensatz und verschaffe dir einen √úberblick √ºber die Daten. Gibt es Encoding-Issues, die bereinigt werden m√ºssen?\nBereinige und Normalisiere den Datensatz. Hinterfrage kritisch welche Bereinigungsschritte es tats√§chlich braucht. Am Ende solltest du eine Document-Feature-Matrix haben.\nBei welchem Sternzeichen f√§llt am meisten das Stichwort ‚Äúwinning‚Äù?\nWas kommt √∂fter vor: ‚Äúsecret fears‚Äù oder ‚Äúin love‚Äù? #Notiz f√ºr uns: n-gramme\nFinde heraus welcher der Planeten am meisten in Zusammenhang mit Ver√§nderungen und Neuanf√§ngen genannt wird. #Notiz f√ºr uns: Co-Occurence\nWas wird den Lesenden geraten mit ihrem Geld zu machen? #Notiz f√ºr uns: POS tagging und dann Verben zu money"
  },
  {
    "objectID": "tutorial/05_qualit√§tskriterien.html",
    "href": "tutorial/05_qualit√§tskriterien.html",
    "title": "Sitzung 5: Qualit√§tskriterien",
    "section": "",
    "text": "#Packages laden und Objekte erstellen\n\n#install.packages(\"RCurl\")\n#install.packages(\"quanteda\")\n#install.packages(\"tidyverse)\n#install.packages(\"dplyr\")\n#install.packages(\"quanteda.textplots\")\n#install.packages(\"quanteda.textstats\")\n#install.packages(\"udpipe\")\n\nlibrary(\"RCurl\")\nlibrary(\"quanteda\")\nlibrary(\"tidyverse\")\nlibrary(\"dplyr\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"quanteda.textstats\")\nlibrary(\"udpipe\")\n\n\n#Daten laden\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv\")\ndaten_df &lt;-  read.csv2(text = url)\n\n#Tokens\nstopwords(\"english\")\n\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n\ndaten_tokens &lt;- daten_df$Description %&gt;% \n  tokens(what = \"word\",\n         remove_punct = TRUE, \n         remove_numbers = TRUE, \n         remove_url = TRUE, \n         remove_symbols = TRUE) %&gt;% \n  tokens_tolower() %&gt;% \n  tokens_remove(stopwords(\"english\")) %&gt;% \n  tokens_wordstem()\n\n#dfm\ndaten_dfm &lt;- daten_tokens %&gt;% \n  dfm() %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE) \n\n\n9 Qualit√§tskriterien"
  },
  {
    "objectID": "tutorial/03_diktion√§re.html",
    "href": "tutorial/03_diktion√§re.html",
    "title": "Sitzung 3: Diktion√§re",
    "section": "",
    "text": "#Packages laden und Objekte erstellen\n#install.packages(\"RCurl\")\n#install.packages(\"quanteda\")\n#install.packages(\"tidyverse)\n#install.packages(\"dplyr\")\n#install.packages(\"quanteda.textplots\")\n#install.packages(\"quanteda.textstats\")\n#install.packages(\"udpipe\")\n\nlibrary(\"RCurl\")\nlibrary(\"quanteda\")\nlibrary(\"tidyverse\")\nlibrary(\"dplyr\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"quanteda.textstats\")\nlibrary(\"udpipe\")\n#Daten laden\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv\")\ndaten_df &lt;-  read.csv2(text = url)\n\n#Tokens\nstopwords(\"english\")\n\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n\ndaten_tokens &lt;- daten_df$Description %&gt;% \n  tokens(what = \"word\",\n         remove_punct = TRUE, \n         remove_numbers = TRUE, \n         remove_url = TRUE, \n         remove_symbols = TRUE) %&gt;% \n  tokens_tolower() %&gt;% \n  tokens_remove(stopwords(\"english\")) %&gt;% \n  tokens_wordstem()\n\n#dfm\ndaten_dfm &lt;- daten_tokens %&gt;% \n  dfm() %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE)"
  },
  {
    "objectID": "tutorial/03_diktion√§re.html#off-the-shelf-diktion√§re",
    "href": "tutorial/03_diktion√§re.html#off-the-shelf-diktion√§re",
    "title": "Sitzung 3: Diktion√§re",
    "section": "7.1 Off-the-shelf Diktion√§re",
    "text": "7.1 Off-the-shelf Diktion√§re\nEs gibt viele off-the-shelf Diktion√§re. Der Einfachkeit halber nutzen wir hier zur Demonstation den data_dictionary_LSD2015aus dem quanteda Packet (Young & Soroka, 2012).\n\ndiktion√§r &lt;- data_dictionary_LSD2015\n\ndiktion√§r %&gt;% \n  head()\n\nDictionary object with 4 key entries.\n- [negative]:\n  - a lie, abandon*, abas*, abattoir*, abdicat*, aberra*, abhor*, abject*, abnormal*, abolish*, abominab*, abominat*, abrasiv*, absent*, abstrus*, absurd*, abus*, accident*, accost*, accursed* [ ... and 2,838 more ]\n- [positive]:\n  - ability*, abound*, absolv*, absorbent*, absorption*, abundanc*, abundant*, acced*, accentuat*, accept*, accessib*, acclaim*, acclamation*, accolad*, accommodat*, accomplish*, accord, accordan*, accorded*, accords [ ... and 1,689 more ]\n- [neg_positive]:\n  - best not, better not, no damag*, no no, not ability*, not able, not abound*, not absolv*, not absorbent*, not absorption*, not abundanc*, not abundant*, not acced*, not accentuat*, not accept*, not accessib*, not acclaim*, not acclamation*, not accolad*, not accommodat* [ ... and 1,701 more ]\n- [neg_negative]:\n  - not a lie, not abandon*, not abas*, not abattoir*, not abdicat*, not aberra*, not abhor*, not abject*, not abnormal*, not abolish*, not abominab*, not abominat*, not abrasiv*, not absent*, not abstrus*, not absurd*, not abus*, not accident*, not accost*, not accursed* [ ... and 2,840 more ]\n\n?data_dictionary_LSD2015\n\nNun wollen wir den Diktion√§r auf unsere Daten anwenden. Diese m√ºssen daf√ºr im dfm-Format sein. Mit der Funktion dfm_lookup() aus dem quanteda Packet wird f√ºr jeden Text, also in diesem Fall f√ºr jede TV Show, geschaut, wie viele W√∂rter aus den ersten zwei Spalten des Diktion√§rs vorkommen. Die Funktion dfm_weight(scheme = \"prop\")setzt die Anzahl der dictionary W√∂rter ins Verh√§ltnis mit der L√§nge des Textes.\n\nsentiment_tvshows &lt;-  daten_dfm %&gt;% \n  dfm_weight(scheme = \"prop\") %&gt;% \n  dfm_lookup(dictionary = data_dictionary_LSD2015[1:2])\n\nsentiment_tvshows %&gt;% \n  head()\n\nDocument-feature matrix of: 6 documents, 2 features (50.00% sparse) and 0 docvars.\n       features\ndocs      negative   positive\n  text1 0.28571429 0         \n  text2 0          0         \n  text3 0.06666667 0         \n  text4 0          0.09090909\n  text5 0          0.22222222\n  text6 0.16666667 0.16666667\n\n\nEine erste Auswertung kann dann beispielsweise so aussehen:\n\n#S4 Objekt in einen data frame umwandeln\nsentiment_tvshows &lt;- convert(sentiment_tvshows, to = \"data.frame\")\n\n#Anzahl Texte, die mindestens ein negatives Wort haben\nsentiment_tvshows %&gt;% \n  filter(negative != 0) %&gt;% \n  count()\n\n    n\n1 436\n\n#Negativit√§t in Summe\nsentiment_tvshows$negative %&gt;% \n  sum()\n\n[1] 73.02149\n\n#Durschnittlicher negativ-Wert bei negativen Texten\n(sentiment_tvshows$negative %&gt;% \n  sum())/(sentiment_tvshows %&gt;% \n  filter(negative != 0) %&gt;% \n  count())\n\n          n\n1 0.1674805\n\n#Negativster Text\nsentiment_tvshows %&gt;% \n  filter(negative != 0) %&gt;% \n  arrange(desc(negative)) %&gt;% \n  head()\n\n   doc_id  negative  positive\n1 text329 0.6000000 0.1000000\n2 text687 0.5000000 0.0000000\n3 text888 0.5000000 0.2500000\n4 text253 0.4444444 0.2222222\n5 text444 0.4285714 0.1428571\n6 text723 0.4285714 0.0000000\n\ndaten_df[329,] %&gt;% \n  select(Description) %&gt;% \n  print()\n\n                                                                                                                                                                                                Description\n329 Ash has spent the last thirty years avoiding responsibility, maturity, and the terrors of the Evil Dead until a Deadite plague threatens to destroy all of mankind and Ash becomes mankind's only hope."
  },
  {
    "objectID": "tutorial/03_diktion√§re.html#eigene-diktion√§re",
    "href": "tutorial/03_diktion√§re.html#eigene-diktion√§re",
    "title": "Sitzung 3: Diktion√§re",
    "section": "7.2 Eigene Diktion√§re",
    "text": "7.2 Eigene Diktion√§re\nZum Teil kann es sinnvoller sein einen eigenen Diktion√§r zu nutzen. So eine Wortliste zu erstellen geht sehr einfach:\n\ndiktion√§r_crime &lt;- dictionary(list(crime = c(\"crime\", \"police\", \"gun\", \"shot\", \"dead\", \"murder\", \"kill\", \"court\", \"suspect\", \"witness\", \"arrest\", \"officer\", \"verdict\")))\n\nDie Anwendung und Auswertung bleibt gleich:\n\n#Diktion√§r anwenden\ncrime_tvshows &lt;-  daten_dfm %&gt;% \n  dfm_weight(scheme = \"prop\") %&gt;% \n  dfm_lookup(dictionary = diktion√§r_crime)\n\n#S4 Objekt in einen data frame umwandeln \ncrime_tvshows &lt;- convert(crime_tvshows, to = \"data.frame\")\n\n#Text, der am meisten dem Genre crime entspricht\ncrime_tvshows %&gt;% \n  arrange(desc(crime)) %&gt;% \n  head()\n\n   doc_id     crime\n1 text723 0.2857143\n2 text259 0.2500000\n3 text314 0.2500000\n4 text352 0.2500000\n5 text494 0.2222222\n6 text656 0.2222222\n\n## FRAGE: wenn ich es in RStudio ausf√ºhre, dann hat text314 mit einem Wert von 0.333 am meisten crime W√∂rter. Bei der html Seite ist text314 aber nur auf Platz 3 und hat dort einen Wert von 0.250...\n\ndaten_df[314,] %&gt;% \n  select(Description) %&gt;% \n  print()\n\n                                                                                                                                               Description\n314 An L.A.P.D. homicide detective works to solve the murder of a 13-year-old boy while standing trial in federal court for the murder of a serial killer."
  },
  {
    "objectID": "tutorial/04_topicmodeling.html",
    "href": "tutorial/04_topicmodeling.html",
    "title": "Sitzung 4: Topic Modeling",
    "section": "",
    "text": "#Packages laden und Objekte erstellen\n\n#install.packages(\"RCurl\")\n#install.packages(\"quanteda\")\n#install.packages(\"tidyverse)\n#install.packages(\"dplyr\")\n#install.packages(\"quanteda.textplots\")\n#install.packages(\"quanteda.textstats\")\n#install.packages(\"udpipe\")\n#install.packages(\"stm\") #√ºberall erg√§nzen\n#install.packages(\"reshape2\")\n\nlibrary(\"RCurl\")\nlibrary(\"quanteda\")\nlibrary(\"tidyverse\")\nlibrary(\"dplyr\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"quanteda.textstats\")\nlibrary(\"udpipe\")\nlibrary(\"stm\")\nlibrary(\"reshape2\")\n\n\n#Daten laden\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv\")\ndaten_df &lt;-  read.csv2(text = url)\n\n#Tokens\nstopwords(\"english\")\n\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n\ndaten_tokens &lt;- daten_df$Description %&gt;% \n  tokens(what = \"word\",\n         remove_punct = TRUE, \n         remove_numbers = TRUE, \n         remove_url = TRUE, \n         remove_symbols = TRUE) %&gt;% \n  tokens_tolower() %&gt;% \n  tokens_remove(stopwords(\"english\")) %&gt;% \n  tokens_wordstem()\n\n#dfm\ndaten_dfm &lt;- daten_tokens %&gt;% \n  dfm() %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE) \n\n\n8 Topic Modeling\nZun√§chst m√ºssen wir unsere dfm in eine stm umwandeln, damit das stm Packet das Datenformat versteht. Das stm Packet brauchen wir um danach das topic model zu berechnen.\n\n#dfm in stm umwandeln\ndaten_stm &lt;- convert(daten_dfm, to = \"stm\")\n\nBevor das richtige model gerechnet wird, berechnen wir den statistical fit f√ºr unterschiedliche Ks. Beispielhaft einmal f√ºr K = 4 und f√ºr K = 6. Hierf√ºr brauchen wir nun das stm Packet, dass wir davor laden.\n\nlibrary(stm)\nstatistical_fit_46 &lt;- searchK(daten_stm$documents, daten_stm$vocab, K = c(4,6), verbose = TRUE)\n\nBeginning Spectral Initialization \n     Calculating the gram matrix...\n     Finding anchor words...\n    ....\n     Recovering initialization...\n    ......\nInitialization complete.\n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 1 (approx. per word bound = -6.342) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 2 (approx. per word bound = -6.172, relative change = 2.680e-02) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 3 (approx. per word bound = -6.117, relative change = 9.019e-03) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 4 (approx. per word bound = -6.093, relative change = 3.785e-03) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 5 (approx. per word bound = -6.082, relative change = 1.912e-03) \nTopic 1: life, new, find, year, crime \n Topic 2: famili, live, citi, secret, one \n Topic 3: young, follow, seri, friend, becom \n Topic 4: world, mysteri, two, set, team \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 6 (approx. per word bound = -6.075, relative change = 1.105e-03) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 7 (approx. per word bound = -6.071, relative change = 7.021e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 8 (approx. per word bound = -6.068, relative change = 4.777e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 9 (approx. per word bound = -6.066, relative change = 3.427e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 10 (approx. per word bound = -6.064, relative change = 2.566e-04) \nTopic 1: life, new, find, year, crime \n Topic 2: famili, live, citi, secret, one \n Topic 3: young, follow, seri, friend, becom \n Topic 4: world, mysteri, two, power, set \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 11 (approx. per word bound = -6.063, relative change = 1.990e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 12 (approx. per word bound = -6.062, relative change = 1.589e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 13 (approx. per word bound = -6.061, relative change = 1.299e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 14 (approx. per word bound = -6.061, relative change = 1.084e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 15 (approx. per word bound = -6.060, relative change = 9.204e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: famili, live, citi, secret, one \n Topic 3: young, follow, seri, friend, becom \n Topic 4: world, mysteri, two, power, set \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 16 (approx. per word bound = -6.060, relative change = 7.961e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 17 (approx. per word bound = -6.059, relative change = 7.018e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 18 (approx. per word bound = -6.059, relative change = 6.311e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 19 (approx. per word bound = -6.058, relative change = 5.799e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 20 (approx. per word bound = -6.058, relative change = 5.438e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, seri, friend, becom \n Topic 4: world, mysteri, two, power, set \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 21 (approx. per word bound = -6.058, relative change = 5.193e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 22 (approx. per word bound = -6.057, relative change = 5.051e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 23 (approx. per word bound = -6.057, relative change = 4.963e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 24 (approx. per word bound = -6.057, relative change = 4.904e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 25 (approx. per word bound = -6.057, relative change = 4.836e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, set \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 26 (approx. per word bound = -6.056, relative change = 4.740e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 27 (approx. per word bound = -6.056, relative change = 4.610e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 28 (approx. per word bound = -6.056, relative change = 4.448e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 29 (approx. per word bound = -6.055, relative change = 4.271e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 30 (approx. per word bound = -6.055, relative change = 4.099e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, set \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 31 (approx. per word bound = -6.055, relative change = 3.931e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 32 (approx. per word bound = -6.055, relative change = 3.774e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 33 (approx. per word bound = -6.055, relative change = 3.633e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 34 (approx. per word bound = -6.054, relative change = 3.492e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 35 (approx. per word bound = -6.054, relative change = 3.346e-05) \nTopic 1: new, life, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, group \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 36 (approx. per word bound = -6.054, relative change = 3.196e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 37 (approx. per word bound = -6.054, relative change = 3.036e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 38 (approx. per word bound = -6.054, relative change = 2.880e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 39 (approx. per word bound = -6.053, relative change = 2.738e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 40 (approx. per word bound = -6.053, relative change = 2.629e-05) \nTopic 1: new, life, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, group \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 41 (approx. per word bound = -6.053, relative change = 2.559e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 42 (approx. per word bound = -6.053, relative change = 2.534e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 43 (approx. per word bound = -6.053, relative change = 2.549e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 44 (approx. per word bound = -6.053, relative change = 2.594e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 45 (approx. per word bound = -6.052, relative change = 2.657e-05) \nTopic 1: new, life, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, group \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 46 (approx. per word bound = -6.052, relative change = 2.731e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 47 (approx. per word bound = -6.052, relative change = 2.815e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 48 (approx. per word bound = -6.052, relative change = 2.907e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 49 (approx. per word bound = -6.052, relative change = 2.985e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 50 (approx. per word bound = -6.052, relative change = 3.027e-05) \nTopic 1: new, life, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, group \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 51 (approx. per word bound = -6.051, relative change = 3.003e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 52 (approx. per word bound = -6.051, relative change = 2.913e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 53 (approx. per word bound = -6.051, relative change = 2.776e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 54 (approx. per word bound = -6.051, relative change = 2.643e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 55 (approx. per word bound = -6.051, relative change = 2.550e-05) \nTopic 1: new, life, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, group \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 56 (approx. per word bound = -6.051, relative change = 2.495e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 57 (approx. per word bound = -6.050, relative change = 2.478e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 58 (approx. per word bound = -6.050, relative change = 2.485e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 59 (approx. per word bound = -6.050, relative change = 2.509e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 60 (approx. per word bound = -6.050, relative change = 2.541e-05) \nTopic 1: new, life, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, group \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 61 (approx. per word bound = -6.050, relative change = 2.585e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 62 (approx. per word bound = -6.050, relative change = 2.638e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 63 (approx. per word bound = -6.050, relative change = 2.714e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 64 (approx. per word bound = -6.049, relative change = 2.812e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 65 (approx. per word bound = -6.049, relative change = 2.942e-05) \nTopic 1: new, life, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, group \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 66 (approx. per word bound = -6.049, relative change = 3.100e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 67 (approx. per word bound = -6.049, relative change = 3.275e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 68 (approx. per word bound = -6.049, relative change = 3.442e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 69 (approx. per word bound = -6.048, relative change = 3.579e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 70 (approx. per word bound = -6.048, relative change = 3.681e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, group \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 71 (approx. per word bound = -6.048, relative change = 3.761e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 72 (approx. per word bound = -6.048, relative change = 3.848e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 73 (approx. per word bound = -6.047, relative change = 3.963e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 74 (approx. per word bound = -6.047, relative change = 4.117e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 75 (approx. per word bound = -6.047, relative change = 4.294e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, group \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 76 (approx. per word bound = -6.047, relative change = 4.427e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 77 (approx. per word bound = -6.046, relative change = 4.468e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 78 (approx. per word bound = -6.046, relative change = 4.406e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 79 (approx. per word bound = -6.046, relative change = 4.270e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 80 (approx. per word bound = -6.046, relative change = 4.135e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, group \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 81 (approx. per word bound = -6.045, relative change = 4.058e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 82 (approx. per word bound = -6.045, relative change = 4.056e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 83 (approx. per word bound = -6.045, relative change = 4.109e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 84 (approx. per word bound = -6.045, relative change = 4.160e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 85 (approx. per word bound = -6.044, relative change = 4.204e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, group \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 86 (approx. per word bound = -6.044, relative change = 4.216e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 87 (approx. per word bound = -6.044, relative change = 4.220e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 88 (approx. per word bound = -6.044, relative change = 4.244e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 89 (approx. per word bound = -6.043, relative change = 4.298e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 90 (approx. per word bound = -6.043, relative change = 4.360e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, group \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 91 (approx. per word bound = -6.043, relative change = 4.404e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 92 (approx. per word bound = -6.043, relative change = 4.412e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 93 (approx. per word bound = -6.042, relative change = 4.365e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 94 (approx. per word bound = -6.042, relative change = 4.280e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 95 (approx. per word bound = -6.042, relative change = 4.159e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, group \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 96 (approx. per word bound = -6.042, relative change = 4.029e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 97 (approx. per word bound = -6.041, relative change = 3.888e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 98 (approx. per word bound = -6.041, relative change = 3.748e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 99 (approx. per word bound = -6.041, relative change = 3.598e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 100 (approx. per word bound = -6.041, relative change = 3.442e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, group \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 101 (approx. per word bound = -6.040, relative change = 3.284e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 102 (approx. per word bound = -6.040, relative change = 3.143e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 103 (approx. per word bound = -6.040, relative change = 3.042e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 104 (approx. per word bound = -6.040, relative change = 2.983e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 105 (approx. per word bound = -6.040, relative change = 2.993e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, group \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 106 (approx. per word bound = -6.040, relative change = 3.054e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 107 (approx. per word bound = -6.039, relative change = 3.150e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 108 (approx. per word bound = -6.039, relative change = 3.257e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 109 (approx. per word bound = -6.039, relative change = 3.340e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 110 (approx. per word bound = -6.039, relative change = 3.375e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, group \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 111 (approx. per word bound = -6.039, relative change = 3.360e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 112 (approx. per word bound = -6.038, relative change = 3.315e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 113 (approx. per word bound = -6.038, relative change = 3.243e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 114 (approx. per word bound = -6.038, relative change = 3.181e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 115 (approx. per word bound = -6.038, relative change = 3.132e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, group \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 116 (approx. per word bound = -6.038, relative change = 3.098e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 117 (approx. per word bound = -6.037, relative change = 3.081e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 118 (approx. per word bound = -6.037, relative change = 3.074e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 119 (approx. per word bound = -6.037, relative change = 3.074e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 120 (approx. per word bound = -6.037, relative change = 3.066e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 121 (approx. per word bound = -6.037, relative change = 3.065e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 122 (approx. per word bound = -6.036, relative change = 3.049e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 123 (approx. per word bound = -6.036, relative change = 3.032e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 124 (approx. per word bound = -6.036, relative change = 3.009e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 125 (approx. per word bound = -6.036, relative change = 2.986e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 126 (approx. per word bound = -6.036, relative change = 2.967e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 127 (approx. per word bound = -6.036, relative change = 2.964e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 128 (approx. per word bound = -6.035, relative change = 2.980e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 129 (approx. per word bound = -6.035, relative change = 3.021e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 130 (approx. per word bound = -6.035, relative change = 3.079e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 131 (approx. per word bound = -6.035, relative change = 3.134e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 132 (approx. per word bound = -6.035, relative change = 3.173e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 133 (approx. per word bound = -6.034, relative change = 3.175e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 134 (approx. per word bound = -6.034, relative change = 3.145e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 135 (approx. per word bound = -6.034, relative change = 3.090e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 136 (approx. per word bound = -6.034, relative change = 3.024e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 137 (approx. per word bound = -6.034, relative change = 2.966e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 138 (approx. per word bound = -6.034, relative change = 2.918e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 139 (approx. per word bound = -6.033, relative change = 2.892e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 140 (approx. per word bound = -6.033, relative change = 2.879e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 141 (approx. per word bound = -6.033, relative change = 2.878e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 142 (approx. per word bound = -6.033, relative change = 2.896e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 143 (approx. per word bound = -6.033, relative change = 2.936e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 144 (approx. per word bound = -6.032, relative change = 3.003e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 145 (approx. per word bound = -6.032, relative change = 3.098e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 146 (approx. per word bound = -6.032, relative change = 3.217e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 147 (approx. per word bound = -6.032, relative change = 3.357e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 148 (approx. per word bound = -6.032, relative change = 3.484e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 149 (approx. per word bound = -6.031, relative change = 3.582e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 150 (approx. per word bound = -6.031, relative change = 3.624e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, one \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 151 (approx. per word bound = -6.031, relative change = 3.597e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 152 (approx. per word bound = -6.031, relative change = 3.510e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 153 (approx. per word bound = -6.031, relative change = 3.383e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 154 (approx. per word bound = -6.030, relative change = 3.227e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 155 (approx. per word bound = -6.030, relative change = 3.057e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, group \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 156 (approx. per word bound = -6.030, relative change = 2.866e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 157 (approx. per word bound = -6.030, relative change = 2.652e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 158 (approx. per word bound = -6.030, relative change = 2.413e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 159 (approx. per word bound = -6.030, relative change = 2.175e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 160 (approx. per word bound = -6.030, relative change = 1.967e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, group \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 161 (approx. per word bound = -6.029, relative change = 1.807e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 162 (approx. per word bound = -6.029, relative change = 1.708e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 163 (approx. per word bound = -6.029, relative change = 1.646e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 164 (approx. per word bound = -6.029, relative change = 1.624e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 165 (approx. per word bound = -6.029, relative change = 1.636e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, secret, group \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, two, power, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 166 (approx. per word bound = -6.029, relative change = 1.679e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 167 (approx. per word bound = -6.029, relative change = 1.749e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 168 (approx. per word bound = -6.029, relative change = 1.841e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 169 (approx. per word bound = -6.029, relative change = 1.941e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 170 (approx. per word bound = -6.028, relative change = 2.046e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, group, secret \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, power, two, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 171 (approx. per word bound = -6.028, relative change = 2.131e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 172 (approx. per word bound = -6.028, relative change = 2.197e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 173 (approx. per word bound = -6.028, relative change = 2.249e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 174 (approx. per word bound = -6.028, relative change = 2.293e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 175 (approx. per word bound = -6.028, relative change = 2.316e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, group, secret \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, power, two, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 176 (approx. per word bound = -6.028, relative change = 2.333e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 177 (approx. per word bound = -6.027, relative change = 2.344e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 178 (approx. per word bound = -6.027, relative change = 2.329e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 179 (approx. per word bound = -6.027, relative change = 2.291e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 180 (approx. per word bound = -6.027, relative change = 2.214e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, group, secret \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, power, two, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 181 (approx. per word bound = -6.027, relative change = 2.105e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 182 (approx. per word bound = -6.027, relative change = 1.977e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 183 (approx. per word bound = -6.027, relative change = 1.853e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 184 (approx. per word bound = -6.027, relative change = 1.755e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 185 (approx. per word bound = -6.027, relative change = 1.675e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, group, secret \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, power, two, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 186 (approx. per word bound = -6.026, relative change = 1.621e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 187 (approx. per word bound = -6.026, relative change = 1.583e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 188 (approx. per word bound = -6.026, relative change = 1.558e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 189 (approx. per word bound = -6.026, relative change = 1.534e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 190 (approx. per word bound = -6.026, relative change = 1.507e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, group, secret \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, power, two, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 191 (approx. per word bound = -6.026, relative change = 1.478e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 192 (approx. per word bound = -6.026, relative change = 1.437e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 193 (approx. per word bound = -6.026, relative change = 1.390e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 194 (approx. per word bound = -6.026, relative change = 1.341e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 195 (approx. per word bound = -6.026, relative change = 1.283e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, group, secret \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, power, two, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 196 (approx. per word bound = -6.026, relative change = 1.241e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 197 (approx. per word bound = -6.025, relative change = 1.202e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 198 (approx. per word bound = -6.025, relative change = 1.185e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 199 (approx. per word bound = -6.025, relative change = 1.179e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 200 (approx. per word bound = -6.025, relative change = 1.192e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, group, secret \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, power, two, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 201 (approx. per word bound = -6.025, relative change = 1.225e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 202 (approx. per word bound = -6.025, relative change = 1.276e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 203 (approx. per word bound = -6.025, relative change = 1.340e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 204 (approx. per word bound = -6.025, relative change = 1.415e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 205 (approx. per word bound = -6.025, relative change = 1.495e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, group, secret \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, power, two, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 206 (approx. per word bound = -6.025, relative change = 1.574e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 207 (approx. per word bound = -6.025, relative change = 1.646e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 208 (approx. per word bound = -6.025, relative change = 1.709e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 209 (approx. per word bound = -6.024, relative change = 1.752e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 210 (approx. per word bound = -6.024, relative change = 1.788e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, group, secret \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, power, two, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 211 (approx. per word bound = -6.024, relative change = 1.815e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 212 (approx. per word bound = -6.024, relative change = 1.833e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 213 (approx. per word bound = -6.024, relative change = 1.852e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 214 (approx. per word bound = -6.024, relative change = 1.870e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 215 (approx. per word bound = -6.024, relative change = 1.896e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, group, secret \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, power, two, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 216 (approx. per word bound = -6.024, relative change = 1.927e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 217 (approx. per word bound = -6.024, relative change = 1.964e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 218 (approx. per word bound = -6.023, relative change = 2.009e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 219 (approx. per word bound = -6.023, relative change = 2.060e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 220 (approx. per word bound = -6.023, relative change = 2.111e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, group, secret \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, power, two, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 221 (approx. per word bound = -6.023, relative change = 2.156e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 222 (approx. per word bound = -6.023, relative change = 2.190e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 223 (approx. per word bound = -6.023, relative change = 2.200e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 224 (approx. per word bound = -6.023, relative change = 2.193e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 225 (approx. per word bound = -6.023, relative change = 2.155e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, group, secret \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, power, two, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 226 (approx. per word bound = -6.022, relative change = 2.099e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 227 (approx. per word bound = -6.022, relative change = 2.026e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 228 (approx. per word bound = -6.022, relative change = 1.937e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 229 (approx. per word bound = -6.022, relative change = 1.848e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 230 (approx. per word bound = -6.022, relative change = 1.758e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, group, secret \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, power, two, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 231 (approx. per word bound = -6.022, relative change = 1.682e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 232 (approx. per word bound = -6.022, relative change = 1.615e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 233 (approx. per word bound = -6.022, relative change = 1.569e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 234 (approx. per word bound = -6.022, relative change = 1.538e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 235 (approx. per word bound = -6.021, relative change = 1.509e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, group, secret \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, power, two, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 236 (approx. per word bound = -6.021, relative change = 1.491e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 237 (approx. per word bound = -6.021, relative change = 1.474e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 238 (approx. per word bound = -6.021, relative change = 1.449e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 239 (approx. per word bound = -6.021, relative change = 1.423e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 240 (approx. per word bound = -6.021, relative change = 1.385e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, group, secret \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, power, two, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 241 (approx. per word bound = -6.021, relative change = 1.348e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 242 (approx. per word bound = -6.021, relative change = 1.305e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 243 (approx. per word bound = -6.021, relative change = 1.276e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 244 (approx. per word bound = -6.021, relative change = 1.244e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 245 (approx. per word bound = -6.021, relative change = 1.218e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, group, secret \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, power, two, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 246 (approx. per word bound = -6.021, relative change = 1.192e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 247 (approx. per word bound = -6.021, relative change = 1.168e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 248 (approx. per word bound = -6.020, relative change = 1.139e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 249 (approx. per word bound = -6.020, relative change = 1.105e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 250 (approx. per word bound = -6.020, relative change = 1.066e-05) \nTopic 1: life, new, find, year, crime \n Topic 2: live, famili, citi, group, secret \n Topic 3: young, follow, friend, seri, becom \n Topic 4: world, mysteri, power, two, work \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 251 (approx. per word bound = -6.020, relative change = 1.024e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nModel Converged \nBeginning Spectral Initialization \n     Calculating the gram matrix...\n     Finding anchor words...\n    ......\n     Recovering initialization...\n    ......\nInitialization complete.\n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 1 (approx. per word bound = -6.369) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 2 (approx. per word bound = -6.202, relative change = 2.619e-02) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 3 (approx. per word bound = -6.131, relative change = 1.150e-02) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 4 (approx. per word bound = -6.095, relative change = 5.903e-03) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 5 (approx. per word bound = -6.074, relative change = 3.350e-03) \nTopic 1: famili, life, live, friend, new \n Topic 2: young, power, team, becom, murder \n Topic 3: group, take, home, deal, make \n Topic 4: must, crime, time, get, earth \n Topic 5: find, seri, new, work, human \n Topic 6: follow, set, mysteri, forc, togeth \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 6 (approx. per word bound = -6.062, relative change = 2.069e-03) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 7 (approx. per word bound = -6.053, relative change = 1.369e-03) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 8 (approx. per word bound = -6.048, relative change = 9.616e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 9 (approx. per word bound = -6.043, relative change = 7.070e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 10 (approx. per word bound = -6.040, relative change = 5.383e-04) \nTopic 1: famili, live, life, new, friend \n Topic 2: young, power, team, becom, two \n Topic 3: group, take, make, home, deal \n Topic 4: must, crime, time, get, adventur \n Topic 5: find, seri, work, human, new \n Topic 6: follow, mysteri, set, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 11 (approx. per word bound = -6.038, relative change = 4.237e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 12 (approx. per word bound = -6.035, relative change = 3.440e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 13 (approx. per word bound = -6.034, relative change = 2.872e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 14 (approx. per word bound = -6.032, relative change = 2.453e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 15 (approx. per word bound = -6.031, relative change = 2.153e-04) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, power, two, world, team \n Topic 3: group, take, make, home, crimin \n Topic 4: must, time, get, adventur, crime \n Topic 5: find, seri, work, human, detect \n Topic 6: follow, mysteri, set, forc, fight \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 16 (approx. per word bound = -6.030, relative change = 1.960e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 17 (approx. per word bound = -6.029, relative change = 1.851e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 18 (approx. per word bound = -6.028, relative change = 1.802e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 19 (approx. per word bound = -6.026, relative change = 1.783e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 20 (approx. per word bound = -6.025, relative change = 1.768e-04) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, power, world, two, team \n Topic 3: group, take, make, crimin, home \n Topic 4: must, year, time, get, adventur \n Topic 5: find, seri, work, human, investig \n Topic 6: follow, mysteri, set, fight, forc \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 21 (approx. per word bound = -6.024, relative change = 1.737e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 22 (approx. per word bound = -6.023, relative change = 1.682e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 23 (approx. per word bound = -6.022, relative change = 1.606e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 24 (approx. per word bound = -6.021, relative change = 1.511e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 25 (approx. per word bound = -6.021, relative change = 1.415e-04) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, team \n Topic 3: group, take, make, crimin, home \n Topic 4: must, year, time, get, adventur \n Topic 5: find, seri, work, investig, human \n Topic 6: follow, mysteri, set, fight, forc \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 26 (approx. per word bound = -6.020, relative change = 1.332e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 27 (approx. per word bound = -6.019, relative change = 1.274e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 28 (approx. per word bound = -6.018, relative change = 1.249e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 29 (approx. per word bound = -6.018, relative change = 1.262e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 30 (approx. per word bound = -6.017, relative change = 1.300e-04) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, team \n Topic 3: group, take, make, crimin, home \n Topic 4: year, must, time, get, adventur \n Topic 5: find, seri, work, crime, investig \n Topic 6: follow, mysteri, set, fight, forc \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 31 (approx. per word bound = -6.016, relative change = 1.346e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 32 (approx. per word bound = -6.015, relative change = 1.378e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 33 (approx. per word bound = -6.014, relative change = 1.395e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 34 (approx. per word bound = -6.013, relative change = 1.419e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 35 (approx. per word bound = -6.013, relative change = 1.477e-04) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, team \n Topic 3: group, take, make, crimin, home \n Topic 4: year, must, time, get, adventur \n Topic 5: find, seri, work, crime, investig \n Topic 6: follow, mysteri, set, fight, forc \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 36 (approx. per word bound = -6.012, relative change = 1.582e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 37 (approx. per word bound = -6.011, relative change = 1.711e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 38 (approx. per word bound = -6.009, relative change = 1.786e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 39 (approx. per word bound = -6.008, relative change = 1.730e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 40 (approx. per word bound = -6.008, relative change = 1.566e-04) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, team \n Topic 3: group, take, make, crimin, home \n Topic 4: year, must, time, get, adventur \n Topic 5: find, seri, work, crime, investig \n Topic 6: follow, mysteri, set, fight, forc \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 41 (approx. per word bound = -6.007, relative change = 1.387e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 42 (approx. per word bound = -6.006, relative change = 1.236e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 43 (approx. per word bound = -6.005, relative change = 1.119e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 44 (approx. per word bound = -6.005, relative change = 1.029e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 45 (approx. per word bound = -6.004, relative change = 9.546e-05) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, team \n Topic 3: group, take, make, crimin, home \n Topic 4: year, must, time, get, adventur \n Topic 5: find, seri, work, crime, investig \n Topic 6: follow, mysteri, set, fight, forc \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 46 (approx. per word bound = -6.004, relative change = 8.899e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 47 (approx. per word bound = -6.003, relative change = 8.344e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 48 (approx. per word bound = -6.003, relative change = 7.932e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 49 (approx. per word bound = -6.002, relative change = 7.697e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 50 (approx. per word bound = -6.002, relative change = 7.674e-05) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, team \n Topic 3: group, take, make, crimin, home \n Topic 4: year, must, time, get, adventur \n Topic 5: find, seri, work, discov, crime \n Topic 6: follow, mysteri, set, fight, forc \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 51 (approx. per word bound = -6.001, relative change = 7.798e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 52 (approx. per word bound = -6.001, relative change = 7.879e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 53 (approx. per word bound = -6.000, relative change = 7.776e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 54 (approx. per word bound = -6.000, relative change = 7.598e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 55 (approx. per word bound = -5.999, relative change = 7.470e-05) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, team \n Topic 3: group, take, make, crimin, home \n Topic 4: year, must, time, get, secret \n Topic 5: find, seri, work, discov, crime \n Topic 6: follow, mysteri, set, fight, forc \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 56 (approx. per word bound = -5.999, relative change = 7.459e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 57 (approx. per word bound = -5.998, relative change = 7.538e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 58 (approx. per word bound = -5.998, relative change = 7.718e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 59 (approx. per word bound = -5.997, relative change = 7.977e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 60 (approx. per word bound = -5.997, relative change = 8.361e-05) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, team \n Topic 3: group, take, make, crimin, home \n Topic 4: year, must, secret, time, get \n Topic 5: find, seri, work, discov, crime \n Topic 6: follow, mysteri, set, fight, forc \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 61 (approx. per word bound = -5.996, relative change = 8.847e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 62 (approx. per word bound = -5.996, relative change = 9.399e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 63 (approx. per word bound = -5.995, relative change = 9.923e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 64 (approx. per word bound = -5.995, relative change = 1.035e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 65 (approx. per word bound = -5.994, relative change = 1.051e-04) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, must, human, secret, time \n Topic 5: find, seri, work, discov, crime \n Topic 6: follow, mysteri, set, fight, forc \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 66 (approx. per word bound = -5.993, relative change = 1.033e-04) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 67 (approx. per word bound = -5.993, relative change = 9.817e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 68 (approx. per word bound = -5.992, relative change = 9.121e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 69 (approx. per word bound = -5.992, relative change = 8.383e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 70 (approx. per word bound = -5.991, relative change = 7.726e-05) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, must, human, secret, time \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, follow, set, fight, forc \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 71 (approx. per word bound = -5.991, relative change = 7.233e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 72 (approx. per word bound = -5.990, relative change = 6.884e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 73 (approx. per word bound = -5.990, relative change = 6.745e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 74 (approx. per word bound = -5.990, relative change = 6.805e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 75 (approx. per word bound = -5.989, relative change = 7.066e-05) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, must, human, secret, time \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, follow, set, fight, forc \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 76 (approx. per word bound = -5.989, relative change = 7.522e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 77 (approx. per word bound = -5.988, relative change = 8.083e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 78 (approx. per word bound = -5.988, relative change = 8.667e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 79 (approx. per word bound = -5.987, relative change = 9.142e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 80 (approx. per word bound = -5.987, relative change = 9.356e-05) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, must, secret, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, follow, fight, forc \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 81 (approx. per word bound = -5.986, relative change = 9.225e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 82 (approx. per word bound = -5.986, relative change = 8.791e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 83 (approx. per word bound = -5.985, relative change = 8.192e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 84 (approx. per word bound = -5.985, relative change = 7.675e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 85 (approx. per word bound = -5.984, relative change = 7.354e-05) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, must, secret, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 86 (approx. per word bound = -5.984, relative change = 7.325e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 87 (approx. per word bound = -5.983, relative change = 7.575e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 88 (approx. per word bound = -5.983, relative change = 7.991e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 89 (approx. per word bound = -5.982, relative change = 8.312e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 90 (approx. per word bound = -5.982, relative change = 8.295e-05) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, must, secret, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 91 (approx. per word bound = -5.981, relative change = 7.908e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 92 (approx. per word bound = -5.981, relative change = 7.236e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 93 (approx. per word bound = -5.981, relative change = 6.537e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 94 (approx. per word bound = -5.980, relative change = 6.004e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 95 (approx. per word bound = -5.980, relative change = 5.651e-05) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 96 (approx. per word bound = -5.980, relative change = 5.500e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 97 (approx. per word bound = -5.979, relative change = 5.495e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 98 (approx. per word bound = -5.979, relative change = 5.628e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 99 (approx. per word bound = -5.979, relative change = 5.741e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 100 (approx. per word bound = -5.978, relative change = 5.807e-05) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 101 (approx. per word bound = -5.978, relative change = 5.667e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 102 (approx. per word bound = -5.978, relative change = 5.342e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 103 (approx. per word bound = -5.977, relative change = 4.929e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 104 (approx. per word bound = -5.977, relative change = 4.537e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 105 (approx. per word bound = -5.977, relative change = 4.224e-05) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 106 (approx. per word bound = -5.976, relative change = 4.033e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 107 (approx. per word bound = -5.976, relative change = 3.929e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 108 (approx. per word bound = -5.976, relative change = 3.887e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 109 (approx. per word bound = -5.976, relative change = 3.873e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 110 (approx. per word bound = -5.976, relative change = 3.779e-05) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 111 (approx. per word bound = -5.975, relative change = 3.615e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 112 (approx. per word bound = -5.975, relative change = 3.374e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 113 (approx. per word bound = -5.975, relative change = 3.215e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 114 (approx. per word bound = -5.975, relative change = 3.169e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 115 (approx. per word bound = -5.975, relative change = 3.305e-05) \nTopic 1: live, famili, life, new, friend \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 116 (approx. per word bound = -5.974, relative change = 3.561e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 117 (approx. per word bound = -5.974, relative change = 3.822e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 118 (approx. per word bound = -5.974, relative change = 3.994e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 119 (approx. per word bound = -5.974, relative change = 4.031e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 120 (approx. per word bound = -5.973, relative change = 3.991e-05) \nTopic 1: live, famili, life, new, follow \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 121 (approx. per word bound = -5.973, relative change = 3.903e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 122 (approx. per word bound = -5.973, relative change = 3.762e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 123 (approx. per word bound = -5.973, relative change = 3.589e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 124 (approx. per word bound = -5.973, relative change = 3.386e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 125 (approx. per word bound = -5.972, relative change = 3.160e-05) \nTopic 1: live, famili, life, new, follow \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 126 (approx. per word bound = -5.972, relative change = 2.947e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 127 (approx. per word bound = -5.972, relative change = 2.759e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 128 (approx. per word bound = -5.972, relative change = 2.600e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 129 (approx. per word bound = -5.972, relative change = 2.478e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 130 (approx. per word bound = -5.972, relative change = 2.383e-05) \nTopic 1: live, famili, life, new, follow \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 131 (approx. per word bound = -5.971, relative change = 2.308e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 132 (approx. per word bound = -5.971, relative change = 2.238e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 133 (approx. per word bound = -5.971, relative change = 2.179e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 134 (approx. per word bound = -5.971, relative change = 2.119e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 135 (approx. per word bound = -5.971, relative change = 2.058e-05) \nTopic 1: live, famili, life, new, follow \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 136 (approx. per word bound = -5.971, relative change = 2.034e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 137 (approx. per word bound = -5.971, relative change = 2.012e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 138 (approx. per word bound = -5.971, relative change = 2.045e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 139 (approx. per word bound = -5.970, relative change = 2.095e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 140 (approx. per word bound = -5.970, relative change = 2.183e-05) \nTopic 1: live, famili, life, new, follow \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 141 (approx. per word bound = -5.970, relative change = 2.296e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 142 (approx. per word bound = -5.970, relative change = 2.399e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 143 (approx. per word bound = -5.970, relative change = 2.510e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 144 (approx. per word bound = -5.970, relative change = 2.589e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 145 (approx. per word bound = -5.970, relative change = 2.634e-05) \nTopic 1: live, famili, life, new, follow \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 146 (approx. per word bound = -5.969, relative change = 2.644e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 147 (approx. per word bound = -5.969, relative change = 2.629e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 148 (approx. per word bound = -5.969, relative change = 2.614e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 149 (approx. per word bound = -5.969, relative change = 2.591e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 150 (approx. per word bound = -5.969, relative change = 2.605e-05) \nTopic 1: live, famili, life, new, follow \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 151 (approx. per word bound = -5.969, relative change = 2.652e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 152 (approx. per word bound = -5.968, relative change = 2.722e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 153 (approx. per word bound = -5.968, relative change = 2.811e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 154 (approx. per word bound = -5.968, relative change = 2.874e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 155 (approx. per word bound = -5.968, relative change = 2.926e-05) \nTopic 1: live, famili, life, new, follow \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 156 (approx. per word bound = -5.968, relative change = 2.957e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 157 (approx. per word bound = -5.968, relative change = 2.987e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 158 (approx. per word bound = -5.967, relative change = 3.029e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 159 (approx. per word bound = -5.967, relative change = 3.087e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 160 (approx. per word bound = -5.967, relative change = 3.148e-05) \nTopic 1: live, famili, life, new, follow \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 161 (approx. per word bound = -5.967, relative change = 3.210e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 162 (approx. per word bound = -5.967, relative change = 3.254e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 163 (approx. per word bound = -5.966, relative change = 3.274e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 164 (approx. per word bound = -5.966, relative change = 3.259e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 165 (approx. per word bound = -5.966, relative change = 3.214e-05) \nTopic 1: live, famili, life, new, follow \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 166 (approx. per word bound = -5.966, relative change = 3.143e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 167 (approx. per word bound = -5.966, relative change = 3.048e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 168 (approx. per word bound = -5.965, relative change = 2.955e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 169 (approx. per word bound = -5.965, relative change = 2.829e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 170 (approx. per word bound = -5.965, relative change = 2.711e-05) \nTopic 1: live, famili, life, new, follow \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 171 (approx. per word bound = -5.965, relative change = 2.567e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 172 (approx. per word bound = -5.965, relative change = 2.411e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 173 (approx. per word bound = -5.965, relative change = 2.257e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 174 (approx. per word bound = -5.965, relative change = 2.123e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 175 (approx. per word bound = -5.964, relative change = 1.963e-05) \nTopic 1: live, famili, life, new, follow \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 176 (approx. per word bound = -5.964, relative change = 1.811e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 177 (approx. per word bound = -5.964, relative change = 1.628e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 178 (approx. per word bound = -5.964, relative change = 1.480e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 179 (approx. per word bound = -5.964, relative change = 1.301e-05) \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 180 (approx. per word bound = -5.964, relative change = 1.137e-05) \nTopic 1: live, famili, life, new, follow \n Topic 2: young, world, power, two, becom \n Topic 3: group, take, make, crimin, home \n Topic 4: year, secret, must, human, war \n Topic 5: find, seri, work, discov, crime \n Topic 6: mysteri, set, fight, forc, town \n....................................................................................................\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nModel Converged \n\n\nUm das Ergebnis von der Berechnung zu interpretieren ploten wir die Daten.\n\n#K = c(4,6)\n# Graph\n#plot &lt;- data.frame(\"K\" = c(4,6),\n                   #\"Coherence\" = unlist(statistical_fit_46$results$semcoh),\n                   #\"Perplexity\" = unlist(statistical_fit_46$results$heldout))\n\n# Daten in das long-Format umwandeln\n#plot &lt;- melt(plot, id = c())\n\n#Plot erstellen\n#ggplot(plot, aes(K, value, color = variable)) +\n # geom_line(linewidth = 1.5, show.legend = FALSE) +\n  #scale_x_continuous(breaks = c(4, 6)) +\n  #facet_wrap(~ variable, scales = \"free_y\") +\n  #labs(x = \"Number of topics K\",\n   #    title = \"Statistical fit of models with different K\")"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html",
    "href": "tutorial/01_einlesen_bereinigen.html",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "",
    "text": "Zun√§chst installieren alle Packete, die wir f√ºr diese Sitzung brauchten (z.B. tidyverse). Ihr braucht install.packages() nur, wenn ihr die Pakete im Methodencafe noch nicht installiert hattet-\n\n#install.packages(\"tidyverse)\n#install.packages(\"quanteda\")\n#install.packages(\"quanteda.textplots\")\n#install.packages(\"RCurl\")\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"RCurl\")\n\n\n\nZun√§chst k√∂nnt ihr die Text-Daten via diesem Link auf der Webseite downloaden:\n\nAnschliessen laden wir die Dateien in R. Wenn ihr via JupyterHub arbeitet, geht das via Click-and-Point. Andernfalls nutzt den read.csv()-Befehl. Der Datensatz wird im Objekt data gespeichert.\n\ndata &lt;- read.csv2(\"data_tvseries.csv\")\n\n\n\n\nOft wollen wir Dateien auch direkt von z. B. einer Webseite laden. Daf√ºr nutzen wir die Funktion getURL() aus dem package Rcurl und dann wieder die read.csv2()-Funktion.\n\nlibrary(\"RCurl\")\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/textasdata-ms/main/data/data_tvseries.csv\")\ndata &lt;- read.csv2(text = url)\n\nNach dem Einlesen der Daten verschaffen wir uns einen √úberblick √ºber die Daten und kontrollieren, dass alles korrekt eingelesen wurde.\n\nhead(data)\n\n                Title      Year Parental.Rating Rating Number.of.Votes\n1  1. Game of Thrones 2011‚Äì2019           TV-MA    9.2            2.3M\n2     2. Breaking Bad 2008‚Äì2013           TV-MA    9.5            2.1M\n3  3. Stranger Things 2016‚Äì2025           TV-14    8.7            1.3M\n4          4. Friends 1994‚Äì2004           TV-14    8.9            1.1M\n5 5. The Walking Dead 2010‚Äì2022           TV-MA    8.1            1.1M\n6         6. Sherlock 2010‚Äì2017           TV-14    9.1              1M\n                                                                                                                                                                                                                                    Description\n1                                                                                                           Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\n2                                                                    A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine with a former student in order to secure his family's future.\n3                                                                                          When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatural forces and one strange little girl.\n4                                                                                                        Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan borough of New York City.\n5                                                                                                              Sheriff Deputy Rick Grimes wakes up from a coma to learn the world is in ruins and must lead a group of survivors to stay alive.\n6 The quirky spin on Conan Doyle's iconic sleuth pitches him as a \"high-functioning sociopath\" in modern-day London. Assisting him in his investigations: Afghanistan War vet John Watson, who's introduced to Holmes by a mutual acquaintance."
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#encoding-issues-checken",
    "href": "tutorial/01_einlesen_bereinigen.html#encoding-issues-checken",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.1 Encoding issues checken",
    "text": "2.1 Encoding issues checken\nNach dem Einlesen haben wir bereits einen Blick in den Datensatz geworfen. Nun schauen wir uns gezielt die Variable Description an, um zu √ºberpr√ºfen, ob der zu analysierende Text gut aussieht.\n\ndata %&gt;%\n  select(Description) %&gt;% \n  slice(1)\n\n                                                                                                                          Description\n1 Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\n\n\nDer Text sieht gut aus! Allerdings kann es, gerade bei Texten aus anderen Sprachen, zu sogenannten Encoding Issues kommen.\nSchauen wir uns ein Beispiel an: deutsche Umlaute\n\n#Beispiel-Satz\nstring &lt;- \"Sch√∂ne Gr√º√üe aus M√ºnchen\"\n\n#Encoding pr√ºfen\nEncoding(string)\n\n[1] \"UTF-8\"\n\n#Encoding testweise √§ndern\nEncoding(string) &lt;- \"latin1\"\nstring\n\n[1] \"Sch√É¬∂ne Gr√É¬º√É≈∏e aus M√É¬ºnchen\"\n\n\nWie k√∂nnten wir Encoding-Probleme adressieren?\n\n\nBeim Einlesen das richtige Encoding als Argument mitgeben (siehe z.B. fileEncoding-Argument in read.csv2())\nMit Hilfe von regul√§ren Ausdr√ºcken bereinigen\n\n\nBeim manuellen Bereinigen kann die Funktion gsub() helfen, die Zeichenketten ersetzen kann. Zum Beispiel so:\n\n#Mit Hilfe von regul√§ren Ausdr√ºcken bereinigen\nstring_bereinigt &lt;- string %&gt;% \n  gsub(pattern = \"√É¬∂\", replacement =\"√∂\") %&gt;% \n  gsub(pattern = \"√É¬º\", replacement = \"√º\") %&gt;% \n  gsub(pattern = \"√É≈∏\", replacement = \"√ü\") \nstring_bereinigt\n\n[1] \"Sch√∂ne Gr√º√üe aus M√ºnchen\""
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#tokenisierung-zahlen-urls-etc.-entfernen",
    "href": "tutorial/01_einlesen_bereinigen.html#tokenisierung-zahlen-urls-etc.-entfernen",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.2 Tokenisierung & Zahlen, URLs, etc. entfernen",
    "text": "2.2 Tokenisierung & Zahlen, URLs, etc. entfernen\nDie Funktion tokens()von quanteda erm√∂glicht es uns bei der Aufteilung von Text in tokens direkt bestimmte Zeichen zu entfernen. Hier entfernen wir Punkte, Zahlen, URLs und Symbole.\n\ndaten_tokens &lt;- tokens(data$Description, what = \"word\", remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_symbols = TRUE) #wollen wir das alles entfernen?\n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"Nine\"     \"noble\"    \"families\" \"fight\"    \"for\"      \"control\" \n [7] \"over\"     \"the\"      \"lands\"    \"of\"       \"Westeros\" \"while\"   \n[ ... and 9 more ]\n\ntext2 :\n [1] \"A\"             \"chemistry\"     \"teacher\"       \"diagnosed\"    \n [5] \"with\"          \"inoperable\"    \"lung\"          \"cancer\"       \n [9] \"turns\"         \"to\"            \"manufacturing\" \"and\"          \n[ ... and 13 more ]\n\ntext3 :\n [1] \"When\"      \"a\"         \"young\"     \"boy\"       \"vanishes\"  \"a\"        \n [7] \"small\"     \"town\"      \"uncovers\"  \"a\"         \"mystery\"   \"involving\"\n[ ... and 10 more ]"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#anpassung-auf-kleinschreibung",
    "href": "tutorial/01_einlesen_bereinigen.html#anpassung-auf-kleinschreibung",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.3 Anpassung auf Kleinschreibung",
    "text": "2.3 Anpassung auf Kleinschreibung\nMit der Funktion tokens_tolower()aus dem quanteda Packet k√∂nnen alle Buchstaben in Kleinbuchstaben umgeformt werden.\n\ndaten_tokens &lt;- tokens_tolower(daten_tokens)\n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"     \"noble\"    \"families\" \"fight\"    \"for\"      \"control\" \n [7] \"over\"     \"the\"      \"lands\"    \"of\"       \"westeros\" \"while\"   \n[ ... and 9 more ]\n\ntext2 :\n [1] \"a\"             \"chemistry\"     \"teacher\"       \"diagnosed\"    \n [5] \"with\"          \"inoperable\"    \"lung\"          \"cancer\"       \n [9] \"turns\"         \"to\"            \"manufacturing\" \"and\"          \n[ ... and 13 more ]\n\ntext3 :\n [1] \"when\"      \"a\"         \"young\"     \"boy\"       \"vanishes\"  \"a\"        \n [7] \"small\"     \"town\"      \"uncovers\"  \"a\"         \"mystery\"   \"involving\"\n[ ... and 10 more ]"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#stoppw√∂rter-entfernen",
    "href": "tutorial/01_einlesen_bereinigen.html#stoppw√∂rter-entfernen",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.4 Stoppw√∂rter entfernen",
    "text": "2.4 Stoppw√∂rter entfernen\nEs gibt verschiedene M√∂glichkeiten, Stoppw√∂rter zu entfernen. Am einfachsten ist dies mithilfe der in quanteda integrierten Stoppwortlisten m√∂glich. Diese sind in mehreren Sprachen verf√ºgbar, darunter auch Deutsch.\n\nstopwords(\"english\")\n\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n\ndaten_tokens &lt;- tokens_remove(daten_tokens, stopwords(\"english\"))\n\nJe nach Forschungsfrage k√∂nnen Stoppwortlisten angepasst werden, indem W√∂rter entfernt oder hinzugef√ºgt werden. Es ist aber auch m√∂glich eine eigene Liste zu erstellen.\n\n#W√∂rter aus der quanteda Stoppwortliste entfernen\nstoppw√∂rter &lt;- stopwords(\"english\")\nstoppw√∂rter &lt;- stoppw√∂rter[!stoppw√∂rter %in% c(\"i\", \"me\")]\n\n#W√∂rter der quanteda Stoppwortliste hinzuf√ºgen\nstoppw√∂rter &lt;- c(stoppw√∂rter, \"i\", \"me\")\n\n#Eigene Liste erstellen\neigene_stoppw√∂rter &lt;- c(\"hier\", \"eigene\", \"stoppw√∂rter\")"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#vereinheitlichung",
    "href": "tutorial/01_einlesen_bereinigen.html#vereinheitlichung",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.5 Vereinheitlichung",
    "text": "2.5 Vereinheitlichung\nOft gibt es W√∂rter, die unterschiedliche Abk√ºrzungen oder Schreibweisen haben. Nehmen wir das Beispiel der Europ√§ischen Union, die auch mit EU oder E.U. abgek√ºrzt wird. Mit Hilfe der Funktion gsub() k√∂nnen wir strings mit anderen strings ersetzen.\n\nstring &lt;- \"Bei den EU Wahlen k√∂nnen alle B√ºrger*innen der Europ√§ischen Union w√§hlen gehen.\"\nstring &lt;- gsub(\"Europ√§ischen Union\", \"EU\", string)\nprint(string)\n\n[1] \"Bei den EU Wahlen k√∂nnen alle B√ºrger*innen der EU w√§hlen gehen.\""
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#stemming",
    "href": "tutorial/01_einlesen_bereinigen.html#stemming",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.6 Stemming",
    "text": "2.6 Stemming\nMit der Funktion tokens_wordstem()aus quanteda reduzieren wir alle tokens auf ihren Wortstamm.\n\ndaten_tokens &lt;- daten_tokens %&gt;% \n  tokens_wordstem() \n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"      \"nobl\"      \"famili\"    \"fight\"     \"control\"   \"land\"     \n [7] \"westero\"   \"ancient\"   \"enemi\"     \"return\"    \"dormant\"   \"millennia\"\n\ntext2 :\n [1] \"chemistri\"      \"teacher\"        \"diagnos\"        \"inoper\"        \n [5] \"lung\"           \"cancer\"         \"turn\"           \"manufactur\"    \n [9] \"sell\"           \"methamphetamin\" \"former\"         \"student\"       \n[ ... and 4 more ]\n\ntext3 :\n [1] \"young\"      \"boy\"        \"vanish\"     \"small\"      \"town\"      \n [6] \"uncov\"      \"mysteri\"    \"involv\"     \"secret\"     \"experi\"    \n[11] \"terrifi\"    \"supernatur\"\n[ ... and 5 more ]"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#document-feature-matrix",
    "href": "tutorial/01_einlesen_bereinigen.html#document-feature-matrix",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.7 Document-Feature-Matrix",
    "text": "2.7 Document-Feature-Matrix\nUm aus unseren tokens eine dfm zu machen nutzen wir die dfm()Funktion aus dem quanteda package.\n\ndatam &lt;- daten_tokens %&gt;% \n  dfm()"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#selteneh√§ufige-features-entfernen",
    "href": "tutorial/01_einlesen_bereinigen.html#selteneh√§ufige-features-entfernen",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.8 Seltene/h√§ufige features entfernen",
    "text": "2.8 Seltene/h√§ufige features entfernen\nIm letzten Schritt des Preprocessings entfernen wir h√§ufig und selten vorkommende features aus der dfm. Das geht mit der Funktion dfm_trim()aus dem quanteda Packet.\nEs k√∂nnen unterschiedliche thresholds gesetzt werden - hier lassen wir nur features in der dfm die mindestens in 0.5% und h√∂chstens in 99% der Dokumente vorkommen. Das Argument docfreq_type = \"prop\"berechnet den Anteil der Dokumente, die ein bestimmtes feature beinhalten relativ zur Gesamtzahl der Dokumente. verbose = TRUEprinted w√§hrend der Ausf√ºhrung der Funktion Informationen √ºber den Rechenvorgang in die Konsole.\n\ndatam &lt;- datam %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE)"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#word-cloud-erster-blick-in-die-daten",
    "href": "tutorial/01_einlesen_bereinigen.html#word-cloud-erster-blick-in-die-daten",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.9 Word cloud: Erster Blick in die Daten",
    "text": "2.9 Word cloud: Erster Blick in die Daten\nF√ºr einen ersten Einblick in die Daten lassen wir uns mit der topfeatures()Funktion aus dem quanteda Packet die 10 am h√§ufigsten vorkommenden features ausgeben.\n\ndatam %&gt;% \n  topfeatures(n = 10)\n\n  live   life famili    new  young follow  world friend   find   seri \n   108    108    107    103     74     74     74     70     69     65 \n\n\nDas Ergebnis k√∂nnen wir mit einer word cloud visualisieren. Hierf√ºr nutzen wir die textplot_wordcloud()Funktion aus dem quanteda.textplots Packet.\n\nword_cloud &lt;- datam %&gt;% \n  textplot_wordcloud(max_words = 100)\n\n\n\n\n\n\n\n\n√úbung: mit emoji einleiten ‚Äútest your knowledge‚Äù mit anderem Datensatz, der nicht zu gro√ü ist"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#textdaten-aus-einer-lokalen-datei-einlesen",
    "href": "tutorial/01_einlesen_bereinigen.html#textdaten-aus-einer-lokalen-datei-einlesen",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "",
    "text": "Zun√§chst k√∂nnt ihr die Text-Daten via diesem Link auf der Webseite downloaden:\n\nAnschliessen laden wir die Dateien in R. Wenn ihr via JupyterHub arbeitet, geht das via Click-and-Point. Andernfalls nutzt den read.csv()-Befehl. Der Datensatz wird im Objekt data gespeichert.\n\ndata &lt;- read.csv2(\"data_tvseries.csv\")"
  },
  {
    "objectID": "tutorial/01_einlesen_bereinigen.html#textdaten-von-einer-url-downloaden",
    "href": "tutorial/01_einlesen_bereinigen.html#textdaten-von-einer-url-downloaden",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "",
    "text": "Oft wollen wir Dateien auch direkt von z. B. einer Webseite laden. Daf√ºr nutzen wir die Funktion getURL() aus dem package Rcurl und dann wieder die read.csv2()-Funktion.\n\nlibrary(\"RCurl\")\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/textasdata-ms/main/data/data_tvseries.csv\")\ndata &lt;- read.csv2(text = url)\n\nNach dem Einlesen der Daten verschaffen wir uns einen √úberblick √ºber die Daten und kontrollieren, dass alles korrekt eingelesen wurde.\n\nhead(data)\n\n                Title      Year Parental.Rating Rating Number.of.Votes\n1  1. Game of Thrones 2011‚Äì2019           TV-MA    9.2            2.3M\n2     2. Breaking Bad 2008‚Äì2013           TV-MA    9.5            2.1M\n3  3. Stranger Things 2016‚Äì2025           TV-14    8.7            1.3M\n4          4. Friends 1994‚Äì2004           TV-14    8.9            1.1M\n5 5. The Walking Dead 2010‚Äì2022           TV-MA    8.1            1.1M\n6         6. Sherlock 2010‚Äì2017           TV-14    9.1              1M\n                                                                                                                                                                                                                                    Description\n1                                                                                                           Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\n2                                                                    A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine with a former student in order to secure his family's future.\n3                                                                                          When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatural forces and one strange little girl.\n4                                                                                                        Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan borough of New York City.\n5                                                                                                              Sheriff Deputy Rick Grimes wakes up from a coma to learn the world is in ruins and must lead a group of survivors to stay alive.\n6 The quirky spin on Conan Doyle's iconic sleuth pitches him as a \"high-functioning sociopath\" in modern-day London. Assisting him in his investigations: Afghanistan War vet John Watson, who's introduced to Holmes by a mutual acquaintance."
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html",
    "href": "tutorial/01-einlesen-bereinigen.html",
    "title": "Sitzung 1: Einf√ºhrung & Preprocessing",
    "section": "",
    "text": "Zun√§chst installieren alle Pakete, die wir f√ºr diese Sitzung brauchten (z.B. tidyverse). Ihr braucht install.packages() nur, wenn ihr die Pakete im Methodencafe noch nicht installiert hattet.\n\n#install.packages(\"tidyverse)\n#install.packages(\"quanteda\")\n#install.packages(\"quanteda.textplots\")\n#install.packages(\"RCurl\")\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"RCurl\")\n\n\n\nZun√§chst k√∂nnt ihr die Text-Daten via der Tutorials-Webseite downloaden (s. ‚ÄúMaterialien/Daten‚Äù).\nAnschliessen laden wir die Dateien in R. Wenn ihr via JupyterHub arbeitet, geht das via Click-and-Point. Andernfalls nutzt den read.csv2()-Befehl. Der Datensatz wird im Objekt data gespeichert.\n\ndata &lt;- read.csv2(\"data_tvseries.csv\")\n\n\n\n\nOft wollen wir Dateien auch direkt von z. B. einer Webseite laden. Daf√ºr nutzen wir die Funktion getURL() aus dem package Rcurl und dann wieder die read.csv2()-Funktion.\n\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/textasdata-ms/main/data/data_tvseries.csv\")\ndata &lt;- read.csv2(text = url)\n\nNach dem Einlesen der Daten verschaffen wir uns einen √úberblick √ºber die Daten und kontrollieren, dass alles korrekt eingelesen wurde.\n\nhead(data)\n\n                Title      Year Parental.Rating Rating Number.of.Votes\n1  1. Game of Thrones 2011‚Äì2019           TV-MA    9.2            2.3M\n2     2. Breaking Bad 2008‚Äì2013           TV-MA    9.5            2.1M\n3  3. Stranger Things 2016‚Äì2025           TV-14    8.7            1.3M\n4          4. Friends 1994‚Äì2004           TV-14    8.9            1.1M\n5 5. The Walking Dead 2010‚Äì2022           TV-MA    8.1            1.1M\n6         6. Sherlock 2010‚Äì2017           TV-14    9.1              1M\n                                                                                                                                                                                                                                    Description\n1                                                                                                           Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\n2                                                                    A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine with a former student in order to secure his family's future.\n3                                                                                          When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatural forces and one strange little girl.\n4                                                                                                        Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan borough of New York City.\n5                                                                                                              Sheriff Deputy Rick Grimes wakes up from a coma to learn the world is in ruins and must lead a group of survivors to stay alive.\n6 The quirky spin on Conan Doyle's iconic sleuth pitches him as a \"high-functioning sociopath\" in modern-day London. Assisting him in his investigations: Afghanistan War vet John Watson, who's introduced to Holmes by a mutual acquaintance.\n\n\nSieht soweit gut aus!"
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#textdaten-aus-einer-lokalen-datei-einlesen",
    "href": "tutorial/01-einlesen-bereinigen.html#textdaten-aus-einer-lokalen-datei-einlesen",
    "title": "Sitzung 1: Einf√ºhrung & Preprocessing",
    "section": "",
    "text": "Zun√§chst k√∂nnt ihr die Text-Daten via der Tutorials-Webseite downloaden (s. ‚ÄúMaterialien/Daten‚Äù).\nAnschliessen laden wir die Dateien in R. Wenn ihr via JupyterHub arbeitet, geht das via Click-and-Point. Andernfalls nutzt den read.csv2()-Befehl. Der Datensatz wird im Objekt data gespeichert.\n\ndata &lt;- read.csv2(\"data_tvseries.csv\")"
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#textdaten-von-einer-url-downloaden",
    "href": "tutorial/01-einlesen-bereinigen.html#textdaten-von-einer-url-downloaden",
    "title": "Sitzung 1: Einf√ºhrung & Preprocessing",
    "section": "",
    "text": "Oft wollen wir Dateien auch direkt von z. B. einer Webseite laden. Daf√ºr nutzen wir die Funktion getURL() aus dem package Rcurl und dann wieder die read.csv2()-Funktion.\n\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/textasdata-ms/main/data/data_tvseries.csv\")\ndata &lt;- read.csv2(text = url)\n\nNach dem Einlesen der Daten verschaffen wir uns einen √úberblick √ºber die Daten und kontrollieren, dass alles korrekt eingelesen wurde.\n\nhead(data)\n\n                Title      Year Parental.Rating Rating Number.of.Votes\n1  1. Game of Thrones 2011‚Äì2019           TV-MA    9.2            2.3M\n2     2. Breaking Bad 2008‚Äì2013           TV-MA    9.5            2.1M\n3  3. Stranger Things 2016‚Äì2025           TV-14    8.7            1.3M\n4          4. Friends 1994‚Äì2004           TV-14    8.9            1.1M\n5 5. The Walking Dead 2010‚Äì2022           TV-MA    8.1            1.1M\n6         6. Sherlock 2010‚Äì2017           TV-14    9.1              1M\n                                                                                                                                                                                                                                    Description\n1                                                                                                           Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\n2                                                                    A chemistry teacher diagnosed with inoperable lung cancer turns to manufacturing and selling methamphetamine with a former student in order to secure his family's future.\n3                                                                                          When a young boy vanishes, a small town uncovers a mystery involving secret experiments, terrifying supernatural forces and one strange little girl.\n4                                                                                                        Follows the personal and professional lives of six twenty to thirty year-old friends living in the Manhattan borough of New York City.\n5                                                                                                              Sheriff Deputy Rick Grimes wakes up from a coma to learn the world is in ruins and must lead a group of survivors to stay alive.\n6 The quirky spin on Conan Doyle's iconic sleuth pitches him as a \"high-functioning sociopath\" in modern-day London. Assisting him in his investigations: Afghanistan War vet John Watson, who's introduced to Holmes by a mutual acquaintance.\n\n\nSieht soweit gut aus!"
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#encoding-issues-checken",
    "href": "tutorial/01-einlesen-bereinigen.html#encoding-issues-checken",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.1 Encoding issues checken",
    "text": "2.1 Encoding issues checken\nNach dem Einlesen haben wir bereits einen Blick in den Datensatz geworfen. Nun schauen wir uns gezielt die Variable Description an, um zu √ºberpr√ºfen, ob der zu analysierende Text gut aussieht.\n\ndata %&gt;%\n  select(Description) %&gt;% \n  slice(1)\n\n                                                                                                                          Description\n1 Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\n\n\nDer Text sieht gut aus! Allerdings kann es, gerade bei Texten aus anderen Sprachen, zu sogenannten Encoding Issues kommen.\nSchauen wir uns ein Beispiel an: deutsche Umlaute\n\n#Beispiel-Satz\nstring &lt;- \"Sch√∂ne Gr√º√üe aus M√ºnchen\"\n\n#Encoding pr√ºfen\nEncoding(string)\n\n[1] \"UTF-8\"\n\n#Encoding testweise √§ndern\nEncoding(string) &lt;- \"latin1\"\nstring\n\n[1] \"Sch√É¬∂ne Gr√É¬º√É≈∏e aus M√É¬ºnchen\"\n\n\nWie k√∂nnten wir Encoding-Probleme adressieren?\n\n\nBeim Einlesen das richtige Encoding als Argument mitgeben (siehe z.B. fileEncoding-Argument in read.csv2())\nMit Hilfe von regul√§ren Ausdr√ºcken bereinigen\n\n\nBeim manuellen Bereinigen kann die Funktion gsub() helfen, die Zeichenketten ersetzen kann. Zum Beispiel so:\n\n#Mit Hilfe von regul√§ren Ausdr√ºcken bereinigen\nstring_bereinigt &lt;- string %&gt;% \n  gsub(pattern = \"√É¬∂\", replacement =\"√∂\") %&gt;% \n  gsub(pattern = \"√É¬º\", replacement = \"√º\") %&gt;% \n  gsub(pattern = \"√É≈∏\", replacement = \"√ü\") \nstring_bereinigt\n\n[1] \"Sch√∂ne Gr√º√üe aus M√ºnchen\""
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#tokenisierung-zahlen-urls-etc.-entfernen",
    "href": "tutorial/01-einlesen-bereinigen.html#tokenisierung-zahlen-urls-etc.-entfernen",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.2 Tokenisierung & Zahlen, URLs, etc. entfernen",
    "text": "2.2 Tokenisierung & Zahlen, URLs, etc. entfernen"
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#anpassung-auf-kleinschreibung",
    "href": "tutorial/01-einlesen-bereinigen.html#anpassung-auf-kleinschreibung",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.3 Anpassung auf Kleinschreibung",
    "text": "2.3 Anpassung auf Kleinschreibung\nMit der Funktion tokens_tolower()aus dem quanteda Packet k√∂nnen alle Buchstaben in Kleinbuchstaben umgeformt werden.\n\ndaten_tokens &lt;- tokens_tolower(daten_tokens)\n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"     \"noble\"    \"families\" \"fight\"    \"for\"      \"control\" \n [7] \"over\"     \"the\"      \"lands\"    \"of\"       \"westeros\" \"while\"   \n[ ... and 9 more ]\n\ntext2 :\n [1] \"a\"             \"chemistry\"     \"teacher\"       \"diagnosed\"    \n [5] \"with\"          \"inoperable\"    \"lung\"          \"cancer\"       \n [9] \"turns\"         \"to\"            \"manufacturing\" \"and\"          \n[ ... and 13 more ]\n\ntext3 :\n [1] \"when\"      \"a\"         \"young\"     \"boy\"       \"vanishes\"  \"a\"        \n [7] \"small\"     \"town\"      \"uncovers\"  \"a\"         \"mystery\"   \"involving\"\n[ ... and 10 more ]"
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#stoppw√∂rter-entfernen",
    "href": "tutorial/01-einlesen-bereinigen.html#stoppw√∂rter-entfernen",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.4 Stoppw√∂rter entfernen",
    "text": "2.4 Stoppw√∂rter entfernen\nEs gibt verschiedene M√∂glichkeiten, Stoppw√∂rter zu entfernen. Am einfachsten ist dies mithilfe der in quanteda integrierten Stoppwortlisten m√∂glich. Diese sind in mehreren Sprachen verf√ºgbar, darunter auch Deutsch.\n\nstopwords(\"english\")\n\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n\ndaten_tokens &lt;- tokens_remove(daten_tokens, stopwords(\"english\"))\n\nJe nach Forschungsfrage k√∂nnen Stoppwortlisten angepasst werden, indem W√∂rter entfernt oder hinzugef√ºgt werden. Es ist aber auch m√∂glich eine eigene Liste zu erstellen.\n\n#W√∂rter aus der quanteda Stoppwortliste entfernen\nstoppw√∂rter &lt;- stopwords(\"english\")\nstoppw√∂rter &lt;- stoppw√∂rter[!stoppw√∂rter %in% c(\"i\", \"me\")]\n\n#W√∂rter der quanteda Stoppwortliste hinzuf√ºgen\nstoppw√∂rter &lt;- c(stoppw√∂rter, \"i\", \"me\")\n\n#Eigene Liste erstellen\neigene_stoppw√∂rter &lt;- c(\"hier\", \"eigene\", \"stoppw√∂rter\")"
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#vereinheitlichung",
    "href": "tutorial/01-einlesen-bereinigen.html#vereinheitlichung",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.5 Vereinheitlichung",
    "text": "2.5 Vereinheitlichung\nOft gibt es W√∂rter, die unterschiedliche Abk√ºrzungen oder Schreibweisen haben. Nehmen wir das Beispiel der Europ√§ischen Union, die auch mit EU oder E.U. abgek√ºrzt wird. Mit Hilfe der Funktion gsub() k√∂nnen wir strings mit anderen strings ersetzen.\n\nstring &lt;- \"Bei den EU Wahlen k√∂nnen alle B√ºrger*innen der Europ√§ischen Union w√§hlen gehen.\"\nstring &lt;- gsub(\"Europ√§ischen Union\", \"EU\", string)\nprint(string)\n\n[1] \"Bei den EU Wahlen k√∂nnen alle B√ºrger*innen der EU w√§hlen gehen.\""
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#stemming",
    "href": "tutorial/01-einlesen-bereinigen.html#stemming",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.6 Stemming",
    "text": "2.6 Stemming\nMit der Funktion tokens_wordstem()aus quanteda reduzieren wir alle tokens auf ihren Wortstamm.\n\ndaten_tokens &lt;- daten_tokens %&gt;% \n  tokens_wordstem() \n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"nine\"      \"nobl\"      \"famili\"    \"fight\"     \"control\"   \"land\"     \n [7] \"westero\"   \"ancient\"   \"enemi\"     \"return\"    \"dormant\"   \"millennia\"\n\ntext2 :\n [1] \"chemistri\"      \"teacher\"        \"diagnos\"        \"inoper\"        \n [5] \"lung\"           \"cancer\"         \"turn\"           \"manufactur\"    \n [9] \"sell\"           \"methamphetamin\" \"former\"         \"student\"       \n[ ... and 4 more ]\n\ntext3 :\n [1] \"young\"      \"boy\"        \"vanish\"     \"small\"      \"town\"      \n [6] \"uncov\"      \"mysteri\"    \"involv\"     \"secret\"     \"experi\"    \n[11] \"terrifi\"    \"supernatur\"\n[ ... and 5 more ]"
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#document-feature-matrix",
    "href": "tutorial/01-einlesen-bereinigen.html#document-feature-matrix",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.7 Document-Feature-Matrix",
    "text": "2.7 Document-Feature-Matrix\nUm aus unseren tokens eine dfm zu machen nutzen wir die dfm()Funktion aus dem quanteda package.\n\ndatam &lt;- daten_tokens %&gt;% \n  dfm()"
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#selteneh√§ufige-features-entfernen",
    "href": "tutorial/01-einlesen-bereinigen.html#selteneh√§ufige-features-entfernen",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.8 Seltene/h√§ufige features entfernen",
    "text": "2.8 Seltene/h√§ufige features entfernen\nIm letzten Schritt des Preprocessings entfernen wir h√§ufig und selten vorkommende features aus der dfm. Das geht mit der Funktion dfm_trim()aus dem quanteda Packet.\nEs k√∂nnen unterschiedliche thresholds gesetzt werden - hier lassen wir nur features in der dfm die mindestens in 0.5% und h√∂chstens in 99% der Dokumente vorkommen. Das Argument docfreq_type = \"prop\"berechnet den Anteil der Dokumente, die ein bestimmtes feature beinhalten relativ zur Gesamtzahl der Dokumente. verbose = TRUEprinted w√§hrend der Ausf√ºhrung der Funktion Informationen √ºber den Rechenvorgang in die Konsole.\n\ndatam &lt;- datam %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE)"
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#word-cloud-erster-blick-in-die-daten",
    "href": "tutorial/01-einlesen-bereinigen.html#word-cloud-erster-blick-in-die-daten",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.9 Word cloud: Erster Blick in die Daten",
    "text": "2.9 Word cloud: Erster Blick in die Daten\nF√ºr einen ersten Einblick in die Daten lassen wir uns mit der topfeatures()Funktion aus dem quanteda Packet die 10 am h√§ufigsten vorkommenden features ausgeben.\n\ndatam %&gt;% \n  topfeatures(n = 10)\n\n  live   life famili    new  young follow  world friend   find   seri \n   108    108    107    103     74     74     74     70     69     65 \n\n\nDas Ergebnis k√∂nnen wir mit einer word cloud visualisieren. Hierf√ºr nutzen wir die textplot_wordcloud()Funktion aus dem quanteda.textplots Packet.\n\nword_cloud &lt;- datam %&gt;% \n  textplot_wordcloud(max_words = 100)\n\n\n\n\n\n\n\n\n√úbung: mit emoji einleiten ‚Äútest your knowledge‚Äù mit anderem Datensatz, der nicht zu gro√ü ist"
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#bereinigung-z.-b.-encoding-probleme",
    "href": "tutorial/01-einlesen-bereinigen.html#bereinigung-z.-b.-encoding-probleme",
    "title": "Sitzung 1: Einf√ºhrung & Preprocessing",
    "section": "2.1 Bereinigung (z. B. Encoding-Probleme)",
    "text": "2.1 Bereinigung (z. B. Encoding-Probleme)\nNach dem Einlesen haben wir bereits einen Blick in den Datensatz geworfen. Nun schauen wir uns gezielt die Variable Description an, um zu √ºberpr√ºfen, ob der zu analysierende Text gut aussieht. Hier nutzen wir slice(), um uns nur den allerersten Text ausgeben zu lassen.\n\ndata %&gt;%\n  \n  #Auswahl der Variable \"Description\"\n  select(Description) %&gt;% \n  \n  #Reduktion auf ersten Text\n  slice(1)\n\n                                                                                                                          Description\n1 Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\n\n\nDer Text sieht gut aus! Allerdings kann es, gerade bei Texten aus anderen Sprachen, zu sogenannten Encoding-Problemen kommen.\n\n2.1.1 Encoding-Probleme\nSchauen wir uns ein Beispiel an: deutsche Umlaute. Was passiert wenn wir hier das Encoding √§ndern?\n\n#Beispiel-Satz\nstring &lt;- \"Sch√∂ne Gr√º√üe aus M√ºnchen\"\n\n#Encoding pr√ºfen\nEncoding(string)\n\n[1] \"UTF-8\"\n\n#Encoding testweise √§ndern\nEncoding(string) &lt;- \"latin1\"\nstring\n\n[1] \"Sch√É¬∂ne Gr√É¬º√É≈∏e aus M√É¬ºnchen\"\n\n\nWie k√∂nnen wir Encoding-Probleme adressieren?\n\nBeim Einlesen das richtige Encoding als Argument mitgeben (siehe z.B. fileEncoding-Argument in read.csv2())\nMit Hilfe von regul√§ren Ausdr√ºcken bereinigen\n\nBeim manuellen Bereinigen k√∂nnen regul√§re Ausdr√ºcke (mehr dazu hier) und die Funktion gsub() helfen, mit der wir Zeichen ersetzen k√∂nnen. Zum Beispiel so:\n\n#Mit Hilfe von regul√§ren Ausdr√ºcken bereinigen\nstring %&gt;% \n  \n  #Ersatz f√ºr falsches Encoding \"√∂\"\n  gsub(pattern = \"√É¬∂\", replacement =\"√∂\") %&gt;% \n  \n  #Ersatz f√ºr falsches Encoding \"√º\"\n  gsub(pattern = \"√É¬º\", replacement = \"√º\") %&gt;% \n  \n  #Ersatz f√ºr falsches Encoding \"√ü\"\n  gsub(pattern = \"√É≈∏\", replacement = \"√ü\") \n\n[1] \"Sch√∂ne Gr√º√üe aus M√ºnchen\"\n\n\n\n\n2.1.2 Datenbereinigung mit regul√§ren Ausdr√ºcken\nProbieren wir dies f√ºr unseren Datensatz aus. Wir wollen die Nummer, den Punkt und das Leerzeichen vor dem Titel der TV-Serie in der Variable Title mit gsub() entfernen:\n\ndata %&gt;%\n  select(Title) %&gt;%\n  head(5)\n\n                Title\n1  1. Game of Thrones\n2     2. Breaking Bad\n3  3. Stranger Things\n4          4. Friends\n5 5. The Walking Dead\n\n\nDas ginge mit folgenden Befehlen:\n\n#Entfernung der Zeichen vor dem Titel der TV-Serie\ndata &lt;- data %&gt;%\n  mutate(Title = gsub(\"^[0-9]+[[:punct:]] \", \"\", Title))\n\n#So sieht das Ergebnis aus:\ndata %&gt;%\n  select(Title) %&gt;%\n  head(5)\n\n             Title\n1  Game of Thrones\n2     Breaking Bad\n3  Stranger Things\n4          Friends\n5 The Walking Dead\n\n\n\n\n2.1.3 Datenfilterung mit regul√§ren Ausdr√ºcken\nMit regul√§ren Ausdr√ºcken k√∂nnen wir aber noch mehr machen, z.B. Daten filtern. Wir wollen nur TV-Serien behalten, die als ‚ÄúDrama‚Äù klassifiziert wurden.\n\ndata %&gt;%\n\n  # filtern aller TV_Serien, die \"Drama\" in der Beschreibung beinhalten\n  filter(grepl(\"[D|d]rama\", Description)) %&gt;%\n\n  # Inspektion der ersten f√ºnf Titel\n  select(Title) %&gt;%\n  head(5)\n\n                                      Title\n1                              Black Mirror\n2 The Lord of the Rings: The Rings of Power\n3                            Grey's Anatomy\n4                                   Mad Men\n5                                 Teen Wolf\n\n\nUnd jetzt behalten wir solche, die als ‚ÄúDrama‚Äù oder ‚ÄúCrime‚Äù klassifiziert wurden.\n\ndata %&gt;%\n\n  # filtern aller TV_Serien, die \"Drama\"und \"Crime\" in der Beschreibung beinhalten\n  filter(grepl(\"[D|d]rama|[C|c]rime\", Description)) %&gt;%\n\n  # Inspektion der ersten f√ºnf Titel\n  select(Title) %&gt;%\n  head(5)\n\n                                      Title\n1                                    Dexter\n2                              Black Mirror\n3                                 Daredevil\n4                                 The Flash\n5 The Lord of the Rings: The Rings of Power\n\n\n\n\n2.1.4 Aufgabe 1 üìå\nK√∂nnt ihr‚Ä¶\n\nBasis: Alle Serien identifizieren, die in Deutschland spielen?\nFortgeschritten: Alle Serien identifizieren, in denen es um Superhelden geht und ‚Äúsuperhero/superheroes‚Äù in der Variable Description mit ‚Äúfancy R programmers‚Äú ersetzen?"
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#aufgabe-1",
    "href": "tutorial/01-einlesen-bereinigen.html#aufgabe-1",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "üìåAufgabe 1",
    "text": "üìåAufgabe 1\nK√∂nnt ihr‚Ä¶\n\nBasis: Alle Serien identifizieren, die in Deutschland spielen?\nFortgeschritten: Alle Serien identifizieren, in denen es um Superhelden geht und ‚Äúsuperhero/superheroes‚Äù in der Variable Description mit ‚Äúfancy R programmers‚Äú ersetzen?"
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#tokenisierung-zahlen-urls-etc.-entfernen-1",
    "href": "tutorial/01-einlesen-bereinigen.html#tokenisierung-zahlen-urls-etc.-entfernen-1",
    "title": "Sitzung 1: Daten einlesen und bereinigen",
    "section": "2.2 Tokenisierung & Zahlen, URLs, etc. entfernen",
    "text": "2.2 Tokenisierung & Zahlen, URLs, etc. entfernen\nDie Funktion tokens()von quanteda erm√∂glicht es uns bei der Aufteilung von Text in tokens direkt bestimmte Zeichen zu entfernen. Hier entfernen wir Punkte, Zahlen, URLs und Symbole.\n\ndaten_tokens &lt;- tokens(data$Description, what = \"word\", remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_symbols = TRUE) #wollen wir das alles entfernen?\n\ndaten_tokens %&gt;% \n  head(n=3)\n\nTokens consisting of 3 documents.\ntext1 :\n [1] \"Nine\"     \"noble\"    \"families\" \"fight\"    \"for\"      \"control\" \n [7] \"over\"     \"the\"      \"lands\"    \"of\"       \"Westeros\" \"while\"   \n[ ... and 9 more ]\n\ntext2 :\n [1] \"A\"             \"chemistry\"     \"teacher\"       \"diagnosed\"    \n [5] \"with\"          \"inoperable\"    \"lung\"          \"cancer\"       \n [9] \"turns\"         \"to\"            \"manufacturing\" \"and\"          \n[ ... and 13 more ]\n\ntext3 :\n [1] \"When\"      \"a\"         \"young\"     \"boy\"       \"vanishes\"  \"a\"        \n [7] \"small\"     \"town\"      \"uncovers\"  \"a\"         \"mystery\"   \"involving\"\n[ ... and 10 more ]"
  },
  {
    "objectID": "tutorial/02_cooccurence_kollokationen_postagging_depparsing.html",
    "href": "tutorial/02_cooccurence_kollokationen_postagging_depparsing.html",
    "title": "Sitzung 2: Co-Occurence Analysen",
    "section": "",
    "text": "Zun√§chst installieren alle Pakete, die wir f√ºr diese Sitzung brauchten (z.B. tidyverse). Ihr braucht install.packages() nur, wenn ihr die Pakete im Methodencafe noch nicht installiert hattet.\n\n#install.packages(\"tidyverse)\n#install.packages(\"quanteda\")\n#install.packages(\"quanteda.textplots\")\n#install.packages(\"RCurl\")\n#install.packages(\"quanteda.textstats\")\n#install.packages(\"udpipe\")\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"RCurl\")\nlibrary(\"quanteda.textstats\")\nlibrary(\"udpipe\")\n\nNeu hinzugekommen seit dem Methodencafe ist das rsyntax-Paket, das ihr neu installieren m√ºsstest:\n\ninstall.packages(\"rsyntax\")\nlibrary(\"rsyntax\")\n\nNun laden wir die Daten wieder ein und f√ºhren die bereits erlernten Preprocessing-Schritte, inkl. der Transformation in eine Document-Feature-Matrix, aus:\n\n#Daten laden\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/textasdata-ms/main/data/data_tvseries.csv\")\ndata &lt;- read.csv2(text = url)\n\n#Preprocessing\ntokens &lt;- tokens(data$Description,\n                 what = \"word\", #Tokenisierung, hier zu W√∂rtern als Analyseeinheit\n                 remove_punct = TRUE, #Entfernung von Satzzeichen\n                 remove_numbers = TRUE) %&gt;% #Entfernung von Zahlen\n  \n  #Kleinschreibung\n  tokens_tolower() %&gt;% \n  \n  #Entfernung von Stoppw√∂rtern\n  tokens_remove(stopwords(\"english\")) %&gt;% \n  \n  #Stemming\n  tokens_wordstem()\n\n#Text-as-Data Repr√§sentation als Document-Feature-Matrix\ndfm &lt;- tokens %&gt;% \n  dfm() %&gt;% \n  \n  #Relative pruning\n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE) \n\nJetzt sind wir bereit f√ºr die ersten Co-Occurence-Analysen!"
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#normalisierung",
    "href": "tutorial/01-einlesen-bereinigen.html#normalisierung",
    "title": "Sitzung 1: Einf√ºhrung & Preprocessing",
    "section": "2.2 Normalisierung",
    "text": "2.2 Normalisierung\nDie Funktionen tokens() bzw. entsprechende Subfunktionen von quanteda erm√∂glichen es uns, mehrere Preprocessing-Schritte in einer einzigen Pipe (d.h. Analyse-Pipeline auf Basis des tidyverse) durchzuf√ºhren:\n\nTokenisierung: tokens() (hier k√∂nnen Tokenisierung anwenden, aber bereits auch Sonderzeichen, wie Satzzeichen, entfernen)\nKleinschreibung: tokens_tolower()\nStoppw√∂rter entfernen: tokens_remove_stopwords().\nStemming: tokens_wordstem()\n\n\ntokens &lt;- tokens(data$Description,\n                 what = \"word\", #Tokenisierung, hier zu W√∂rtern als Analyseeinheit\n                 remove_punct = TRUE, #Entfernung von Satzzeichen\n                 remove_numbers = TRUE) %&gt;% #Entfernung von Zahlen\n  \n  #Kleinschreibung\n  tokens_tolower() %&gt;% \n  \n  #Entfernung von Stoppw√∂rtern\n  tokens_remove(stopwords(\"english\")) %&gt;% \n  \n  #Stemming\n  tokens_wordstem()\n\n\n#So sah unser erster Text vor dem Preprocessing aus\ndata$Description[1]\n\n[1] \"Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for millennia.\"\n\n#Und so danach\ntokens[1]\n\nTokens consisting of 1 document.\ntext1 :\n [1] \"nine\"      \"nobl\"      \"famili\"    \"fight\"     \"control\"   \"land\"     \n [7] \"westero\"   \"ancient\"   \"enemi\"     \"return\"    \"dormant\"   \"millennia\"\n\n\n\n2.2.1 Entfernung von Stoppw√∂rtern\nEs gibt verschiedene M√∂glichkeiten, Stoppw√∂rter zu entfernen. Am einfachsten ist dies mithilfe der im quanteda-Paket integrierten Stoppwortlisten m√∂glich. Diese sind in mehreren Sprachen verf√ºgbar, darunter auch Deutsch.\nJe nach Forschungsfrage k√∂nnen diese Stoppwortlisten angepasst werden, indem eigene Stoppw√∂rter definiert und dann entfernt werden. Es ist aber auch m√∂glich, eigene Stoppwortlisten zu erstellen.\n\n#W√∂rter aus der quanteda Stoppwortliste entfernen\nstoppw√∂rter &lt;- stopwords(\"english\")\nstoppw√∂rter &lt;- stoppw√∂rter[!stoppw√∂rter %in% c(\"i\", \"me\")]\n\n#Beispielhafte Anwendung \ntokens(data$Description,\n                 what = \"word\", #Tokenisierung, hier zu W√∂rtern als Analyseeinheit\n                 remove_punct = TRUE, #Entfernung von Satzzeichen\n                 remove_numbers = TRUE) %&gt;% #Entfernung von Zahlen\n  \n  #Kleinschreibung\n  tokens_tolower() %&gt;% \n  \n  #Entfernung von Stoppw√∂rtern - hier z.B. reduzierte quanteda-Liste\n  tokens_remove(stoppw√∂rter) %&gt;% \n  \n  #Stemming\n  tokens_wordstem() %&gt;%\n  \n  #Ausgabe des ersten Textes\n  head(1)\n\nTokens consisting of 1 document.\ntext1 :\n [1] \"nine\"      \"nobl\"      \"famili\"    \"fight\"     \"control\"   \"land\"     \n [7] \"westero\"   \"ancient\"   \"enemi\"     \"return\"    \"dormant\"   \"millennia\"\n\n\n\n\n2.2.2 Aufgabe 2 üìå\nK√∂nnt ihr‚Ä¶\n\nBasis: Eine Liste mit 3-5 Stopw√∂rtern erstellen und diese als Teil des Preprocessings zus√§tzlich entfernen?\nFortgeschritten: Daf√ºr sorgen, dass Namen von St√§dten (hier als Beispiel ‚ÄûNew York‚Äú) als ein einzelnes Feature beibehalten werden?"
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#top-features",
    "href": "tutorial/01-einlesen-bereinigen.html#top-features",
    "title": "Sitzung 1: Einf√ºhrung & Preprocessing",
    "section": "4.1 Top Features",
    "text": "4.1 Top Features\nF√ºr einen ersten Einblick in die Daten lassen wir uns mit der topfeatures()-Funktion aus dem quanteda-Packet die zehn am h√§ufigsten vorkommenden Features ausgeben.\n\ntopfeatures(dfm, 10) %&gt;%\n  \n  #Umwandlung in einen \"sch√∂neren\" Dataframe mit der Spalte \"H√§ufigkeit\"\n  as.data.frame() %&gt;%\n  rename(\"H√§ufigkeit\" = '.')\n\n       H√§ufigkeit\nlive          108\nlife          108\nfamili        107\nnew           103\nyoung          74\nfollow         74\nworld          74\nfriend         70\nfind           69\nseri           65"
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#die-ber√ºhmt-ber√ºchtigte-word-cloud",
    "href": "tutorial/01-einlesen-bereinigen.html#die-ber√ºhmt-ber√ºchtigte-word-cloud",
    "title": "Sitzung 1: Einf√ºhrung & Preprocessing",
    "section": "4.2. Die ber√ºhmt-ber√ºchtigte Word Cloud",
    "text": "4.2. Die ber√ºhmt-ber√ºchtigte Word Cloud\nDas Ergebnis k√∂nnen wir mit einer Word Cloud visualisieren. Hierf√ºr nutzen wir die textplot_wordcloud()Funktion aus dem quanteda.textplots-Paket. Dabei werden besonders ‚Äúh√§ufige‚Äù Features gr√∂sser dargestellt.\n\ntextplot_wordcloud(dfm, max_words = 100)"
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#erstellung-einer-dfm",
    "href": "tutorial/01-einlesen-bereinigen.html#erstellung-einer-dfm",
    "title": "Sitzung 1: Einf√ºhrung & Preprocessing",
    "section": "3.1 Erstellung einer DFM",
    "text": "3.1 Erstellung einer DFM\nUm aus unseren tokens eine Document-Feature-Matrix zu machen, damit der Computer ‚ÄúText-as-Data‚Äù, d.h. als numerisches Datenformat, verarbeiten kann, nutzen wir die dfm()Funktion aus dem quanteda-Paket.\n\n#Wir erstellen eine Document-Feature matrix\ndfm &lt;- tokens %&gt;%\n  dfm()\n\n#So sieht das Ergebnis aus\ndfm\n\nDocument-feature matrix of: 900 documents, 4,246 features (99.66% sparse) and 0 docvars.\n       features\ndocs    nine nobl famili fight control land westero ancient enemi return\n  text1    1    1      1     1       1    1       1       1     1      1\n  text2    0    0      1     0       0    0       0       0     0      0\n  text3    0    0      0     0       0    0       0       0     0      0\n  text4    0    0      0     0       0    0       0       0     0      0\n  text5    0    0      0     0       0    0       0       0     0      0\n  text6    0    0      0     0       0    0       0       0     0      0\n[ reached max_ndoc ... 894 more documents, reached max_nfeat ... 4,236 more features ]\n\n\nWir sehen:\n\nDie DFM besteht aus 900 Dokumenten.\nDie DFM hat nach dem Preprocessing immer noch 4,246 individuelle Features, hier W√∂rter.\nDie DFM ist 99.66% ‚Äúspare‚Äù, d.h. mehr als 99% der Zellen enthalten eine 0, weil viele Features nur sehr selten vorkommen."
  },
  {
    "objectID": "tutorial/01-einlesen-bereinigen.html#zus√§tzliche-normalisierung-relative-pruning",
    "href": "tutorial/01-einlesen-bereinigen.html#zus√§tzliche-normalisierung-relative-pruning",
    "title": "Sitzung 1: Einf√ºhrung & Preprocessing",
    "section": "3.2 Zus√§tzliche Normalisierung: Relative Pruning",
    "text": "3.2 Zus√§tzliche Normalisierung: Relative Pruning\nIm letzten Schritt des Preprocessings entfernen wir h√§ufig und selten vorkommende Features aus der DFM. Das geht mit der Funktion dfm_trim()aus dem quanteda-Paket.\nEs k√∂nnen unterschiedliche Grenzwerte gesetzt werden. Hier behalten wir nur Features, die in mindestens in 0.5% und h√∂chstens in 99% der Dokumente vorkommen. Das Argument docfreq_type = \"prop\"berechnet den Anteil der Dokumente, die ein bestimmtes Feature beinhalten relativ zur Gesamtzahl der Dokumente. verbose = TRUEprinted w√§hrend der Ausf√ºhrung der Funktion Informationen √ºber den Rechenvorgang in die Konsole.\n\n#Anzahl Features vor relative pruning\ndfm\n\nDocument-feature matrix of: 900 documents, 4,246 features (99.66% sparse) and 0 docvars.\n       features\ndocs    nine nobl famili fight control land westero ancient enemi return\n  text1    1    1      1     1       1    1       1       1     1      1\n  text2    0    0      1     0       0    0       0       0     0      0\n  text3    0    0      0     0       0    0       0       0     0      0\n  text4    0    0      0     0       0    0       0       0     0      0\n  text5    0    0      0     0       0    0       0       0     0      0\n  text6    0    0      0     0       0    0       0       0     0      0\n[ reached max_ndoc ... 894 more documents, reached max_nfeat ... 4,236 more features ]\n\n#Anwendung des relative pruning\ndfm  &lt;- dfm  %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE) \n\n#Anzahl Features nach relative pruning\ndfm\n\nDocument-feature matrix of: 900 documents, 605 features (98.58% sparse) and 0 docvars.\n       features\ndocs    famili fight control land ancient enemi return turn former student\n  text1      1     1       1    1       1     1      1    0      0       0\n  text2      1     0       0    0       0     0      0    1      1       1\n  text3      0     0       0    0       0     0      0    0      0       0\n  text4      0     0       0    0       0     0      0    0      0       0\n  text5      0     0       0    0       0     0      0    0      0       0\n  text6      0     0       0    0       0     0      0    0      0       0\n[ reached max_ndoc ... 894 more documents, reached max_nfeat ... 595 more features ]\n\n\nWir sehen: Relative pruning kann unseren Datensatz noch einmal deutlich verkleinern (und damit z.B. Analysen beschleunigen): Wir haben nun 605 anstelle von 4,246 Features!"
  },
  {
    "objectID": "tutorial/02_cooccurence_kollokationen_postagging_depparsing.html#n-grams",
    "href": "tutorial/02_cooccurence_kollokationen_postagging_depparsing.html#n-grams",
    "title": "Sitzung 2: Co-Occurence Analysen",
    "section": "2.1 N-grams",
    "text": "2.1 N-grams\nZun√§chst schauen wir uns als Beispiel f√ºr ngrams bigrams, d.h. Abfolgen zweier W√∂rter, an:\n\ntokens %&gt;%\n  \n  #Umwandlung in bigrams\n  tokens_ngrams(n = 2) %&gt;%\n  \n  #Ausgabe f√ºr erstes Dokument\n  head(1)\n\nTokens consisting of 1 document.\ntext1 :\n [1] \"nine_nobl\"         \"nobl_famili\"       \"famili_fight\"     \n [4] \"fight_control\"     \"control_land\"      \"land_westero\"     \n [7] \"westero_ancient\"   \"ancient_enemi\"     \"enemi_return\"     \n[10] \"return_dormant\"    \"dormant_millennia\"\n\n\nAus Sitzung 1 kennen wir ja bereits den Befehl, um uns die h√§ufigsten Features ausgeben zu lassen. Nun lassen wir uns nicht die h√§ufigsten unigrams (einzelne W√∂rter), sondern die h√§ufigsten bigrams (Abfolge von zwei W√∂rtern) ausgeben.\n\ntokens %&gt;%\n  \n  #Umwandlung in bigrams\n  tokens_ngrams(n = 2) %&gt;%\n  \n  #Umwandlung in dfm f√ºr topfeatures-Befehl\n  dfm() %&gt;%\n  \n  #Ausgabe der h√§ufigsten Features\n  topfeatures(10) %&gt;%\n  \n  #Umwandlung in einen \"sch√∂neren\" Dataframe mit der Spalte \"H√§ufigkeit\"\n  as.data.frame() %&gt;%\n  rename(\"H√§ufigkeit\" = '.')\n\n                  H√§ufigkeit\nnew_york                  39\nbest_friend               25\nhigh_school               22\nlos_angel                 22\nyork_citi                 19\nperson_profession         13\nserial_killer             10\nantholog_seri             10\nseri_follow               10\nyoung_boy                  9\n\n\nWir sehen, dass unter den h√§ufigsten bigrams einige Features sind, die in Kombination miteinander eine andere Bedeutung haben als als einzelne unigrams. Dazu geh√∂ren z.B. Orte (z.B. New York) oder Ausdr√ºcke (z. B. serial killer)."
  },
  {
    "objectID": "tutorial/02_cooccurence_kollokationen_postagging_depparsing.html#keywords-in-context-kwic",
    "href": "tutorial/02_cooccurence_kollokationen_postagging_depparsing.html#keywords-in-context-kwic",
    "title": "Sitzung 2: Co-Occurence Analysen",
    "section": "2.2 Keywords-in-Context (KWIC)",
    "text": "2.2 Keywords-in-Context (KWIC)\nAls eher qualitativer Einblick bietet sich die Analyse von Konkordanzen, d.h. der Analyse von Schl√ºsselw√∂rtern und ihres Kontexts, im Text an.\nDie kwic()Funktion aus dem quanteda-Paket identifiziert hierzu Schl√ºsselw√∂rter und W√∂rter vor bzw. nach diesen. Wir schauen uns also W√∂rter an, die um ein sogenanntes Window) von z.B. einem oder mehr W√∂rtern vor oder nach dem Schl√ºsselwort vorkommen.\nMit folgendem Code k√∂nnen wir beispielsweise herausfinden, in welchem Kontext das Wort hero vorkommt. In der Ausgabe werden jeweils das Wort vor und nach dem Wort hero angezeigt. Um die Bedeutung dieser W√∂rter besser zu verstehen, nutzen wir dabei die ‚Äúunbereinigte‚Äù Description-Variable aus dem Objekt data.\n\ndata$Description %&gt;% \n  \n  #Keywords-in-Context mit Window von 5 W√∂rtern vor und nach Schl√ºsselwort\n  kwic(pattern = \"hero\", \n       window = 1) %&gt;%\n  \n  #Ausgabe der ersten Zeilen\n  head()\n\nKeyword-in-context with 5 matches.                                           \n  [text124, 7]           a | hero | that   \n [text140, 38]       Rebel | hero | .      \n  [text336, 8] prestigious | hero | academy\n [text336, 19]           a | hero | ,      \n  [text756, 8]           a | hero | of"
  },
  {
    "objectID": "tutorial/02_cooccurence_kollokationen_postagging_depparsing.html#co-occurence-analysen-semantische-netzwerke",
    "href": "tutorial/02_cooccurence_kollokationen_postagging_depparsing.html#co-occurence-analysen-semantische-netzwerke",
    "title": "Sitzung 2: Co-Occurence Analysen",
    "section": "2.3 Co-Occurence Analysen & semantische Netzwerke",
    "text": "2.3 Co-Occurence Analysen & semantische Netzwerke\nAls eine Form der verallgemeinerten und quantifizierenden Form von ngrams bieter sich die Co-Occurence-Analyse an. Co-Occurence Analysen zeigen auf, welche W√∂rter h√§ufig in einem gemeinsamen Kontext (z.B. in direkter Abfolge, innerhalb eines Textes) vorkommen.\nHierf√ºr m√ºssen wir zun√§chst die DFM in eine Feature-Co-occurrence Matrix (FCM) umwandeln. Das machen wir mit der Funktion fcm() aus dem quanteda-Paket. Uns interessiert zun√§chst, wie h√§ufig Features innerhalb eines Textes vorkommen, weshalb wir das context-Argument nutzen und auf document setzen.\n\ntokens %&gt;%\n  \n  #Umwandlung in eine Feature-Co-Occurence-Matrix\n  fcm(context = \"document\") %&gt;%\n  \n  #Ausgabe der ersten Zeilen\n  head()\n\nFeature co-occurrence matrix of: 6 by 4,246 features.\n         features\nfeatures  nine nobl famili fight control land westero ancient enemi return\n  nine       0    1      1     1       1    1       1       1     1      1\n  nobl       0    0      1     1       1    1       1       1     1      1\n  famili     0    0      8     6       2    3       1       1     4      6\n  fight      0    0      0     1       2    2       1       2     2      2\n  control    0    0      0     0       0    1       1       1     1      1\n  land       0    0      0     0       0    0       1       1     1      1\n[ reached max_nfeat ... 4,236 more features ]\n\n\nWir sehen:\n\nDie FCM besteht aus 4,246 Features (da wir nur wenig Preprocessing angewedent haben).\nDie Zellen der FCM illustrieren, wie h√§ufig welche W√∂rter innerhalb eines Textes - hier der Beschreibung einer Serie mittels Description - gemeinsam vorkommen. Beispielsweise scheinen in zwei Serien-Beschreibungen die W√∂rter fight und famili gemeinsam vorzukommen.\n\nWir k√∂nnten aber auch spezifische Features ausw√§hlen, die uns in unserer Analyse interessieren. Das machen wir mit der Funktion fcm_select() aus dem quanteda-Paket.\nZudem k√∂nnten wir √ºber das window-Argument noch spezifizieren, dass uns die Co-Occurence von Features nicht innerhalb der gesamten Serienbeschreibung, sondern nur innerhalb eines Windows von z. B. 6 W√∂rtern interessiert. Ein Argument hierf√ºr w√§re, dass W√∂rter, die nah aufeinander folgen, auch st√§rker eine inhaltlich geteilte Bedeutung haben. Zudem setzen wir das selection-Aargument auf keep, um die FCM nur auf die angegebenen Features zu reduzieren.\nBeispielsweise wollen wir uns anschauen, ob M√§nner (man) und Frauen (woman) in Serienbeschreibung stereotypisiert, z. B. im Hinblick auf Geschlechterrollen, dargestellt werden.\nDaf√ºr schauen wir, welche W√∂rter (z.B. fight vs.¬†romance) h√§ufiger in Verbindung mit M√§nnern bzw. Frauen genannt werden\n\nfcm &lt;- tokens %&gt;%\n  \n  #Erstellung einer FCM mit einem Window von 8\n  fcm(window = 8) %&gt;%\n  \n  #Reduktion auf ausgew√§hler Features\n  fcm_select(pattern = c(\"fight\", \"man\", \n                         \"love\", \"young\", \"woman\"), \n             selection = \"keep\")\n\nWir visualisieren die Ergebnisse Co-Occurence-Analyse mit der textplot_network()Funktion aus dem quanteda.textplots()-Paket.\nWenn Features im selben Kontext (hier 8 W√∂rter) vorkommen, werden sie mit einer Linie verbunden. Umso dicker die Linie, desto √∂fter Features gemeinsam vor - sie ‚Äúco-occurren‚Äù also.\nDabei k√∂nnen wir sehen, dass M√§nner wie Frauen h√§ufig als ‚Äújung‚Äù beschrieben werden - M√§nner aber h√§ufiger mit ‚ÄúK√§mpfen‚Äù und Frauen mit ‚ÄúLiebe‚Äù assoziiert werden:\n\n#Plot des semantischen Netzwerks\ntextplot_network(fcm)"
  },
  {
    "objectID": "tutorial/02_cooccurence_kollokationen_postagging_depparsing.html#collocations",
    "href": "tutorial/02_cooccurence_kollokationen_postagging_depparsing.html#collocations",
    "title": "Sitzung 2: Co-Occurence Analysen",
    "section": "2.4 Collocations",
    "text": "2.4 Collocations\nEin weiterer Ansatz sind Collocations. Collocations bezeichnen Features, die h√§ufig (und damit vermutlich nicht-zuf√§llig) nacheinander auftreten, was auf eine gemeinsame semantische Bedeutung hindeutet (z.B. United und States).\nUm herauszufinden, welche Features vorkommen, k√∂nnen wir die Funktion textstat_collocations aus dem quanteda.textstats-Paket verwenden.\n\ntokens %&gt;%\n  \n  #Identifikation von Collocations, die mind. 10 Mal vorkommen\n  textstat_collocations(min_count = 10) %&gt;%\n  \n  #Sortierung nach lambda: Je gr√∂sser, \n  #desto wahrscheinlicher handelt es sich um nicht-zuf√§llige Collocations\n  arrange(-lambda) %&gt;%\n  \n  #Ausgabe der h√§ufigsten 10 Collocation\n  head(10)\n\n        collocation count count_nested length    lambda         z\n8         los angel    22            0      2 11.992530  7.856166\n9          new york    39            0      2  9.635186  6.744491\n5     serial killer    10            0      2  8.665918 11.849150\n4 person profession    13            0      2  7.817347 12.191558\n7     antholog seri    10            0      2  7.632806  8.612064\n1       high school    22            0      2  7.041380 16.490961\n3       best friend    25            0      2  7.006030 15.088077\n2         york citi    19            0      2  5.810906 16.071342\n6       seri follow    10            0      2  4.323497 11.362725\n\n\nDas kennen wir doch schon - n√§mlich aus Abschnitt 2.1 zu ngrams."
  },
  {
    "objectID": "tutorial/02_cooccurence_kollokationen_postagging_depparsing.html#semantische-netzwerke-auf-basis-von-co-occurenzen",
    "href": "tutorial/02_cooccurence_kollokationen_postagging_depparsing.html#semantische-netzwerke-auf-basis-von-co-occurenzen",
    "title": "Sitzung 2: Co-Occurence Analysen",
    "section": "2.3 Semantische Netzwerke auf Basis von Co-Occurenzen",
    "text": "2.3 Semantische Netzwerke auf Basis von Co-Occurenzen\nAls st√§rker quantifizierende Analyse bieten sich semantische Netzwerke an. Diese visualisieren, welche Features h√§ufig in einem gemeinsamen Kontext (z.B. in direkter Abfolge, innerhalb eines Textes) vorkommen.\nHierf√ºr m√ºssen wir zun√§chst die DFM in eine Feature-Co-occurrence Matrix (FCM) umwandeln. Das machen wir mit der Funktion fcm() aus dem quanteda-Paket.\nUns interessiert zun√§chst, wie h√§ufig Features innerhalb eines Textes vorkommen, weshalb wir das context-Argument nutzen und auf document setzen.\n\ntokens %&gt;%\n  \n  #Umwandlung in eine Feature-Co-Occurence-Matrix\n  fcm(context = \"document\") %&gt;%\n  \n  #Ausgabe der ersten Zeilen\n  head()\n\nFeature co-occurrence matrix of: 6 by 4,246 features.\n         features\nfeatures  nine nobl famili fight control land westero ancient enemi return\n  nine       0    1      1     1       1    1       1       1     1      1\n  nobl       0    0      1     1       1    1       1       1     1      1\n  famili     0    0      8     6       2    3       1       1     4      6\n  fight      0    0      0     1       2    2       1       2     2      2\n  control    0    0      0     0       0    1       1       1     1      1\n  land       0    0      0     0       0    0       1       1     1      1\n[ reached max_nfeat ... 4,236 more features ]\n\n\nWir sehen:\n\nDie FCM besteht aus 4,246 Features (da wir nur wenig Preprocessing genutzt haben, sind dies noch recht viele Features).\nDie Zellen der FCM illustrieren, wie h√§ufig welche W√∂rter in der Beschreibung einer Serie mittels Description gemeinsam vorkommen.\n\nWir k√∂nnen jetzt spezifische Features ausw√§hlen, die uns interessieren. Das machen wir mit der Funktion fcm_select() aus dem quanteda-Paket.\nZudem k√∂nnten wir √ºber das window-Argument noch spezifizieren, dass uns die Co-Occurence von Features nicht innerhalb der gesamten Serienbeschreibung, sondern nur innerhalb eines Windows von z. B. acht Featuren interessiert. Ein Argument hierf√ºr w√§re, dass Feature, die nah aufeinander folgen, auch st√§rker eine inhaltlich geteilte Bedeutung haben. Zudem setzen wir das selection-Aargument auf keep, um die FCM nur auf die angegebenen Features zu reduzieren.\nBeispielsweise wollen wir uns anschauen, ob M√§nner (man) und Frauen (woman) in Serienbeschreibung stereotypisiert, z. B. im Hinblick auf Geschlechterrollen, dargestellt werden.\nDaf√ºr schauen wir, welche W√∂rter (z.B. fight vs.¬†romance) h√§ufiger in Verbindung mit M√§nnern bzw. Frauen genannt werden\n\nfcm &lt;- tokens %&gt;%\n  \n  #Erstellung einer FCM mit einem Window von 8\n  fcm(window = 8) %&gt;%\n  \n  #Reduktion auf ausgew√§hler Features\n  fcm_select(pattern = c(\"fight\", \"man\", \n                         \"love\", \"young\", \"woman\"), \n             selection = \"keep\")\n\nWir visualisieren die Ergebnisse als semantisches Netzwerk mit der textplot_network()Funktion aus dem quanteda.textplots()-Paket.\nWenn Features im selben Kontext (hier 8 W√∂rter) vorkommen, werden sie mit einer Linie verbunden. Umso dicker die Linie, desto √∂fter kommen Features gemeinsam vor.\nDabei k√∂nnen wir sehen, dass M√§nner wie Frauen h√§ufig als ‚Äújung‚Äù beschrieben werden - M√§nner aber h√§ufiger mit ‚ÄúK√§mpfen‚Äù und Frauen mit ‚ÄúLiebe‚Äù assoziiert werden:\n\n#Plot des semantischen Netzwerks\ntextplot_network(fcm)"
  },
  {
    "objectID": "tutorial/02_cooccurence_kollokationen_postagging_depparsing.html#ngrams-als-ein-features-zusammenfassen",
    "href": "tutorial/02_cooccurence_kollokationen_postagging_depparsing.html#ngrams-als-ein-features-zusammenfassen",
    "title": "Sitzung 2: Co-Occurence Analysen",
    "section": "2.5 Ngrams als ein Features zusammenfassen",
    "text": "2.5 Ngrams als ein Features zusammenfassen\nIn einem weiteren Schritt kann es manchmal sinnvoll sein, Collocations f√ºr die weitere Analyse zu einem Feature zusammenzufassen (dieser Schritt w√§re dann Teil des Preprocessings). Beispielsweise wollen wir, dass United immer mit States als einzelnes Feature United States verstanden wird, sobald diese beiden Features in dieser Abfolge genutzt werden (statt diese in einzelne Features zu splitten).\nDieses Zusammenfassen (compounding) l√§sst sich mit der tokens_compound()-Funktion aus dem quanteda.textstats-Paket umsetzen.\nDie Funktion verbindet unigrams mit einem Unterstrich zu einem ngram, das als ein einziges Feature weiter analysiert wird. Dieser Schritt wird zwischen dem tokens-Befehl und dem dfm-Befehle (d.h. der Tokenisierung und der Erstellung einer Document-Feature-Matrix) eingef√ºgt.\n\n#Definition h√§ufiger Collocations auf Basis der vorherigen Ausgabe\nngrams &lt;- c(\"los angel\",\"new york citi\", \"serial killer\", \"high school\", \"best friend\")\n\n#Text-as-Data Repr√§sentation als Document-Feature-Matrix\ndfm &lt;- tokens %&gt;% \n  \n  #Zus√§tzlicher Schritt, um Collocations als einzelnes Feature einzulesen\n  tokens_compound(pattern = phrase(ngrams)) %&gt;%\n  \n  #regul√§re DFM, inkl. relative pruning\n  dfm() %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE) \n\n#Beispiel: Einlesen des Features \"Los Angeles\"\ndfm %&gt;%\n  \n  #Umwandlung zu Data Frame\n  convert(to = \"data.frame\") %&gt;%\n  \n  #Reduktion auf Doc ID und Features, die mit \"los\" beginnen\n  select(doc_id, starts_with(\"los\")) %&gt;%\n  \n  #Ausgabe ausgew√§hlter Serien (Zeile 125 bis 130)\n  slice(125:130)\n\n   doc_id lost los_angel\n1 text125    0         0\n2 text126    0         1\n3 text127    0         0\n4 text128    0         0\n5 text129    0         1\n6 text130    0         0"
  },
  {
    "objectID": "tutorial/02_cooccurence.html",
    "href": "tutorial/02_cooccurence.html",
    "title": "Sitzung 2: Co-Occurence Analysen",
    "section": "",
    "text": "Zun√§chst installieren alle Pakete, die wir f√ºr diese Sitzung brauchten (z.B. tidyverse). Ihr braucht install.packages() nur, wenn ihr die Pakete im Methodencafe noch nicht installiert hattet.\n\n#install.packages(\"tidyverse)\n#install.packages(\"quanteda\")\n#install.packages(\"quanteda.textplots\")\n#install.packages(\"RCurl\")\n#install.packages(\"quanteda.textstats\")\n#install.packages(\"udpipe\")\n\nlibrary(\"tidyverse\")\nlibrary(\"quanteda\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"RCurl\")\nlibrary(\"quanteda.textstats\")\nlibrary(\"udpipe\")\n\nNeu hinzugekommen seit dem Methodencafe ist das rsyntax-Paket, das ihr neu installieren m√ºsstest:\n\ninstall.packages(\"rsyntax\")\nlibrary(\"rsyntax\")\n\nNun laden wir die Daten wieder ein und f√ºhren die bereits erlernten Preprocessing-Schritte, inkl. der Transformation in eine Document-Feature-Matrix, aus:\n\n#Daten laden\nurl &lt;-  getURL(\"https://raw.githubusercontent.com/valeriehase/textasdata-ms/main/data/data_tvseries.csv\")\ndata &lt;- read.csv2(text = url)\n\n#Preprocessing\ntokens &lt;- tokens(data$Description,\n                 what = \"word\", #Tokenisierung, hier zu W√∂rtern als Analyseeinheit\n                 remove_punct = TRUE, #Entfernung von Satzzeichen\n                 remove_numbers = TRUE) %&gt;% #Entfernung von Zahlen\n  \n  #Kleinschreibung\n  tokens_tolower() %&gt;% \n  \n  #Entfernung von Stoppw√∂rtern\n  tokens_remove(stopwords(\"english\")) %&gt;% \n  \n  #Stemming\n  tokens_wordstem()\n\n#Text-as-Data Repr√§sentation als Document-Feature-Matrix\ndfm &lt;- tokens %&gt;% \n  dfm() %&gt;% \n  \n  #Relative pruning\n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE) \n\nJetzt sind wir bereit f√ºr die ersten Co-Occurence-Analysen!"
  },
  {
    "objectID": "tutorial/02_cooccurence.html#n-grams",
    "href": "tutorial/02_cooccurence.html#n-grams",
    "title": "Sitzung 2: Co-Occurence Analysen",
    "section": "2.1 N-grams",
    "text": "2.1 N-grams\nZun√§chst schauen wir uns als Beispiel f√ºr ngrams bigrams, d.h. Abfolgen zweier W√∂rter, an:\n\ntokens %&gt;%\n  \n  #Umwandlung in bigrams\n  tokens_ngrams(n = 2) %&gt;%\n  \n  #Ausgabe f√ºr erstes Dokument\n  head(1)\n\nTokens consisting of 1 document.\ntext1 :\n [1] \"nine_nobl\"         \"nobl_famili\"       \"famili_fight\"     \n [4] \"fight_control\"     \"control_land\"      \"land_westero\"     \n [7] \"westero_ancient\"   \"ancient_enemi\"     \"enemi_return\"     \n[10] \"return_dormant\"    \"dormant_millennia\"\n\n\nAus Sitzung 1 kennen wir ja bereits den Befehl, um uns die h√§ufigsten Features ausgeben zu lassen. Nun lassen wir uns nicht die h√§ufigsten unigrams (einzelne W√∂rter), sondern die h√§ufigsten bigrams (Abfolge von zwei W√∂rtern) ausgeben.\n\ntokens %&gt;%\n  \n  #Umwandlung in bigrams\n  tokens_ngrams(n = 2) %&gt;%\n  \n  #Umwandlung in dfm f√ºr topfeatures-Befehl\n  dfm() %&gt;%\n  \n  #Ausgabe der h√§ufigsten Features\n  topfeatures(10) %&gt;%\n  \n  #Umwandlung in einen \"sch√∂neren\" Dataframe mit der Spalte \"H√§ufigkeit\"\n  as.data.frame() %&gt;%\n  rename(\"H√§ufigkeit\" = '.')\n\n                  H√§ufigkeit\nnew_york                  39\nbest_friend               25\nhigh_school               22\nlos_angel                 22\nyork_citi                 19\nperson_profession         13\nserial_killer             10\nantholog_seri             10\nseri_follow               10\nyoung_boy                  9\n\n\nWir sehen, dass unter den h√§ufigsten bigrams einige Features sind, die in Kombination miteinander eine andere Bedeutung haben als als einzelne unigrams. Dazu geh√∂ren z.B. Orte (z.B. New York) oder Ausdr√ºcke (z. B. serial killer)."
  },
  {
    "objectID": "tutorial/02_cooccurence.html#keywords-in-context-kwic",
    "href": "tutorial/02_cooccurence.html#keywords-in-context-kwic",
    "title": "Sitzung 2: Co-Occurence Analysen",
    "section": "2.2 Keywords-in-Context (KWIC)",
    "text": "2.2 Keywords-in-Context (KWIC)\nAls eher qualitativer Einblick bietet sich die Analyse von Konkordanzen, d.h. der Analyse von Schl√ºsselw√∂rtern und ihres Kontexts, im Text an.\nDie kwic()Funktion aus dem quanteda-Paket identifiziert hierzu Schl√ºsselw√∂rter und W√∂rter vor bzw. nach diesen. Wir schauen uns also W√∂rter an, die um ein sogenanntes Window) von z.B. einem oder mehr W√∂rtern vor oder nach dem Schl√ºsselwort vorkommen.\nMit folgendem Code k√∂nnen wir beispielsweise herausfinden, in welchem Kontext das Wort hero vorkommt. In der Ausgabe werden jeweils das Wort vor und nach dem Wort hero angezeigt. Um die Bedeutung dieser W√∂rter besser zu verstehen, nutzen wir dabei die ‚Äúunbereinigte‚Äù Description-Variable aus dem Objekt data.\n\ndata$Description %&gt;% \n  \n  #Keywords-in-Context mit Window von 5 W√∂rtern vor und nach Schl√ºsselwort\n  kwic(pattern = \"hero\", \n       window = 1) %&gt;%\n  \n  #Ausgabe der ersten Zeilen\n  head()\n\nKeyword-in-context with 5 matches.                                           \n  [text124, 7]           a | hero | that   \n [text140, 38]       Rebel | hero | .      \n  [text336, 8] prestigious | hero | academy\n [text336, 19]           a | hero | ,      \n  [text756, 8]           a | hero | of"
  },
  {
    "objectID": "tutorial/02_cooccurence.html#semantische-netzwerke-auf-basis-von-co-occurenzen",
    "href": "tutorial/02_cooccurence.html#semantische-netzwerke-auf-basis-von-co-occurenzen",
    "title": "Sitzung 2: Co-Occurence Analysen",
    "section": "2.3 Semantische Netzwerke auf Basis von Co-Occurenzen",
    "text": "2.3 Semantische Netzwerke auf Basis von Co-Occurenzen\nAls st√§rker quantifizierende Analyse bieten sich semantische Netzwerke an. Diese visualisieren, welche Features h√§ufig in einem gemeinsamen Kontext (z.B. in direkter Abfolge, innerhalb eines Textes) vorkommen.\nHierf√ºr m√ºssen wir zun√§chst die DFM in eine Feature-Co-occurrence Matrix (FCM) umwandeln. Das machen wir mit der Funktion fcm() aus dem quanteda-Paket.\nUns interessiert zun√§chst, wie h√§ufig Features innerhalb eines Textes vorkommen, weshalb wir das context-Argument nutzen und auf document setzen.\n\ntokens %&gt;%\n  \n  #Umwandlung in eine Feature-Co-Occurence-Matrix\n  fcm(context = \"document\") %&gt;%\n  \n  #Ausgabe der ersten Zeilen\n  head()\n\nFeature co-occurrence matrix of: 6 by 4,246 features.\n         features\nfeatures  nine nobl famili fight control land westero ancient enemi return\n  nine       0    1      1     1       1    1       1       1     1      1\n  nobl       0    0      1     1       1    1       1       1     1      1\n  famili     0    0      8     6       2    3       1       1     4      6\n  fight      0    0      0     1       2    2       1       2     2      2\n  control    0    0      0     0       0    1       1       1     1      1\n  land       0    0      0     0       0    0       1       1     1      1\n[ reached max_nfeat ... 4,236 more features ]\n\n\nWir sehen:\n\nDie FCM besteht aus 4,246 Features (da wir nur wenig Preprocessing genutzt haben, sind dies noch recht viele Features).\nDie Zellen der FCM illustrieren, wie h√§ufig welche W√∂rter in der Beschreibung einer Serie mittels Description gemeinsam vorkommen.\n\nWir k√∂nnen jetzt spezifische Features ausw√§hlen, die uns interessieren. Das machen wir mit der Funktion fcm_select() aus dem quanteda-Paket.\nZudem k√∂nnten wir √ºber das window-Argument noch spezifizieren, dass uns die Co-Occurence von Features nicht innerhalb der gesamten Serienbeschreibung, sondern nur innerhalb eines Windows von z. B. acht Featuren interessiert. Ein Argument hierf√ºr w√§re, dass Feature, die nah aufeinander folgen, auch st√§rker eine inhaltlich geteilte Bedeutung haben. Zudem setzen wir das selection-Aargument auf keep, um die FCM nur auf die angegebenen Features zu reduzieren.\nBeispielsweise wollen wir uns anschauen, ob M√§nner (man) und Frauen (woman) in Serienbeschreibung stereotypisiert, z. B. im Hinblick auf Geschlechterrollen, dargestellt werden.\nDaf√ºr schauen wir, welche W√∂rter (z.B. fight vs.¬†romance) h√§ufiger in Verbindung mit M√§nnern bzw. Frauen genannt werden\n\nfcm &lt;- tokens %&gt;%\n  \n  #Erstellung einer FCM mit einem Window von 8\n  fcm(window = 8) %&gt;%\n  \n  #Reduktion auf ausgew√§hler Features\n  fcm_select(pattern = c(\"fight\", \"man\", \n                         \"love\", \"young\", \"woman\"), \n             selection = \"keep\")\n\nWir visualisieren die Ergebnisse als semantisches Netzwerk mit der textplot_network()Funktion aus dem quanteda.textplots()-Paket.\nWenn Features im selben Kontext (hier 8 W√∂rter) vorkommen, werden sie mit einer Linie verbunden. Umso dicker die Linie, desto √∂fter kommen Features gemeinsam vor.\nDabei k√∂nnen wir sehen, dass M√§nner wie Frauen h√§ufig als ‚Äújung‚Äù beschrieben werden - M√§nner aber h√§ufiger mit ‚ÄúK√§mpfen‚Äù und Frauen mit ‚ÄúLiebe‚Äù assoziiert werden:\n\n#Plot des semantischen Netzwerks\ntextplot_network(fcm)"
  },
  {
    "objectID": "tutorial/02_cooccurence.html#collocations",
    "href": "tutorial/02_cooccurence.html#collocations",
    "title": "Sitzung 2: Co-Occurence Analysen",
    "section": "2.4 Collocations",
    "text": "2.4 Collocations\nEin weiterer Ansatz sind Collocations. Collocations bezeichnen Features, die h√§ufig (und damit vermutlich nicht-zuf√§llig) nacheinander auftreten, was auf eine gemeinsame semantische Bedeutung hindeutet (z.B. United und States).\nUm herauszufinden, welche Features vorkommen, k√∂nnen wir die Funktion textstat_collocations aus dem quanteda.textstats-Paket verwenden.\n\ntokens %&gt;%\n  \n  #Identifikation von Collocations, die mind. 10 Mal vorkommen\n  textstat_collocations(min_count = 10) %&gt;%\n  \n  #Sortierung nach lambda: Je gr√∂sser, \n  #desto wahrscheinlicher handelt es sich um nicht-zuf√§llige Collocations\n  arrange(-lambda) %&gt;%\n  \n  #Ausgabe der h√§ufigsten 10 Collocation\n  head(10)\n\n        collocation count count_nested length    lambda         z\n8         los angel    22            0      2 11.992530  7.856166\n9          new york    39            0      2  9.635186  6.744491\n5     serial killer    10            0      2  8.665918 11.849150\n4 person profession    13            0      2  7.817347 12.191558\n7     antholog seri    10            0      2  7.632806  8.612064\n1       high school    22            0      2  7.041380 16.490961\n3       best friend    25            0      2  7.006030 15.088077\n2         york citi    19            0      2  5.810906 16.071342\n6       seri follow    10            0      2  4.323497 11.362725\n\n\nDas kennen wir doch schon - n√§mlich aus Abschnitt 2.1 zu ngrams."
  },
  {
    "objectID": "tutorial/02_cooccurence.html#ngrams-als-ein-features-zusammenfassen",
    "href": "tutorial/02_cooccurence.html#ngrams-als-ein-features-zusammenfassen",
    "title": "Sitzung 2: Co-Occurence Analysen",
    "section": "2.5 Ngrams als ein Features zusammenfassen",
    "text": "2.5 Ngrams als ein Features zusammenfassen\nIn einem weiteren Schritt kann es manchmal sinnvoll sein, Collocations f√ºr die weitere Analyse zu einem Feature zusammenzufassen (dieser Schritt w√§re dann Teil des Preprocessings). Beispielsweise wollen wir, dass United immer mit States als einzelnes Feature United States verstanden wird, sobald diese beiden Features in dieser Abfolge genutzt werden (statt diese in einzelne Features zu splitten).\nDieses Zusammenfassen (compounding) l√§sst sich mit der tokens_compound()-Funktion aus dem quanteda.textstats-Paket umsetzen.\nDie Funktion verbindet unigrams mit einem Unterstrich zu einem ngram, das als ein einziges Feature weiter analysiert wird. Dieser Schritt wird zwischen dem tokens-Befehl und dem dfm-Befehle (d.h. der Tokenisierung und der Erstellung einer Document-Feature-Matrix) eingef√ºgt.\n\n#Definition h√§ufiger Collocations auf Basis der vorherigen Ausgabe\nngrams &lt;- c(\"los angel\",\"new york citi\", \"serial killer\", \"high school\", \"best friend\")\n\n#Text-as-Data Repr√§sentation als Document-Feature-Matrix\ndfm &lt;- tokens %&gt;% \n  \n  #Zus√§tzlicher Schritt, um Collocations als einzelnes Feature einzulesen\n  tokens_compound(pattern = phrase(ngrams)) %&gt;%\n  \n  #regul√§re DFM, inkl. relative pruning\n  dfm() %&gt;% \n  dfm_trim( min_docfreq = 0.005, \n            max_docfreq = 0.99, \n            docfreq_type = \"prop\", \n            verbose = TRUE) \n\n#Beispiel: Einlesen des Features \"Los Angeles\"\ndfm %&gt;%\n  \n  #Umwandlung zu Data Frame\n  convert(to = \"data.frame\") %&gt;%\n  \n  #Reduktion auf Doc ID und Features, die mit \"los\" beginnen\n  select(doc_id, starts_with(\"los\")) %&gt;%\n  \n  #Ausgabe ausgew√§hlter Serien (Zeile 125 bis 130)\n  slice(125:130)\n\n   doc_id lost los_angel\n1 text125    0         0\n2 text126    0         1\n3 text127    0         0\n4 text128    0         0\n5 text129    0         1\n6 text130    0         0"
  }
]