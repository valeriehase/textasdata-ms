#### 1. Pakete laden und Daten einlesen ####
  
#install.packages("tidyverse)
#install.packages("quanteda")
#install.packages("quanteda.textplots")
#install.packages("RCurl")

library("tidyverse")
library("quanteda")
library("quanteda.textplots")
library("RCurl")

##### 1.1 Textdaten aus einer lokalen Datei einlesen #####
# data <- read.csv2("data_tvseries.csv")

##### 1.2 Textdaten von einer URL downloaden #####

url <-  getURL("https://raw.githubusercontent.com/valeriehase/textasdata-ms/main/data/data_tvseries.csv")
data <- read.csv2(text = url)

#Check der Daten
head(data)

#### 2. Preprocessing ####
  
##### 2.1 Bereinigung (z. B. Encoding-Probleme) ##### 
  
data %>%
  
  #Auswahl der Variable "Description"
  select(Description) %>% 
  
  #Reduktion auf ersten Text
  slice(1)

###### 2.1.1 Encoding-Probleme ######

#Beispiel-Satz
string <- "SchÃ¶ne GrÃ¼ÃŸe aus MÃ¼nchen"

#Encoding prÃ¼fen
Encoding(string)

#Encoding testweise Ã¤ndern
Encoding(string) <- "latin1"
string

#Mit Hilfe von regulÃ¤ren AusdrÃ¼cken bereinigen
string %>% 
  
  #Ersatz fÃ¼r falsches Encoding "Ã¶"
  gsub(pattern = "ÃƒÂ¶", replacement ="Ã¶") %>% 
  
  #Ersatz fÃ¼r falsches Encoding "Ã¼"
  gsub(pattern = "ÃƒÂ¼", replacement = "Ã¼") %>% 
  
  #Ersatz fÃ¼r falsches Encoding "ÃŸ"
  gsub(pattern = "ÃƒÅ¸", replacement = "ÃŸ") 

###### 2.1.2 Datenbereinigung mit regulÃ¤ren AusdrÃ¼cken ######

#Schauen wir uns den Titel an
data %>%
  select(Title) %>%
  head(5)

#Entfernung der Zeichen vor dem Titel der TV-Serie
data <- data %>%
  mutate(Title = gsub("^[0-9]+[[:punct:]] ", "", Title))

#So sieht das Ergebnis aus:
data %>%
  select(Title) %>%
  head(5)

###### 2.1.3 Datenfilterung mit regulÃ¤ren AusdrÃ¼cken ######

#TV-Serien, die sich um Drama drehen
data %>%
  
  # filtern aller TV_Serien, die "Drama" in der Beschreibung beinhalten
  filter(grepl("[D|d]rama", Description)) %>%
  
  # Inspektion der ersten fÃ¼nf Titel
  select(Title) %>%
  head(5)

#TV-Serien, die sich um Drama oder Crime drehen
data %>%
  
  # filtern aller TV_Serien, die "Drama"und "Crime" in der Beschreibung beinhalten
  filter(grepl("[D|d]rama|[C|c]rime", Description)) %>%
  
  # Inspektion der ersten fÃ¼nf Titel
  select(Title) %>%
  head(5)

###### 2.1.4 Aufgabe 1 ğŸ“Œ ######

# Basis: Alle Serien identifizieren, die in Deutschland spielen?
  
# Fortgeschritten: Alle Serien identifizieren, in denen es um Superhelden geht und "*superhero/superheroes*â€ in der Variable `Description` mit "*fancy R programmers*â€œ ersetzen?

##### 2.2 Normalisierung #####
tokens <- tokens(data$Description,
                 what = "word", #Tokenisierung, hier zu WÃ¶rtern als Analyseeinheit
                 remove_punct = TRUE, #Entfernung von Satzzeichen
                 remove_numbers = TRUE) %>% #Entfernung von Zahlen
  
  #Kleinschreibung
  tokens_tolower() %>% 
  
  #Entfernung von StoppwÃ¶rtern
  tokens_remove(stopwords("english")) %>% 
  
  #Stemming
  tokens_wordstem()

#So sah unser erster Text vor dem Preprocessing aus
data$Description[1]

#Und so danach
tokens[1]

###### 2.2.1 Mehr Infos zur Entfernung von StoppwÃ¶rtern ######
Es gibt verschiedene MÃ¶glichkeiten, StoppwÃ¶rter zu entfernen. Am einfachsten ist dies mithilfe der im `quanteda`-Paket integrierten Stoppwortlisten mÃ¶glich. Diese sind in mehreren Sprachen verfÃ¼gbar, darunter auch Deutsch. 

#WÃ¶rter aus der quanteda Stoppwortliste entfernen
stoppwÃ¶rter <- stopwords("english")
stoppwÃ¶rter <- stoppwÃ¶rter[!stoppwÃ¶rter %in% c("i", "me")]

#Beispielhafte Anwendung 
tokens(data$Description,
       what = "word", #Tokenisierung, hier zu WÃ¶rtern als Analyseeinheit
       remove_punct = TRUE, #Entfernung von Satzzeichen
       remove_numbers = TRUE) %>% #Entfernung von Zahlen
  
  #Kleinschreibung
  tokens_tolower() %>% 
  
  #Entfernung von StoppwÃ¶rtern - hier z.B. reduzierte quanteda-Liste
  tokens_remove(stoppwÃ¶rter) %>% 
  
  #Stemming
  tokens_wordstem() %>%
  
  #Ausgabe des ersten Textes
  head(1)

####### 2.2.2 Aufgabe 2 ğŸ“Œ #######

#Basis: Eine Liste mit 3-5 StopwÃ¶rtern erstellen und diese als Teil des Preprocessings zusÃ¤tzlich entfernen?
  
#Fortgeschritten: DafÃ¼r sorgen, dass Namen von StÃ¤dten (hier als Beispiel â€New Yorkâ€œ) als ein einzelnes Feature beibehalten werden?
  
##### 3. Text-as-Data-ReprÃ¤sentation #####
  
###### 3.1 Erstellung einer DFM ######
  
#Wir erstellen eine Document-Feature matrix
dfm <- tokens %>%
  dfm()

#So sieht das Ergebnis aus
dfm

###### 3.2 ZusÃ¤tzliche Normalisierung: Relative Pruning ######

#Anzahl Features vor relative pruning
dfm

#Anwendung des relative pruning
dfm  <- dfm  %>% 
  dfm_trim( min_docfreq = 0.005, 
            max_docfreq = 0.99, 
            docfreq_type = "prop", 
            verbose = TRUE) 

#Anzahl Features nach relative pruning
dfm
  
##### 4. Erste Analysen #####
  
###### 4.1 Top Features ######
  
topfeatures(dfm, 10) %>%
  
  #Umwandlung in einen "schÃ¶neren" Dataframe mit der Spalte "HÃ¤ufigkeit"
  as.data.frame() %>%
  rename("HÃ¤ufigkeit" = '.')

###### 4.2. Die berÃ¼hmt-berÃ¼chtigte Word Cloud ######
textplot_wordcloud(dfm, max_words = 100)