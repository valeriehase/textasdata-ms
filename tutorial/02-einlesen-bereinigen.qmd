---
title: "Sitzung 2 - Einlesen & Bereinigen von Text"
author: "Valerie Hase & Luisa Kutlar"
date: "05.04.2024"
format:
  html:
    toc: true
    html-math-method: katex
    css: styles.css
---

#Gliederung

# Paket "readtext" installieren

Wenn ihr beim Workshop kein Internet habt, könnt ihr das Paket auch weglassen.

```{r echo = TRUE, eval = FALSE, message = FALSE, warning = FALSE, error = FALSE}
install.packages("readtext")
```

# Pakete "aktivieren"

Als nächstes laden wir über die folgenden Befehle die Pakete, mit denen wir Daten einlesen und bereinigen werden.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
library("readtext")
library("quanteda")
```

# Daten einlesen

Es gibt einige Packete mit denen Daten in R je nach Typ eingelesen werden können. Hier nutzen wir das packet "readtext", dass wir oben bereits installiert und geladen haben.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
# setwd("your/working/directory")  
# data <- readtext(data.csv) 
```

Mit dem package "readtext" können beispielsweise auch diese Dateiformate eingelesen werden: .txt .json .html .pdf .doc .docx. Für einen besseren Überblick siehe <a href="https://cran.r-project.org/web/packages/readtext/readtext.pdf">hier</a>.

Packete für andere Datentypen

::: incremental
-   utils: .csv Dateien
-   readxl und xlsx: für das Arbeiten mit Excel Dateien, vor allem .xls und .xlsx Dateien
-   foreign: für das Arbeiten mit vielen anderen Dateitypen, wie SPSS oder STATA Dateien
:::

# Text as Data: Datentypen

Mit den folgenden Datentypen werden wir arbeiten:

::: incremental
-   Data frame
-   Corpus
-   Tokens & Types
-   Document-Feature-Matrix (dfm)
:::

# Data frame

::: incremental
-   Objekt in dem alle eingelesen Daten zunächst gespeichert werden
-   Jede Zeile ist ein Dokument
-   Jede Spalte ist eine Variable
:::

# Corpus

::: incremental
-   Ähnlich zum data frame
-   Üblicherweise werden hier die Dokumenten ID und die Textinhalte übernommen
-   Alle hier übernommenen Dokumente sollten einem Typ entsprechen (z.B. Zeitungsartikel, Instagram Posts)
-   Jede Zeile ist ein Dokument
-   Jede Spalte ist eine Variable
:::

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
library(quanteda)
# corpus <- data %>% 
#  corpus(text_field = "text", docid_field = "doc_id")
```

Bild Corpus

# Tokens & Types

::: incremental
-   Bei der Tokenisierung werden Texte in Analyseeinheiten (Token) unterteilt
-   Das können einzelne Wörter (Unigramme), Wortketten (N-Gramme), ganze Sätze, Zahlen, Satzzeichen oder Emojis sein
-   Types sind eindeutige Tokens
-   Der Satz "Coden macht Spaß, weil Coden kreativ ist." beinhaltet 9 Tokens und 8 Typen, weil das Wort "Coden" zwei mal vor kommt
-   Types werden auch features genannt
:::

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
# tokens <- corpus %>% 
#  tokens(what = "word")
```

Bild Tokens

# Document-Feature-Matrix (dfm)

Um Text für den Computer "lesbar" zu machen müssen wir ihn in Zahlen überführen. Das machen wir mit der Document-Feature-Matrix, in der

::: incremental
-   die Zeilen alle Dokumente darstellen
-   die Spalten alle features der Dokumente beinhalten
-   die Zellen angeben, wie oft das jeweilig feature im Dokument vorkommt
:::

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
# dfm <- tokens() %>% 
#  dfm()
```

Bild dfm

# Bag of words

Beim bag of words Modell wird der Syntax bei der Analyse nicht berücksichtigt (siehe dfm). Es wird also angenommen, dass die Reihenfolge von Wörtern keinen Einfluss auf die Interpretation dieser hat.

Bild einfügen

# Preprocessing

Ziel: Text vereinfachen, (systematische) Fehler reduzieren und Text vergleichbar über mehrere Dokumente machen

Mit der wichtigste Schritt bei der automatisierten Inhaltsanalyse. Die hier gefällten Entscheidungen wirken sich maßgeblich auf die Ergebnisse aus. Preprocessing umfasst die folgenden Punkte (nicht alle Schritte müssen immer für die eigenen Analyse zielführend sein):

::: incremental
-   Definiton eines Dokuments festlegen
-   Endcoding issues checken
-   Zahlen, Satzzeichen ... entfernen
-   Normalisierung auf Kleinbuchstaben
-   Stoppwörter entfernen
-   Vereinheitlichung
-   Lemmatizing/Stemming
-   Seltene/Häufige features entfernen
:::

# Definition eines Dokuments festlegen

::: incremental
-   Was ist die angestrebte Analyseeinheit und wofür wird sie benötigt? (z.B. ganzer Artikel, Text-Absätze, Satz)
-   Abwägung zwischen Effizienz und Effektivität
-   Primär theoretischer Schritt
:::

# Encoding issues chekchen

::: incremental
-   Das Problem: Computer speichern Zeichen indem sie ihnen numerischen Code (Bytes) zuordnen
-   z.B. "Wort" wir als "01110111011011110111001001100100" gespeichert
-   Encoding als der Vorgang den numerischen Code wieder in Zeichen zu übersetzen
-   Problem: Co-Existenz von mehreren Encodings (z.B. bei "ß", "ü", "ğ" oder emojis)
-   Lösung: Direkt beim Einlesen das richtige Encoding verwenden ODER manuell bereinigen
:::

######## neu nur Code

# 1 Daten einlesen

Zunächst installieren und laden wir alle Packete, die wir heute und morgen brauchen.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#install.packages("RCurl")
#install.packages("quanteda")

library("RCurl")
library("quanteda")
library("tidyverse")
library("dplyr")
```

Dann laden wir die Datei die hinter dem Link liegt mit der Funktion `getURL()` aus dem package Rcurl herunter. Mit einem Blick in das Environment sehen wir, dass die einzelnen Wörter mit einem ; getrennt werden . Daher brauchen wir die Funktion `read.csv2()` aus dem utils package - das package ist vorinstalliert und immer geladen - um die Daten in R einzulesen. Der Datensatz wird im Objekt `daten_df` gespeichert.

```{r echo = TRUE, message = FALSE, error = FALSE}
url <-  getURL("https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv")
daten_df <-  read.csv2(text = url)
```

Alternativ können Daten in einer .csv Datei auch mit einem , voneinander abgetrennt sein. Hier bräuchte es dann die Funktion `read.csv()` zum Einlesen.

Nach dem Einlesen der Daten ist es üblich sich zunächst einen Überblick über die Daten zu machen und zu kontrollieren, ob alles korrekt eingelesen wurde. 

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
head(daten_df)
View(daten_df)
str(daten_df)
```

# 2 Preprocessing

## 2.1 Encoding issues checken

Nach dem Einlesen haben wir bereits einen Blick in den Datensatz geworfen. Nun schauen wir uns gezielt die Textvariable `description` an, um zu überprüfen, ob alle Zeichen richtig dargestellt werden. 

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_df %>% 
   select(Description) %>% 
  head()
```

In diesem Fall gibt es keine Encoding issues.

Was tun falls doch?

::: incremental
-   Beim Einlesen das richtige Encoding mitgeben
-   Manuell bereinigen 
:::

Beim manuellen Bereinigen kann die Funktion `gsub()` helfen, die Zeichenketten ersetzen kann. Zum Beispiel so: 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#string mit encoding issues
string <- "SchÃ¶ne GrÃ¼Ãe aus MÃ¼nchen!"
print(string)

#Überprüfen, ob "München" in string vorhanden ist
contains_münchen <- grepl("München", string)
print(contains_münchen)

#Zeichen manuell ersetzen
string_bereinigt <- string %>% 
  gsub(pattern = "Ã¶", replacement ="ö") %>% 
  gsub(pattern = "Ã¼", replacement = "ü") %>% 
  gsub(pattern = "Ã\u009f", replacement = "ß") 
print(string_bereinigt)

#Überprüfen, ob "München" in string_bereinigt vorhanden ist
contains_münchen <- grepl("aus", string_bereinigt)
print(contains_münchen)
```

## 2.2 Tokenisierung & Zahlen, URLs, etc. entfernen
Die Funktion `tokens()`von quanteda ermöglicht es uns bei der Aufteilung von Text in tokens direkt bestimmte Zeichen zu entfernen. Hier entfernen wir Punkte, Zahlen, URLs und Symbole.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens <- tokens(daten_df$Description, what = "word", remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_symbols = TRUE) #wollen wir das alles entfernen?

daten_tokens %>% 
  head(n=3)
```
## 2.3 Anpassung auf Kleinschreibung
Mit der Funktion `tokens_tolower()`aus dem quanteda Packet können alle Buchstaben in Kleinbuchstaben umgeformt werden. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens <- tokens_tolower(daten_tokens)

daten_tokens %>% 
  head(n=3)
```
## 2.4 Stoppwörter entfernen
Es gibt verschiedene Möglichkeiten, Stoppwörter zu entfernen. Am einfachsten ist dies mithilfe der in quanteda integrierten Stoppwortlisten möglich. Diese sind in mehreren Sprachen verfügbar, darunter auch Deutsch.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
stopwords("english")
daten_tokens <- tokens_remove(daten_tokens, stopwords("english"))
```
Je nach Forschungsfrage können Stoppwortlisten angepasst werden indem Wörter entfernt oder hinzugefügt werden. Es ist aber auch möglich eine eigene Liste zu erstellen. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#Wörter aus der quanteda Stoppwortliste entfernen
stoppwörter <- stopwords("english")
stoppwörter <- stoppwörter[!stoppwörter %in% c("i", "me")]

#Wörter der quanteda Stoppwortliste hinzufügen
stoppwörter <- c(stoppwörter, "i", "me")

#Eigene Liste erstellen
eigene_stoppwörter <- c("hier", "eigene", "stoppwörter")
```


## 2.5 Vereinheitlichung
Oft gibt es Wörter, die unterschiedliche Abkürzungen oder Schreibweisen haben. Nehmen wir das Beispiel der Europäischen Union, die auch mit EU oder E.U. abgekürzt wird. Mit Hilfe der Funktion `gsub()` können wir strings mit anderen strings ersetzen. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
string <- "Bei den EU Wahlen können alle Bürger*innen der Europäischen Union wählen gehen."
string <- gsub("Europäischen Union", "EU", string)
print(string)
```

## 2.6 Stemming
Mit der Funktion `tokens_wordstem()`aus quanteda reduzieren wir alle tokens auf ihren Wortstamm.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens <- daten_tokens %>% 
  tokens_wordstem() 

daten_tokens %>% 
  head(n=3)
```


## 2.7 Document-Feature-Matrix
Um aus unseren tokens eine dfm zu machen nutzen wir die `dfm()`Funktion aus dem quanteda package.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_dfm <- daten_tokens %>% 
  dfm()
```


## 2.8 Seltene/häufige features entfernen
Im letzten Schritt des Preprocessings entfernen wir häufig und selten vorkommende features aus der dfm. Das geht mit der Funktion `dfm_trim()`aus dem quanteda Packet. 

Es können unterschiedliche thresholds gesetzt werden - hier lassen wir nur features in der dfm die mindestens in 0.5% und höchstens in 99% der Dokumente vorkommen. Das Argument `docfreq_type = "prop"`berechnet den Anteil der Dokumente, die ein bestimmtes feature beinhalten relativ zur Gesamtzahl der Dokumente. 
`verbose = TRUE`printed während der Ausführung der Funktion Informationen über den Rechenvorgang in die Konsole. 

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_dfm <- daten_dfm %>% 
  dfm_trim( min_docfreq = 0.005, 
            max_docfreq = 0.99, 
            docfreq_type = "prop", 
            verbose = TRUE) 
```

hier noch wordcloud einfügen und check most frequent features (in salamanca)

Übung: mit emoji einleiten "test your knowledge" mit anderem Datensatz, der nicht zu groß ist

# 3 Co-Occurrence-Analysen
ab hier neues Dokument
hier mit kwic() anfangen 

Für die Co-Occurence-Analyse müssen wir zunächst die dfm in eine Feature Co-occurrence Matrix (fcm) umwandeln. Das machen wir mit der Funktion `fcm()` aus dem quanteda Packet. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_fcm <- daten_dfm %>% 
  fcm()

daten_fcm %>% 
  head()
```
Beim nächsten Schritt wählen wir die features aus, die uns in unserer Analyse interessieren. Das machen wir mit der Funktion `fcm_select()`aus dem quanteda Packet.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_fcm <- daten_fcm %>% 
  fcm_select(pattern = c("america", "crime", "", "hero"), #hier noch bessere Begriffe vielleicht über topfeatures oder topic modeling
             selection = "keep")
```
Dann visualisieren wir die fcm mit der `textplot_network()`Funktion aus dem`quanteda.textplots()` Paket. Wenn features im selben Dokument vorkommen, werden sie mit einer Linie verbunden. Umso dicker die Linie, desto öfter kommen die features miteinander vor. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#install.packages("quanteda.textplots")
library("quanteda.textplots")
textplot_network(daten_fcm)
```
Diese Visualisierung gibt uns nun aber noch keine genauen Angaben dazu, wie oft ein feature mit einem anderen vorkommt. Um das herauszufinden, müssen wir die fcm mit der `convert()`Funktion aus dem quanteda Packet in einen data frame umwandeln. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_fcm_df <- daten_fcm %>% 
  convert(to = "data.frame")

daten_fcm_df %>% 
  head()
```
Mit Hilfe von `select()`können wir uns nun einzelne Häufigkeiten, wie oft ein feature mit einem anderen feature vorkommt, ausgeben lassen. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_fcm_df %>%
  filter(doc_id == "crime") %>% #Zeile
  select(america) #Spalte #Frage: Wie kann es sein, dass crime america 0 ist und america crime 2?
```
und dann noch code n-grams von salamanca 

und dann noch part of speech tagging (salamanca)

und named entity recognition 


# 4 Diktionäre
## 4.1 Off-the-shelf Diktionäre
Es gibt viele off-the-shelf Diktionäre. Der Einfachkeit halber nutzen wir hier zur Demonstation den `data_dictionary_LSD2015`aus dem quanteda Packet (Young & Soroka, 2012). #Soll hier noch ein Disclaimer wegen Validität und Auswahl Diktionär etc. hin?
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
diktionär <- data_dictionary_LSD2015

diktionär %>% 
  head()

?data_dictionary_LSD2015
```
Nun wollen wir den dictionary auf unsere Daten anwenden. Diese müssen dafür im dfm-Format sein. Mit der Funktion `dfm_lookup()` aus dem quanteda Packet wird für jeden Text, also in diesem Fall für jede TV Show, geschaut, wie viele Wörter aus den ersten zwei Spalten des Diktionärs vorkommen. Die Funktion `dfm_weight(scheme = "prop")`setzt die Anzahl der dictionary Wörter ins Verhältnis mit der Länge des Textes.  
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
sentiment_tvshows <-  daten_dfm %>% 
  dfm_weight(scheme = "prop") %>% 
  dfm_lookup(dictionary = data_dictionary_LSD2015[1:2])

sentiment_tvshows %>% 
  head()
```



## 4.2 Eigene Diktionäre 

Am Ende alles nochmal in ein RSkript packen ohne Text, nur mit Überschrift (alles an Code)
# 5 Topic Modeling

# Qualitätskriterien

Irgendwo einbauen: 
write.csv()/write.csv2()
writeRDS()/readRDS()
save.image()
