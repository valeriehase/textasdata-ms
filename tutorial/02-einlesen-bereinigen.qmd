---
title: "Sitzung 2 - Einlesen & Bereinigen von Text"
author: "Valerie Hase & Luisa Kutlar"
date: "05.04.2024"
format:
  html:
    toc: true
    html-math-method: katex
    css: styles.css
---

# 1 Packete laden und Daten einlesen

Zunächst installieren und laden wir alle Packete, die wir heute und morgen brauchen.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#install.packages("RCurl")
#install.packages("quanteda")
#install.packages("tidyverse)
#install.packages("dplyr")
#install.packages("quanteda.textplots")
#install.packages("quanteda.textstats")
#install.packages("udpipe")

library("RCurl")
library("quanteda")
library("tidyverse")
library("dplyr")
library("quanteda.textplots")
library("quanteda.textstats")
library("udpipe")
```

Dann laden wir die Datei die hinter dem Link liegt mit der Funktion `getURL()` aus dem package Rcurl herunter. Mit einem Blick in das Environment sehen wir, dass die einzelnen Wörter mit einem ; getrennt werden . Daher brauchen wir die Funktion `read.csv2()` aus dem utils Packet - das Packet ist vorinstalliert und immer geladen - um die Daten in R einzulesen. Der Datensatz wird im Objekt `daten_df` gespeichert.

```{r echo = TRUE, message = FALSE, error = FALSE}
url <-  getURL("https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv")
daten_df <-  read.csv2(text = url)
```

Alternativ können Daten in einer .csv Datei auch mit einem , voneinander abgetrennt sein. Hier bräuchte es dann die Funktion `read.csv()` zum Einlesen.

Nach dem Einlesen der Daten ist es üblich sich zunächst einen Überblick über die Daten zu verschaffen und zu kontrollieren, ob alles korrekt eingelesen wurde. 

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
head(daten_df)
str(daten_df)
View(daten_df)
```

# 2 Preprocessing

## 2.1 Encoding issues checken

Nach dem Einlesen haben wir bereits einen Blick in den Datensatz geworfen. Nun schauen wir uns gezielt die Textvariable `description` an, um zu überprüfen, ob alle Zeichen richtig dargestellt werden. 

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_df %>% 
   select(Description) %>% 
  head()
```

In diesem Fall gibt es keine Encoding issues.

Was tun falls doch?

::: incremental
-   Beim Einlesen das richtige Encoding mitgeben
-   Manuell bereinigen 
:::

Beim manuellen Bereinigen kann die Funktion `gsub()` helfen, die Zeichenketten ersetzen kann. Zum Beispiel so: 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#string mit encoding issues
string <- "SchÃ¶ne GrÃ¼Ãe aus MÃ¼nchen!"
print(string)

#Überprüfen, ob "München" in string vorhanden ist
contains_münchen <- grepl("München", string)
print(contains_münchen)

#Zeichen manuell ersetzen
string_bereinigt <- string %>% 
  gsub(pattern = "Ã¶", replacement ="ö") %>% 
  gsub(pattern = "Ã¼", replacement = "ü") %>% 
  gsub(pattern = "Ã\u009f", replacement = "ß") 
print(string_bereinigt)

#Überprüfen, ob "München" in string_bereinigt vorhanden ist
contains_münchen <- grepl("aus", string_bereinigt)
print(contains_münchen)
```

## 2.2 Tokenisierung & Zahlen, URLs, etc. entfernen
Die Funktion `tokens()`von quanteda ermöglicht es uns bei der Aufteilung von Text in tokens direkt bestimmte Zeichen zu entfernen. Hier entfernen wir Punkte, Zahlen, URLs und Symbole.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens <- tokens(daten_df$Description, what = "word", remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_symbols = TRUE) #wollen wir das alles entfernen?

daten_tokens %>% 
  head(n=3)
```
## 2.3 Anpassung auf Kleinschreibung
Mit der Funktion `tokens_tolower()`aus dem quanteda Packet können alle Buchstaben in Kleinbuchstaben umgeformt werden. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens <- tokens_tolower(daten_tokens)

daten_tokens %>% 
  head(n=3)
```
## 2.4 Stoppwörter entfernen
Es gibt verschiedene Möglichkeiten, Stoppwörter zu entfernen. Am einfachsten ist dies mithilfe der in quanteda integrierten Stoppwortlisten möglich. Diese sind in mehreren Sprachen verfügbar, darunter auch Deutsch.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
stopwords("english")
daten_tokens <- tokens_remove(daten_tokens, stopwords("english"))
```
Je nach Forschungsfrage können Stoppwortlisten angepasst werden, indem Wörter entfernt oder hinzugefügt werden. Es ist aber auch möglich eine eigene Liste zu erstellen. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#Wörter aus der quanteda Stoppwortliste entfernen
stoppwörter <- stopwords("english")
stoppwörter <- stoppwörter[!stoppwörter %in% c("i", "me")]

#Wörter der quanteda Stoppwortliste hinzufügen
stoppwörter <- c(stoppwörter, "i", "me")

#Eigene Liste erstellen
eigene_stoppwörter <- c("hier", "eigene", "stoppwörter")
```


## 2.5 Vereinheitlichung
Oft gibt es Wörter, die unterschiedliche Abkürzungen oder Schreibweisen haben. Nehmen wir das Beispiel der Europäischen Union, die auch mit EU oder E.U. abgekürzt wird. Mit Hilfe der Funktion `gsub()` können wir strings mit anderen strings ersetzen. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
string <- "Bei den EU Wahlen können alle Bürger*innen der Europäischen Union wählen gehen."
string <- gsub("Europäischen Union", "EU", string)
print(string)
```

## 2.6 Stemming
Mit der Funktion `tokens_wordstem()`aus quanteda reduzieren wir alle tokens auf ihren Wortstamm.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens <- daten_tokens %>% 
  tokens_wordstem() 

daten_tokens %>% 
  head(n=3)
```


## 2.7 Document-Feature-Matrix
Um aus unseren tokens eine dfm zu machen nutzen wir die `dfm()`Funktion aus dem quanteda package.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_dfm <- daten_tokens %>% 
  dfm()
```


## 2.8 Seltene/häufige features entfernen
Im letzten Schritt des Preprocessings entfernen wir häufig und selten vorkommende features aus der dfm. Das geht mit der Funktion `dfm_trim()`aus dem quanteda Packet. 

Es können unterschiedliche thresholds gesetzt werden - hier lassen wir nur features in der dfm die mindestens in 0.5% und höchstens in 99% der Dokumente vorkommen. Das Argument `docfreq_type = "prop"`berechnet den Anteil der Dokumente, die ein bestimmtes feature beinhalten relativ zur Gesamtzahl der Dokumente. 
`verbose = TRUE`printed während der Ausführung der Funktion Informationen über den Rechenvorgang in die Konsole. 

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_dfm <- daten_dfm %>% 
  dfm_trim( min_docfreq = 0.005, 
            max_docfreq = 0.99, 
            docfreq_type = "prop", 
            verbose = TRUE) 
```

## 2.9 Word cloud: Erster Blick in die Daten
Für einen ersten Einblick in die Daten lassen wir uns mit der `topfeatures()`Funktion aus dem quanteda Packet die 10 am häufigsten vorkommenden features ausgeben.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_dfm %>% 
  topfeatures(n = 10)
```

Das Ergebnis können wir mit einer word cloud visualisieren. Hierfür nutzen wir die `textplot_wordcloud()`Funktion aus dem quanteda.textplots Packet. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
word_cloud <- daten_dfm %>% 
  textplot_wordcloud(max_words = 100)
```



Übung: mit emoji einleiten "test your knowledge" mit anderem Datensatz, der nicht zu groß ist

# 3 Co-Occurrence-Analysen
ab hier neues Dokument
hier mit kwic() anfangen 

Für die Co-Occurence-Analyse müssen wir zunächst die dfm in eine Feature Co-occurrence Matrix (fcm) umwandeln. Das machen wir mit der Funktion `fcm()` aus dem quanteda Packet. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_fcm <- daten_dfm %>% 
  fcm()

daten_fcm %>% 
  head()
```
Beim nächsten Schritt wählen wir die features aus, die uns in unserer Analyse interessieren. Das machen wir mit der Funktion `fcm_select()`aus dem quanteda Packet.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_fcm <- daten_fcm %>% 
  fcm_select(pattern = c("famili", "crime", "america", "school"), #hier noch bessere Begriffe vielleicht über topfeatures oder topic modeling
             selection = "keep")
```

Dann visualisieren wir die fcm mit der `textplot_network()`Funktion aus dem`quanteda.textplots()` Paket. Wenn features im selben Dokument vorkommen, werden sie mit einer Linie verbunden. Umso dicker die Linie, desto öfter kommen die features miteinander vor. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
textplot_network(daten_fcm)
```
Diese Visualisierung gibt uns nun aber noch keine genauen Angaben dazu, wie oft ein feature mit einem anderen vorkommt. Um das herauszufinden, müssen wir die fcm mit der `convert()`Funktion aus dem quanteda Packet in einen data frame umwandeln. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_fcm_df <- daten_fcm %>% 
  convert(to = "data.frame")

daten_fcm_df %>% 
  head()
```

Mit Hilfe von `select()`können wir uns nun einzelne Häufigkeiten, wie oft ein feature mit einem anderen feature vorkommt, ausgeben lassen. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_fcm_df %>%
  filter(doc_id == "crime") %>% #Zeile
  select(america) #Spalte #Frage: Wie kann es sein, dass crime america 0 ist und america crime 2?
```

# Kollokationen und N-gramme
Um herauszufinden, welche tokens oft hintereinander vorkommen, können wir die Funktion `textstat_collocations`aus dem quanteda.textstats Packet verwenden.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens %>%
  textstat_collocations(min_count = 10) %>%
  arrange(-lambda) %>%
  head(10)
```

In einem weiteren Schritt kann es manchmal sinnvoll sein, Kollokationen für die Analyse zu einem token zusammenzufassen. Das lässt sich mit der `tokens_compound()`Funktion aus dem quanteda.textstats Packet umsetzen. Die Funktion verbindet die gegebenen tokens mit einem Unterstrich zu einem token.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
ngramme <- c("los angel","new york citi", "serial killer", "high school", "best friend")
daten_tokens_ngramme <- tokens_compound(daten_tokens, pattern = phrase(ngramme))
```

Wie das nun in den Daten aussieht können wir mit Hilfe der `kwic()`Funktion aus dem quanteda Packet herausfinden (kwic steht für keywords in context). 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens_ngramme %>% 
  kwic(pattern = c("los angel","new york citi", "serial killer", "high school", "best friend")) %>% 
  head(n=30)
```

# Part of speech tagging
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_df$Description %>%

#change format for udpipe package
  as_tibble() %>%
  mutate(doc_id = paste0("text", 1:n())) %>%
  rename(text = value) %>%

  #for simplicity, run for fewer documents
  slice(1) %>%

  #part-of-speech tagging, include only related variables
  udpipe("english") %>%
  select(doc_id, sentence_id, token_id, token, upos) %>%
  head(10)
```

# Named entitiy recognition
 


# 4 Diktionäre
## 4.1 Off-the-shelf Diktionäre
Es gibt viele off-the-shelf Diktionäre. Der Einfachkeit halber nutzen wir hier zur Demonstation den `data_dictionary_LSD2015`aus dem quanteda Packet (Young & Soroka, 2012). #Soll hier noch ein Disclaimer wegen Validität und Auswahl Diktionär etc. hin?
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
diktionär <- data_dictionary_LSD2015

diktionär %>% 
  head()

?data_dictionary_LSD2015
```
Nun wollen wir den dictionary auf unsere Daten anwenden. Diese müssen dafür im dfm-Format sein. Mit der Funktion `dfm_lookup()` aus dem quanteda Packet wird für jeden Text, also in diesem Fall für jede TV Show, geschaut, wie viele Wörter aus den ersten zwei Spalten des Diktionärs vorkommen. Die Funktion `dfm_weight(scheme = "prop")`setzt die Anzahl der dictionary Wörter ins Verhältnis mit der Länge des Textes.  
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
sentiment_tvshows <-  daten_dfm %>% 
  dfm_weight(scheme = "prop") %>% 
  dfm_lookup(dictionary = data_dictionary_LSD2015[1:2])

sentiment_tvshows %>% 
  head()
```



## 4.2 Eigene Diktionäre 

Am Ende alles nochmal in ein RSkript packen ohne Text, nur mit Überschrift (alles an Code)
# 5 Topic Modeling

# Qualitätskriterien

Irgendwo einbauen: 
write.csv()/write.csv2()
writeRDS()/readRDS()
save.image()
