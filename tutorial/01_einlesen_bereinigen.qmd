---
title: "Sitzung 1: Daten einlesen und bereinigen"
author: "Valerie Hase & Luisa Kutlar"
date: "24.07.2024"
format:
  html:
    toc: true
    html-math-method: katex
    css: styles.css
---

# 1 Pakete laden und Daten einlesen

Zunächst installieren alle Packete, die wir für diese Sitzung brauchten (z.B. *tidyverse*). Ihr braucht *install.packages()* nur, wenn ihr die Pakete im Methodencafe noch nicht installiert hattet-

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#install.packages("tidyverse)
#install.packages("quanteda")
#install.packages("quanteda.textplots")
#install.packages("RCurl")

library("tidyverse")
library("quanteda")
library("quanteda.textplots")
library("RCurl")
```

## 1.1 Textdaten aus einer lokalen Datei einlesen

Zunächst könnt ihr die Text-Daten via diesem Link auf der Webseite downloaden:

![](images/clipboard-3533778068.png)

Anschliessen laden wir die Dateien in R. Wenn ihr via JupyterHub arbeitet, geht das via Click-and-Point. Andernfalls nutzt den `read.csv()`-Befehl. Der Datensatz wird im Objekt `data` gespeichert.

```{r echo = TRUE, eval = FALSE, message = FALSE, error = FALSE}
data <- read.csv2("data_tvseries.csv")
```

## 1.2 Textdaten von einer URL downloaden
Oft wollen wir Dateien auch direkt von z. B. einer Webseite laden. Dafür nutzen wir die Funktion `getURL()` aus dem package `Rcurl` und dann wieder die `read.csv2()`-Funktion.
```{r echo = TRUE, message = FALSE, error = FALSE}
library("RCurl")
url <-  getURL("https://raw.githubusercontent.com/valeriehase/textasdata-ms/main/data/data_tvseries.csv")
data <- read.csv2(text = url)
```

Nach dem Einlesen der Daten verschaffen wir uns einen Überblick über die Daten und kontrollieren, dass alles korrekt eingelesen wurde.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
head(data)
```
# 2 Preprocessing

## 2.1 Encoding issues checken

Nach dem Einlesen haben wir bereits einen Blick in den Datensatz geworfen. Nun schauen wir uns gezielt die Variable `Description` an, um zu überprüfen, ob der zu analysierende Text gut aussieht.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
data %>%
  select(Description) %>% 
  slice(1)
```
Der Text sieht gut aus! Allerdings kann es, gerade bei Texten aus anderen Sprachen, zu sogenannten Encoding Issues kommen.

Schauen wir uns ein Beispiel an: deutsche Umlaute

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#Beispiel-Satz
string <- "Schöne Grüße aus München"

#Encoding prüfen
Encoding(string)

#Encoding testweise ändern
Encoding(string) <- "latin1"
string
```

Wie könnten wir Encoding-Probleme adressieren?

::: incremental
-   Beim Einlesen das richtige Encoding als Argument mitgeben (siehe z.B. `fileEncoding`-Argument in `read.csv2()`)
-   Mit Hilfe von regulären Ausdrücken bereinigen
:::

Beim manuellen Bereinigen kann die Funktion `gsub()` helfen, die Zeichenketten ersetzen kann. Zum Beispiel so:

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#Mit Hilfe von regulären Ausdrücken bereinigen
string_bereinigt <- string %>% 
  gsub(pattern = "Ã¶", replacement ="ö") %>% 
  gsub(pattern = "Ã¼", replacement = "ü") %>% 
  gsub(pattern = "ÃŸ", replacement = "ß") 
string_bereinigt
```

## 2.2 Tokenisierung & Zahlen, URLs, etc. entfernen

Die Funktion `tokens()`von quanteda ermöglicht es uns bei der Aufteilung von Text in tokens direkt bestimmte Zeichen zu entfernen. Hier entfernen wir Punkte, Zahlen, URLs und Symbole.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens <- tokens(data$Description, what = "word", remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_symbols = TRUE) #wollen wir das alles entfernen?

daten_tokens %>% 
  head(n=3)
```

## 2.3 Anpassung auf Kleinschreibung

Mit der Funktion `tokens_tolower()`aus dem quanteda Packet können alle Buchstaben in Kleinbuchstaben umgeformt werden.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens <- tokens_tolower(daten_tokens)

daten_tokens %>% 
  head(n=3)
```

## 2.4 Stoppwörter entfernen

Es gibt verschiedene Möglichkeiten, Stoppwörter zu entfernen. Am einfachsten ist dies mithilfe der in quanteda integrierten Stoppwortlisten möglich. Diese sind in mehreren Sprachen verfügbar, darunter auch Deutsch.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
stopwords("english")
daten_tokens <- tokens_remove(daten_tokens, stopwords("english"))
```

Je nach Forschungsfrage können Stoppwortlisten angepasst werden, indem Wörter entfernt oder hinzugefügt werden. Es ist aber auch möglich eine eigene Liste zu erstellen.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#Wörter aus der quanteda Stoppwortliste entfernen
stoppwörter <- stopwords("english")
stoppwörter <- stoppwörter[!stoppwörter %in% c("i", "me")]

#Wörter der quanteda Stoppwortliste hinzufügen
stoppwörter <- c(stoppwörter, "i", "me")

#Eigene Liste erstellen
eigene_stoppwörter <- c("hier", "eigene", "stoppwörter")
```

## 2.5 Vereinheitlichung

Oft gibt es Wörter, die unterschiedliche Abkürzungen oder Schreibweisen haben. Nehmen wir das Beispiel der Europäischen Union, die auch mit EU oder E.U. abgekürzt wird. Mit Hilfe der Funktion `gsub()` können wir strings mit anderen strings ersetzen.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
string <- "Bei den EU Wahlen können alle Bürger*innen der Europäischen Union wählen gehen."
string <- gsub("Europäischen Union", "EU", string)
print(string)
```

## 2.6 Stemming

Mit der Funktion `tokens_wordstem()`aus quanteda reduzieren wir alle tokens auf ihren Wortstamm.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens <- daten_tokens %>% 
  tokens_wordstem() 

daten_tokens %>% 
  head(n=3)
```

## 2.7 Document-Feature-Matrix

Um aus unseren tokens eine dfm zu machen nutzen wir die `dfm()`Funktion aus dem quanteda package.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
datam <- daten_tokens %>% 
  dfm()
```

## 2.8 Seltene/häufige features entfernen

Im letzten Schritt des Preprocessings entfernen wir häufig und selten vorkommende features aus der dfm. Das geht mit der Funktion `dfm_trim()`aus dem quanteda Packet.

Es können unterschiedliche thresholds gesetzt werden - hier lassen wir nur features in der dfm die mindestens in 0.5% und höchstens in 99% der Dokumente vorkommen. Das Argument `docfreq_type = "prop"`berechnet den Anteil der Dokumente, die ein bestimmtes feature beinhalten relativ zur Gesamtzahl der Dokumente. `verbose = TRUE`printed während der Ausführung der Funktion Informationen über den Rechenvorgang in die Konsole.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
datam <- datam %>% 
  dfm_trim( min_docfreq = 0.005, 
            max_docfreq = 0.99, 
            docfreq_type = "prop", 
            verbose = TRUE) 
```

## 2.9 Word cloud: Erster Blick in die Daten

Für einen ersten Einblick in die Daten lassen wir uns mit der `topfeatures()`Funktion aus dem quanteda Packet die 10 am häufigsten vorkommenden features ausgeben.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
datam %>% 
  topfeatures(n = 10)
```

Das Ergebnis können wir mit einer word cloud visualisieren. Hierfür nutzen wir die `textplot_wordcloud()`Funktion aus dem quanteda.textplots Packet.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
word_cloud <- datam %>% 
  textplot_wordcloud(max_words = 100)
```

Übung: mit emoji einleiten "test your knowledge" mit anderem Datensatz, der nicht zu groß ist
