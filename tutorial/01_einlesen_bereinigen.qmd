---
title: "Sitzung 1: Daten einlesen und bereinigen"
author: "Valerie Hase & Luisa Kutlar"
date: "05.04.2024"
format:
  html:
    toc: true
    html-math-method: katex
    css: styles.css
---

# 1 Packete laden und Daten einlesen

Zunächst installieren und laden wir alle Packete, die wir heute und morgen brauchen.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#install.packages("RCurl")
#install.packages("quanteda")
#install.packages("tidyverse)
#install.packages("dplyr")
#install.packages("quanteda.textplots")
#install.packages("quanteda.textstats")
#install.packages("udpipe")

library("RCurl")
library("quanteda")
library("tidyverse")
library("dplyr")
library("quanteda.textplots")
library("quanteda.textstats")
library("udpipe")
```

Dann laden wir die Datei die hinter dem Link liegt mit der Funktion `getURL()` aus dem package Rcurl herunter. Mit einem Blick in das Environment sehen wir, dass die einzelnen Wörter mit einem ; getrennt werden . Daher brauchen wir die Funktion `read.csv2()` aus dem utils Packet - das Packet ist vorinstalliert und immer geladen - um die Daten in R einzulesen. Der Datensatz wird im Objekt `daten_df` gespeichert.

```{r echo = TRUE, message = FALSE, error = FALSE}
url <-  getURL("https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv")
daten_df <-  read.csv2(text = url)
```

Alternativ können Daten in einer .csv Datei auch mit einem , voneinander abgetrennt sein. Hier bräuchte es dann die Funktion `read.csv()` zum Einlesen.

Nach dem Einlesen der Daten ist es üblich sich zunächst einen Überblick über die Daten zu verschaffen und zu kontrollieren, ob alles korrekt eingelesen wurde. 

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
head(daten_df)
str(daten_df)
View(daten_df)
```

# 2 Preprocessing

## 2.1 Encoding issues checken

Nach dem Einlesen haben wir bereits einen Blick in den Datensatz geworfen. Nun schauen wir uns gezielt die Textvariable `description` an, um zu überprüfen, ob alle Zeichen richtig dargestellt werden. 

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_df %>% 
   select(Description) %>% 
  head()
```

In diesem Fall gibt es keine Encoding issues.

Was tun falls doch?

::: incremental
-   Beim Einlesen das richtige Encoding mitgeben
-   Manuell bereinigen 
:::

Beim manuellen Bereinigen kann die Funktion `gsub()` helfen, die Zeichenketten ersetzen kann. Zum Beispiel so: 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#string mit encoding issues
string <- "SchÃ¶ne GrÃ¼Ãe aus MÃ¼nchen!"
print(string)

#Überprüfen, ob "München" in string vorhanden ist
contains_münchen <- grepl("München", string)
print(contains_münchen)

#Zeichen manuell ersetzen
string_bereinigt <- string %>% 
  gsub(pattern = "Ã¶", replacement ="ö") %>% 
  gsub(pattern = "Ã¼", replacement = "ü") %>% 
  gsub(pattern = "Ã\u009f", replacement = "ß") 
print(string_bereinigt)

#Überprüfen, ob "München" in string_bereinigt vorhanden ist
contains_münchen <- grepl("aus", string_bereinigt)
print(contains_münchen)
```

## 2.2 Tokenisierung & Zahlen, URLs, etc. entfernen
Die Funktion `tokens()`von quanteda ermöglicht es uns bei der Aufteilung von Text in tokens direkt bestimmte Zeichen zu entfernen. Hier entfernen wir Punkte, Zahlen, URLs und Symbole.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens <- tokens(daten_df$Description, what = "word", remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_symbols = TRUE) #wollen wir das alles entfernen?

daten_tokens %>% 
  head(n=3)
```
## 2.3 Anpassung auf Kleinschreibung
Mit der Funktion `tokens_tolower()`aus dem quanteda Packet können alle Buchstaben in Kleinbuchstaben umgeformt werden. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens <- tokens_tolower(daten_tokens)

daten_tokens %>% 
  head(n=3)
```
## 2.4 Stoppwörter entfernen
Es gibt verschiedene Möglichkeiten, Stoppwörter zu entfernen. Am einfachsten ist dies mithilfe der in quanteda integrierten Stoppwortlisten möglich. Diese sind in mehreren Sprachen verfügbar, darunter auch Deutsch.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
stopwords("english")
daten_tokens <- tokens_remove(daten_tokens, stopwords("english"))
```
Je nach Forschungsfrage können Stoppwortlisten angepasst werden, indem Wörter entfernt oder hinzugefügt werden. Es ist aber auch möglich eine eigene Liste zu erstellen. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#Wörter aus der quanteda Stoppwortliste entfernen
stoppwörter <- stopwords("english")
stoppwörter <- stoppwörter[!stoppwörter %in% c("i", "me")]

#Wörter der quanteda Stoppwortliste hinzufügen
stoppwörter <- c(stoppwörter, "i", "me")

#Eigene Liste erstellen
eigene_stoppwörter <- c("hier", "eigene", "stoppwörter")
```


## 2.5 Vereinheitlichung
Oft gibt es Wörter, die unterschiedliche Abkürzungen oder Schreibweisen haben. Nehmen wir das Beispiel der Europäischen Union, die auch mit EU oder E.U. abgekürzt wird. Mit Hilfe der Funktion `gsub()` können wir strings mit anderen strings ersetzen. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
string <- "Bei den EU Wahlen können alle Bürger*innen der Europäischen Union wählen gehen."
string <- gsub("Europäischen Union", "EU", string)
print(string)
```

## 2.6 Stemming
Mit der Funktion `tokens_wordstem()`aus quanteda reduzieren wir alle tokens auf ihren Wortstamm.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens <- daten_tokens %>% 
  tokens_wordstem() 

daten_tokens %>% 
  head(n=3)
```


## 2.7 Document-Feature-Matrix
Um aus unseren tokens eine dfm zu machen nutzen wir die `dfm()`Funktion aus dem quanteda package.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_dfm <- daten_tokens %>% 
  dfm()
```


## 2.8 Seltene/häufige features entfernen
Im letzten Schritt des Preprocessings entfernen wir häufig und selten vorkommende features aus der dfm. Das geht mit der Funktion `dfm_trim()`aus dem quanteda Packet. 

Es können unterschiedliche thresholds gesetzt werden - hier lassen wir nur features in der dfm die mindestens in 0.5% und höchstens in 99% der Dokumente vorkommen. Das Argument `docfreq_type = "prop"`berechnet den Anteil der Dokumente, die ein bestimmtes feature beinhalten relativ zur Gesamtzahl der Dokumente. 
`verbose = TRUE`printed während der Ausführung der Funktion Informationen über den Rechenvorgang in die Konsole. 

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_dfm <- daten_dfm %>% 
  dfm_trim( min_docfreq = 0.005, 
            max_docfreq = 0.99, 
            docfreq_type = "prop", 
            verbose = TRUE) 
```

## 2.9 Word cloud: Erster Blick in die Daten
Für einen ersten Einblick in die Daten lassen wir uns mit der `topfeatures()`Funktion aus dem quanteda Packet die 10 am häufigsten vorkommenden features ausgeben.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_dfm %>% 
  topfeatures(n = 10)
```

Das Ergebnis können wir mit einer word cloud visualisieren. Hierfür nutzen wir die `textplot_wordcloud()`Funktion aus dem quanteda.textplots Packet. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
word_cloud <- daten_dfm %>% 
  textplot_wordcloud(max_words = 100)
```



Übung: mit emoji einleiten "test your knowledge" mit anderem Datensatz, der nicht zu groß ist
