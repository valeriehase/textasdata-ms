---
title: "Sitzung 2: Co-Occurrence Analysen"
author: "Valerie Hase & Luisa Kutlar"
format:
  html:
    toc: true
    html-math-method: katex
    css: styles.css
---

# 1. Pakete laden und Daten einlesen

Zun√§chst installieren alle Pakete, die wir f√ºr diese Sitzung brauchten (z.B. `tidyverse`). Ihr braucht `install.packages()` nur, wenn ihr die Pakete im Methodencafe noch nicht installiert hattet.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#install.packages("tidyverse)
#install.packages("quanteda")
#install.packages("quanteda.textplots")
#install.packages("RCurl")
#install.packages("quanteda.textstats")
#install.packages("udpipe")

library("tidyverse")
library("quanteda")
library("quanteda.textplots")
library("RCurl")
library("quanteda.textstats")
library("udpipe")
```

Neu hinzugekommen seit dem Methodencafe ist das `rsyntax`-Paket, das ihr neu installieren m√ºsstest:

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE, eval = FALSE}
install.packages("rsyntax")
library("rsyntax")
```

```{r echo = FALSE, message = FALSE, warning = FALSE, error = FALSE, eval = TRUE}
library("rsyntax")
```

Nun lesen wir die Daten wieder ein und f√ºhren die bereits erlernten Preprocessing-Schritte, inkl. der Transformation in eine Document-Feature-Matrix, aus:

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#Daten laden
url <-  getURL("https://raw.githubusercontent.com/valeriehase/textasdata-ms/main/data/data_tvseries.csv")
data <- read.csv2(text = url)

#Preprocessing
tokens <- tokens(data$Description,
                 what = "word", #Tokenisierung, hier zu W√∂rtern als Analyseeinheit
                 remove_punct = TRUE, #Entfernung von Satzzeichen
                 remove_numbers = TRUE) %>% #Entfernung von Zahlen
  
  #Kleinschreibung
  tokens_tolower() %>% 
  
  #Entfernung von Stoppw√∂rtern
  tokens_remove(stopwords("english")) %>% 
  
  #Stemming
  tokens_wordstem()

#Text-as-Data Repr√§sentation als Document-Feature-Matrix
dfm <- tokens %>% 
  dfm() %>% 
  
  #Relative pruning
  dfm_trim( min_docfreq = 0.005, 
            max_docfreq = 0.99, 
            docfreq_type = "prop", 
            verbose = TRUE) 
```

Jetzt sind wir bereit f√ºr die ersten Co-Occurrence-Analysen!

# 2. Co-Occurrence Analysen

## 2.1 N-grams

Zun√§chst schauen wir uns als Beispiel f√ºr ngrams **bigrams**, d.h. Abfolgen zweier W√∂rter, an:

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
tokens %>%
  
  #Umwandlung in bigrams
  tokens_ngrams(n = 2) %>%
  
  #Ausgabe f√ºr erstes Dokument
  head(1)
```

Aus Sitzung 1 kennen wir ja bereits den Befehl `topfeatures()`, um uns die h√§ufigsten Features ausgeben zu lassen. Nun lassen wir uns nicht die h√§ufigsten **unigrams** (einzelne W√∂rter), sondern die h√§ufigsten **bigrams** (Abfolge von zwei W√∂rtern) ausgeben.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
tokens %>%
  
  #Umwandlung in bigrams
  tokens_ngrams(n = 2) %>%
  
  #Umwandlung in dfm f√ºr topfeatures-Befehl
  dfm() %>%
  
  #Ausgabe der h√§ufigsten Features
  topfeatures(10) %>%
  
  #Umwandlung in einen "sch√∂neren" Dataframe mit der Spalte "H√§ufigkeit"
  as.data.frame() %>%
  rename("H√§ufigkeit" = '.')
```

Wir sehen, dass unter den h√§ufigsten bigrams einige Features sind, die in Kombination miteinander eine andere Bedeutung haben als als einzelne unigrams. Dazu geh√∂ren z.B. Orte (z.B. *New York*) oder Ausdr√ºcke (z. B. *serial killer*).

## 2.2 Keywords-in-Context (KWIC)

Als eher qualitativer Einblick bietet sich die Analyse von *Konkordanzen*, d.h. der Analyse von Schl√ºsselw√∂rtern und ihres Kontexts, im Text an.

Die `kwic()`Funktion aus dem `quanteda`-Paket identifiziert hierzu Schl√ºsselw√∂rter und W√∂rter vor bzw. nach diesen. Wir schauen uns also W√∂rter an, die um ein sogenanntes *Window* von z.B. einem oder mehr W√∂rtern vor oder nach dem Schl√ºsselwort vorkommen.

Mit folgendem Code k√∂nnen wir beispielsweise herausfinden, in welchem Kontext das Wort *hero* vorkommt. In der Ausgabe werden jeweils das Wort vor und nach dem Wort *hero* angezeigt. Um die Bedeutung dieser W√∂rter besser zu verstehen, nutzen wir dabei die "unbereinigte" `Description`-Variable aus dem Objekt `data`.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
data$Description %>% 
  
  #Keywords-in-Context mit Window von 1 Wort vor und nach Schl√ºsselwort
  kwic(pattern = "hero", 
       window = 1) %>%
  
  #Ausgabe der ersten Zeilen
  head()
```

## 2.3 Semantische Netzwerke auf Basis von Co-Occurrenzen

Als st√§rker quantifizierende Analyse bietet sich die Visualisierung von Co-Occurrenzen mittels semantischer Netzwerke an. Diese visualisieren, welche Features h√§ufig in einem gemeinsamen Kontext (z.B. in direkter Abfolge, innerhalb eines Textes) vorkommen.

Hierf√ºr m√ºssen wir zun√§chst die DFM in eine Feature-Co-Occurrence Matrix (FCM) umwandeln. Das machen wir mit der Funktion `fcm()` aus dem `quanteda`-Paket.

Uns interessiert zun√§chst, wie h√§ufig Features *innerhalb eines Textes* vorkommen, weshalb wir das `context`-Argument nutzen und auf `document` setzen.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
tokens %>%
  
  #Umwandlung in eine Feature-Co-Occurrence-Matrix
  fcm(context = "document") %>%
  
  #Ausgabe der ersten Zeilen
  head()
```

Wir sehen:

-   Die FCM besteht aus 4,246 Features (da wir nur wenig Preprocessing genutzt haben, sind dies noch recht viele Features).
-   Die Zellen der FCM illustrieren, wie h√§ufig welche W√∂rter in der Beschreibung einer Serie mittels `Description` gemeinsam vorkommen.

Wir k√∂nnen jetzt spezifische Features ausw√§hlen, die uns interessieren. Das machen wir mit der Funktion `fcm_select()` aus dem `quanteda`-Paket.

Zudem k√∂nnten wir √ºber das `window`-Argument noch spezifizieren, dass uns die Co-Occurrence von Features nur innerhalb eines Windows von z. B. acht Features interessiert. Ein Argument hierf√ºr w√§re, dass Feature, die nah aufeinander folgen, auch st√§rker eine inhaltlich geteilte Bedeutung haben als solche, die "nur" gleichzeitig in einem Dokument vorkommen. Zudem setzen wir das `selection`-Argument auf `keep`, um die FCM nur auf die angegebenen Features zu reduzieren.

Beispielsweise wollen wir uns anschauen, ob M√§nner (*man*) und Frauen (*woman*) in Serienbeschreibungen stereotypisiert, z. B. im Hinblick auf Geschlechterrollen, dargestellt werden.

Daf√ºr schauen wir, welche W√∂rter (z.B. *fight* vs. *romance*) h√§ufiger in Verbindung mit M√§nnern bzw. Frauen genannt werden

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
fcm <- tokens %>%
  
  #Erstellung einer FCM mit einem Window von 8
  fcm(window = 8) %>%
  
  #Reduktion auf ausgew√§hler Features
  fcm_select(pattern = c("fight", "man", 
                         "love", "young", "woman"), 
             selection = "keep")
```

Wir visualisieren die Ergebnisse als semantisches Netzwerk mit der `textplot_network()`Funktion aus dem `quanteda.textplots()`-Paket.

Wenn Features im selben Kontext (hier 8 W√∂rter) vorkommen, werden sie mit einer Linie verbunden. Umso dicker die Linie, desto √∂fter kommen Features gemeinsam vor.

Dabei k√∂nnen wir sehen, dass M√§nner wie Frauen h√§ufig als "jung" beschrieben werden - M√§nner aber h√§ufiger mit "K√§mpfen" und Frauen mit "Liebe" assoziiert werden:

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}

#Plot des semantischen Netzwerks
textplot_network(fcm)
```

## 2.4 Collocations

Ein weiterer Ansatz sind Collocations. Collocations bezeichnen Features, die h√§ufig (und damit vermutlich nicht-zuf√§llig) *nacheinander* auftreten, was auf eine gemeinsame semantische Bedeutung hindeutet (z.B. *United* und *States*).

Um herauszufinden, welche Features vorkommen, k√∂nnen wir die Funktion `textstat_collocations()` aus dem `quanteda.textstats`-Paket verwenden.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
tokens %>%
  
  #Identifikation von Collocations, die mind. 10 Mal vorkommen
  textstat_collocations(min_count = 10) %>%
  
  #Sortierung nach lambda: Je gr√∂sser, 
  #desto wahrscheinlicher handelt es sich um nicht-zuf√§llige Collocations
  arrange(-lambda) %>%
  
  #Ausgabe der h√§ufigsten 10 Collocation
  head(10)
```

Das kennen wir doch schon - n√§mlich aus Abschnitt 2.1 zu *ngrams*.

## 2.5 Ngrams azu einem Feature zusammenfassen

In einem weiteren Schritt kann es manchmal sinnvoll sein, Collocations f√ºr die weitere Analyse zu einem Feature zusammenzufassen (dieser Schritt w√§re dann Teil des Preprocessings). Beispielsweise wollen wir, dass *United* immer mit *States* als ein Feature *United States* verstanden wird, sobald diese beiden Features in dieser Abfolge genutzt werden (statt diese in einzelne Features zu splitten).

Dieses Zusammenfassen (*compounding*) l√§sst sich mit der `tokens_compound()`-Funktion aus dem `quanteda.textstats`-Paket umsetzen.

Die Funktion verbindet unigrams mit einem Unterstrich zu einem ngram, das als ein einziges Feature weiter analysiert wird. Dieser Schritt wird zwischen dem `tokens`-Befehl und dem `dfm`-Befehle (d.h. der Tokenisierung und der Erstellung einer Document-Feature-Matrix) eingef√ºgt.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}

#Definition h√§ufiger Collocations auf Basis der vorherigen Ausgabe
ngrams <- c("los angel","new york citi", "serial killer", "high school", "best friend")

#Text-as-Data Repr√§sentation als Document-Feature-Matrix
dfm <- tokens %>% 
  
  #Zus√§tzlicher Schritt, um Collocations als einzelnes Feature einzulesen
  tokens_compound(pattern = phrase(ngrams)) %>%
  
  #regul√§re DFM, inkl. Relative Pruning
  dfm() %>% 
  dfm_trim( min_docfreq = 0.005, 
            max_docfreq = 0.99, 
            docfreq_type = "prop", 
            verbose = TRUE) 

#Beispiel: Wie wird das Feature "Los Angeles" eingelesen?
dfm %>%
  
  #Umwandlung zu Data-Frame
  convert(to = "data.frame") %>%
  
  #Reduktion auf Doc ID und Features, die mit "los" beginnen
  select(doc_id, starts_with("los")) %>%
  
  #Ausgabe ausgew√§hlter Serien (Zeile 125 bis 130)
  slice(125:130)
```

# 3. Part-of-Speech Tagging

Part-of-Speech Tagging (PoS Tagging) bezeichnet die Zuordnung von Features zu Wortarten (z.B. ob es sich bei einem Feature um ein Verb oder ein Substantiv handelt).

F√ºr das PoS Tagging nutzen wir das Paket `udpipe`. Daf√ºr m√ºssen wir die Daten erst f√ºr die Analyse vorbereiten (den Text in `Description` in ein `tibble`-Format bringen, eine Variable `doc_id` erstellen, welche alle Texte numerisch identifiziert, und die relevanten Daten, die Serienbeschreibungen, in der Variable `Text` abspeichern).

Daf√ºr nutzen wir den "urspr√ºnglichen", unbereinigten Datensatz, da die Part-of-Speech Tagger nicht nur das Feature selbst, sondern auch Kontext wie z.B. Satzzeichen nutzen, um zu verstehen, welche Funktion W√∂rter in S√§tzen haben.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
data_pos_tagged <- data$Description %>%
  
  #Format f√ºr das udpipe Paket anpassen
  as_tibble() %>%
  mutate(doc_id = paste0("text", 1:n())) %>% 
  rename(text = value) %>%

 #Part-of-speech tagging
  udpipe("english") %>% 
  
  #Wir reduzieren die Ausgabe auf relevante Variablen (z.B. Text-ID, Tag)
  select(doc_id, sentence_id, token_id, token, lemma, upos, head_token_id)

#Wir schauen uns die Ausgabe an
head(data_pos_tagged)
```

Wie kann man PoS Tagging jetzt f√ºr wissenschaftliche Analysen nutzen?

Wir k√∂nnen auf Basis der PoS Tags z. B. analysieren, mit welchen Adjektiven das Wort *family* beschrieben wird.

Daf√ºr...

-   Filtern wir mit `filter()` den Datensatz nach Substantiven mit dem Lemma *Family*
-   Suchen mit `inner_join()` S√§tze mit diesem Feature (gleiche `doc_id`, gleiche `sentence_id` des Features) im Datensatz
-   Reduzieren wir mit `filter()` den Datensatz auf Adjektive, die zum Feature *Family* geh√∂ren (identifiziert via `head_token`)
-   Und versch√∂nern mit `rename()`, `select()` und `head()` die Ausgabe so, dass wir nur ausgew√§hlte Ergebnisse erhalten.

Und das geht so:

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
data_pos_tagged %>%
  
  #Wir filtern den Datensatz nach dem Substantiv "Family"
  filter(upos == "NOUN" & lemma == "family") %>%
  
  #F√ºr alle gefundenen F√§lle suchen wir die zugeh√∂rigen S√§tze im "vollen" Datensatz
  #Das Matching geschieht via doc_id (ID des Dokuments) und sentence_ic (ID des Satzes im Dokument)
  inner_join(data_pos_tagged, by = c("doc_id", "sentence_id")) %>%
  
  #Wir behalten mit filter nur Adjektive, die sich auf Familie beziehen
  #N√§mlich solche, die bei "head_token" die "token_id" des Features "Family" haben
  filter(upos.y == "ADJ" & head_token_id.y == token_id.x) %>%
  
  #Wir benennen manche Variablen um, damit das Ganze besser verst√§ndlich ist
  rename(token_id = token_id.y,
         token = token.y) %>%
  
  #Wir w√§hlen nur relevante Variablen aus
  select(doc_id, sentence_id, token_id, token) %>%
  
  #erste Zeilen ausgeben
  head()
```

# 4. Dependency Parsing

Als letzte Analysem√∂glichkeit schauen wir uns Dependency Parsing an. Dependency Parsing beschreibt die Abh√§ngigkeit von Features innerhalb von S√§tzen (z.B. welche Adjektive sich auf welche Substantive beziehen, etc.).

F√ºr Dependency Parsing k√∂nnen wir wieder die Funktion `udpipe()` aus dem Paket `udpipe` nutzen. Wir bereiten die Daten √§hnlich vor wie f√ºr das PoS Tagging. Der einzige Unterschied ist, dass wir uns nun mit `select()` zus√§tzlich `dep_rel` als Art der syntaktischen Beziehung, die das jeweilige Feature zu `head_token` hat, ausgeben lassen.

```{r eval = TRUE}
data$Description %>%
  
  #Format f√ºr das udpipe Paket anpassen
  as_tibble() %>%
  mutate(doc_id = paste0("text", 1:n())) %>% 
  rename(text = value) %>%
  
  #Der Einfachheit halber machen wir diese Analyse nur f√ºr einen Text
  slice(1) %>%
  
  #dependency parsing
  udpipe("english") %>% 
  
  #relevanten Variablen ausw√§hlen
  select(doc_id, sentence_id, token_id, token, head_token_id, dep_rel) %>%
  
  #erste Zeilen ausgeben
  head(5)
```

Wer sich nun fragt, was diese Abk√ºrzungen bedeuten, kann [hier](https://universaldependencies.org/u/dep/index.html) mehr Infos erhalten.

Zus√§tzlich lassen sich solche Dependency Relations auch visualisieren, etwa via dem `rsyntax`-Paket. Probieren wir dies f√ºr einen einfacheren, k√ºrzeren Satz:

```{r echo = TRUE, eval = FALSE, message = FALSE, warning = FALSE, error = FALSE}

#Beispielsatz in udpipe
udpipe("My only goal in life is to understand dependency parsing", "english") %>%
  
  #Umwandlung in Format f√ºr rsyntax-Paket
  as_tokenindex() %>%
  
  #Visualisierung
  plot_tree(., token, lemma, upos)
```

![Beispiel f√ºr Dependency Parsing](images/dependency_tree.PNG){fig-alt="Beispiel f√ºr Dependency Parsing" fig-align="left"}

### Aufgabe 1 üìå

Die folgende √úbung fasst alles zusammen, was wir bisher gelernt haben: Preprocessing und Co-Occurrence Analysen.

Bitte arbeitet f√ºr die √úbung mit dem Horoskop-Datensatz (Download der CSV-Datei entweder via der Webseite oder Einlesen via [dieser](https://raw.githubusercontent.com/valeriehase/textasdata-ms/main/data/data_horoscope.csv) URL)

#### Aufgabe 1.1 üìå

Lest den Datensatz ein und verschafft euch einen √úberblick √ºber die Daten. Welche Variablen sind dort vorhanden?

```{r echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#Daten einlesen
url <- getURL("https://raw.githubusercontent.com/valeriehase/textasdata-ms/main/data/data_horoscope.csv")
horoscope_df <- read.csv2(text = url) 

#√úberblick √ºber Datensatz verschaffen
#str(horoscope_df)
```

#### Aufgabe 1.2 üìå

Bereitet den Datensatz durch Preprocessing und das Umwandeln in eine DFM f√ºr die Analyse vor. Hinterfragt kritisch, welche Bereinigung- und Normalisierungsschritte ihr tats√§chlich braucht.
```{r echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, error = FALSE}

#Normalisierung
horoscope_tokens <- horoscope_df$Horoscope %>% 
  tokens(what = "word",
         remove_punct = TRUE, 
         remove_numbers = TRUE, 
         remove_symbols = TRUE) %>% 
  tokens_tolower() %>% 
  tokens_remove(stopwords("english")) %>% 
  tokens_wordstem()

#Wie sehen die Features aus?
#horoscope_tokens[1]
```
```{r echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#Umwandlung in eine DFM
horoscope_dfm <- horoscope_tokens %>% 
  dfm() %>% 
  dfm_trim( min_docfreq = 0.005, 
            max_docfreq = 0.99, 
            docfreq_type = "prop", 
            verbose = TRUE) 

#head(horoscope_dfm)
```
#### Aufgabe 1.3 üìå
Schaut euch als erste Analyse an, welcher Ausdr√ºck h√§ufiger vorkommt: "secret fear" oder "in love"?
```{r echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#L√∂sung mit regul√§ren Ausdr√ºcken
#horoscope_df %>%
#  
#  #Ausz√§hlen
#  summarize("H√§ufigkeit: in love" = sum(grepl("in love", tolower(Horoscope))),
#            "H√§ufigkeit: secret secret" = sum(grepl("secret fear",  tolower(Horoscope))))
```
#### Aufgabe 1.4 üìå
Jetzt wollen wir wissen, bei welchem Sternzeichen es am mysteri√∂sesten wird: Bei welchem Sternzeichen f√§llt am h√§ufigsten das Stichwort "secret"?
```{r echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#Version 1: Mit regul√§ren Ausdr√ºcken
#horoscope_df %>%
#  
#  #Gruppierung nach Sternzeichen
#  group_by(Signs) %>%
#  
#  #Ausz√§hlen
#  summarize(Geheimnis = sum(grepl("secret", Horoscope))) %>%
#  
#  #Absteigend sortieren
#  arrange(desc(Geheimnis))
```
```{r echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#Version 2: Mit Quanteda

#Wir analysieren die H√§ufigkeit des Features gruppiert nach Sternzeichen
#textstat_frequency(horoscope_dfm, groups = horoscope_df$Signs) %>%
#  
#  #Wir filtern nur f√ºr das Feature "secret"
#  filter(feature == "secret") %>%
#  
#  #Absteigend sortieren
#  arrange(desc(docfreq))
```

#### Aufgabe 1.5 üìå
Visualisiert auf Basis eines semantischen Netzwerk, mit welchen Adjektiven die Sternzeichen "Aquarius" (Wassermann) vs. "Gemini" (Zwilling) h√§ufig in den Horoskopen assoziiert werden.

Das Netzwerk k√∂nnte etwa so aussehen:
```{r echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, error = FALSE}
horoscope_df_filtered <- horoscope_df %>%
  
  #Reduktion des Datensatzes auf ausgew√§hlte Sternzeichen
  filter(Signs %in% c("AQUARIUS", "GEMINI"))

horoscope_df_adj <- horoscope_df_filtered %>%
  
  #Reduktion auf Textvariable "Horoscope"
  select(Horoscope) %>%

  #Format f√ºr das udpipe Paket anpassen
  as_tibble() %>%
  mutate(doc_id = paste0("text", 1:n())) %>% 
  rename(text = Horoscope) %>%

 #Part-of-speech tagging, um Adjektive zu identifizieren
  udpipe("english") %>%
    
  #Wir behalten mit filter nur Adjektive 
  as_tibble() %>%
  filter(upos == "ADJ") %>%
  
  #Wir reduzieren die Ausgabe auf relevante Variablen (Text-ID, Feature - in diesem Fall Adjektive)
  select(doc_id, token) %>%
  
  #Wir erstellen einen auf Adjektive reduzierten Text je Horoskop
  group_by(doc_id) %>%
  summarize(Horoscope_adj = paste0(token, collapse = " ")) %>%
  distinct(doc_id, .keep_all = T) 

#Wir bringen den auf Adjektive reduzierten Datensatz &
#die Informationen zu den einzelnen Horoskopen zusammen
horoscope_df_filtered <- horoscope_df_filtered %>%
  
  #Wir erstellen einen ID, um die Informationen zu matchen
  mutate(doc_id = paste0("text", 1:n())) %>%
  
  #Wir f√ºgen die beiden Datens√§tze zusammen
  left_join(horoscope_df_adj) %>%
  
  #Wir f√ºgen das Sternzeichen zum jeweiligen Text hinzu, quasi als "Titel"
  mutate(Horoscope = paste0(Horoscope_adj, " ", Signs))

#Semantisches Netzwerk

#Preprocessing des Textes
tokens(horoscope_df_filtered$Horoscope,
       what = "word", #Tokenisierung, hier zu W√∂rtern als Analyseeinheit
       remove_punct = TRUE, #Entfernung von Satzzeichen
       remove_numbers = TRUE) %>% #Entfernung von Zahlen
  
  #Kleinschreibung
  tokens_tolower() %>% 
  
  #Entfernung von Stoppw√∂rtern
  tokens_remove(stopwords("english")) %>% 
  
  #Stemming
  tokens_wordstem() %>%
  
  #Wir erstellen die Feature Co-Occurrence-Matrix
  fcm(context = "document") %>%
  
  #Wir erstellen das semantische Netzwerk
  textplot_network(min_freq = 5)
```

