---
title: "Sitzung 2: Co-Occurence Analyse, Kollokationen, POS-tagging und named entity recognition"
author: "Valerie Hase & Luisa Kutlar"
date: "05.04.2024"
format:
  html:
    toc: true
    html-math-method: katex
    css: styles.css
---

#Packages laden und Objekte erstellen

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#install.packages("RCurl")
#install.packages("quanteda")
#install.packages("tidyverse)
#install.packages("dplyr")
#install.packages("quanteda.textplots")
#install.packages("quanteda.textstats")
#install.packages("udpipe")

library("RCurl")
library("quanteda")
library("tidyverse")
library("dplyr")
library("quanteda.textplots")
library("quanteda.textstats")
library("udpipe")


```

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#Daten laden
url <-  getURL("https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv")
daten_df <-  read.csv2(text = url)

#Tokens
stopwords("english")
daten_tokens <- daten_df$Description %>% 
  tokens(what = "word",
         remove_punct = TRUE, 
         remove_numbers = TRUE, 
         remove_url = TRUE, 
         remove_symbols = TRUE) %>% 
  tokens_tolower() %>% 
  tokens_remove(stopwords("english")) %>% 
  tokens_wordstem()

#dfm
daten_dfm <- daten_tokens %>% 
  dfm() %>% 
  dfm_trim( min_docfreq = 0.005, 
            max_docfreq = 0.99, 
            docfreq_type = "prop", 
            verbose = TRUE) 
```

# 3 Co-Occurrence-Analysen

Bevor wir zur Co-Occurence-Analyse kommen, gibt es auch eine Funktion, die es einem schnell und unkompliziert erm√∂glicht herauszufinden, in welchem Kontext ein Wort benutzt wird. Die hier gemeinte Funktion ist die `kwic()`Funktion aus dem quanteda Packet. Mit folgendem Code k√∂nnen wir beispielsweise herausfinden, in welchem Kontext das Wort hero vorkommt. In der Ausgabe werden jeweils die f√ºnf W√∂rter vor und nach dem Wort hero angezeigt.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens %>% 
  kwic(pattern = "hero", window = 5)
```

Nun zur Co-Occurence-Analyse. Hierf√ºr m√ºssen wir zun√§chst die dfm in eine Feature Co-occurrence Matrix (fcm) umwandeln. Das machen wir mit der Funktion `fcm()` aus dem quanteda Packet.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_fcm <- daten_dfm %>% 
  fcm()

daten_fcm %>% 
  head()
```

Beim n√§chsten Schritt w√§hlen wir die features aus, die uns in unserer Analyse interessieren. Das machen wir mit der Funktion `fcm_select()`aus dem quanteda Packet.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_fcm <- daten_fcm %>% 
  fcm_select(pattern = c("famili", "crime", "america", "school"), #hier noch bessere Begriffe vielleicht √ºber topfeatures oder topic modeling
             selection = "keep")
```

Dann visualisieren wir die fcm mit der `textplot_network()`Funktion aus dem`quanteda.textplots()` Paket. Wenn features im selben Dokument vorkommen, werden sie mit einer Linie verbunden. Umso dicker die Linie, desto √∂fter kommen die features miteinander vor.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
textplot_network(daten_fcm)
```

Diese Visualisierung gibt uns nun aber noch keine genauen Angaben dazu, wie oft ein feature mit einem anderen vorkommt. Um das herauszufinden, m√ºssen wir die fcm mit der `convert()`Funktion aus dem quanteda Packet in einen data frame umwandeln.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_fcm_df <- daten_fcm %>% 
  convert(to = "data.frame")

daten_fcm_df %>% 
  head()
```

Mit Hilfe von `select()`k√∂nnen wir uns nun einzelne H√§ufigkeiten, wie oft ein feature mit einem anderen feature vorkommt, ausgeben lassen.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_fcm_df %>%
  filter(doc_id == "crime") %>% #Zeile
  select(america) #Spalte #Frage: Wie kann es sein, dass crime america 0 ist und america crime 2?
```

# 4 Kollokationen und N-gramme

Um herauszufinden, welche tokens oft hintereinander vorkommen, k√∂nnen wir die Funktion `textstat_collocations`aus dem quanteda.textstats Packet verwenden.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens %>%
  textstat_collocations(min_count = 10) %>%
  arrange(-lambda) %>%
  head(10)
```

In einem weiteren Schritt kann es manchmal sinnvoll sein, Kollokationen f√ºr die Analyse zu einem token zusammenzufassen (dieser Schritt w√§re dann Teil des Preprocessings). Das l√§sst sich mit der `tokens_compound()`Funktion aus dem quanteda.textstats Packet umsetzen. Die Funktion verbindet die gegebenen tokens mit einem Unterstrich zu einem token.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
ngramme <- c("los angel","new york citi", "serial killer", "high school", "best friend")
daten_tokens_ngramme <- tokens_compound(daten_tokens, pattern = phrase(ngramme))
```

Wie das nun in den Daten aussieht k√∂nnen wir mit Hilfe der `kwic()`Funktion aus dem quanteda Packet herausfinden (kwic steht f√ºr keywords in context).

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens_ngramme %>% 
  kwic(pattern = c("los angel","new york citi", "serial killer", "high school", "best friend")) %>% 
  head(n=30)
```

# 5 Parts-of-speech tagging

F√ºr das parts of speech tagging nutzen wir das Packet UDPipe. Bevor wir unseren data frame in die `udipie()`Funktion geben, m√ºssen wir ihn ins tibble Format umformen und eine ID Variable erstellen. Des Weiteren bennen wir die Spalte value in text um. Das ist notwendig, weil `as_tibble()`die urspr√ºngliche Spalte `Description` in `value` umbenannt hat. Am Ende lassen wir uns nur einen Teil der Variablen ausgeben, damit das Ergebnis √ºbersichtlich bleibt.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_df_udpipe <- daten_df$Description %>%

#Format f√ºr das udpipe Packet anpassen
  as_tibble() %>%
  mutate(doc_id = paste0("text", 1:n())) %>% 
  rename(text = value) %>%

#Der Einfachheit halber nur f√ºr einen Text
  slice(1) %>%

#part-of-speech tagging
  udpipe("english") 
```

Wenn wir jetzt beispielsweise herausfinden wollen, mit welchen Adjektiven das Wort "family" beschrieben wird, sieht der Code daf√ºr wie folgt aus:

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#Dataframe nach Nomen mit dem lemma "family" filtern
adjectives_describing_family <- daten_df_udpipe %>%
  filter(upos == "NOUN" & lemma == "family") %>%

#Den gefilterten Dataframe mit dem ungefilterten Dataframe joinen
  inner_join(daten_df_udpipe, by = c("doc_id", "sentence_id")) %>%
  
#Gro√üen Dataframe nach Adjektiven filtern, die bei ihrer head_token Variable die family token_id haben
  filter(upos.y == "ADJ" & head_token_id.y == token_id.x) %>%

#relevanten Variablen ausw√§hlen
  select(doc_id, sentence_id, token_id = token_id.y, token = token.y, upos = upos.y)

print(adjectives_describing_family)
```

# 6 Named entitiy recognition

# üë©‚ÄçüíªTeste dein Wissen

Die folgende √úbung fasst alles zusammen, was wir bisher gelernt haben. Gehe Schritt f√ºr Schritt die Teilaufgaben durch und schaue wie weit du kommst. 
Den Datensatz f√ºr die √úbung findest du hier: "https://raw.githubusercontent.com/valeriehase/textasdata-ms/main/data/data_horoscope.csv"

1. Lade den Datensatz und verschaffe dir einen √úberblick √ºber die Daten. Gibt es Encoding-Issues, die bereinigt werden m√ºssen?

2. Bereinige und Normalisiere den Datensatz. Hinterfrage kritisch welche Bereinigungsschritte es tats√§chlich braucht. Am Ende solltest du eine Document-Feature-Matrix haben.

3. Bei welchem Sternzeichen f√§llt am meisten das Stichwort "winning"? 

4. Was kommt √∂fter vor: "secret fears" oder "in love"? #Notiz f√ºr uns: n-gramme

5. Finde heraus welcher der Planeten am meisten in Zusammenhang mit Ver√§nderungen und Neuanf√§ngen genannt wird. #Notiz f√ºr uns: Co-Occurence 

6. Was wird den Lesenden geraten mit ihrem Geld zu machen? #Notiz f√ºr uns: POS tagging und dann Verben zu money 



 
