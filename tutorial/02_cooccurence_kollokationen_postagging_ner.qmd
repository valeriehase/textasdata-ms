---
title: "Sitzung 2: Co-Occurence Analyse, Kollokationen, POS-tagging und named entity recognition"
author: "Valerie Hase & Luisa Kutlar"
date: "05.04.2024"
format:
  html:
    toc: true
    html-math-method: katex
    css: styles.css
---
#Packages laden und Objekte erstellen
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#install.packages("RCurl")
#install.packages("quanteda")
#install.packages("tidyverse)
#install.packages("dplyr")
#install.packages("quanteda.textplots")
#install.packages("quanteda.textstats")
#install.packages("udpipe")

library("RCurl")
library("quanteda")
library("tidyverse")
library("dplyr")
library("quanteda.textplots")
library("quanteda.textstats")
library("udpipe")


```

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#Daten laden
url <-  getURL("https://raw.githubusercontent.com/valeriehase/Salamanca-CSS-SummerSchool/main/Processing%20text%20and%20text%20as%20data/data_tvseries.csv")
daten_df <-  read.csv2(text = url)

#Tokens
stopwords("english")
daten_tokens <- daten_df$Description %>% 
  tokens(what = "word",
         remove_punct = TRUE, 
         remove_numbers = TRUE, 
         remove_url = TRUE, 
         remove_symbols = TRUE) %>% 
  tokens_tolower() %>% 
  tokens_remove(stopwords("english")) %>% 
  tokens_wordstem()

#dfm
daten_dfm <- daten_tokens %>% 
  dfm() %>% 
  dfm_trim( min_docfreq = 0.005, 
            max_docfreq = 0.99, 
            docfreq_type = "prop", 
            verbose = TRUE) 
```

# 3 Co-Occurrence-Analysen
ab hier neues Dokument
hier mit kwic() anfangen 

Für die Co-Occurence-Analyse müssen wir zunächst die dfm in eine Feature Co-occurrence Matrix (fcm) umwandeln. Das machen wir mit der Funktion `fcm()` aus dem quanteda Packet. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_fcm <- daten_dfm %>% 
  fcm()

daten_fcm %>% 
  head()
```
Beim nächsten Schritt wählen wir die features aus, die uns in unserer Analyse interessieren. Das machen wir mit der Funktion `fcm_select()`aus dem quanteda Packet.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_fcm <- daten_fcm %>% 
  fcm_select(pattern = c("famili", "crime", "america", "school"), #hier noch bessere Begriffe vielleicht über topfeatures oder topic modeling
             selection = "keep")
```

Dann visualisieren wir die fcm mit der `textplot_network()`Funktion aus dem`quanteda.textplots()` Paket. Wenn features im selben Dokument vorkommen, werden sie mit einer Linie verbunden. Umso dicker die Linie, desto öfter kommen die features miteinander vor. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
textplot_network(daten_fcm)
```
Diese Visualisierung gibt uns nun aber noch keine genauen Angaben dazu, wie oft ein feature mit einem anderen vorkommt. Um das herauszufinden, müssen wir die fcm mit der `convert()`Funktion aus dem quanteda Packet in einen data frame umwandeln. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_fcm_df <- daten_fcm %>% 
  convert(to = "data.frame")

daten_fcm_df %>% 
  head()
```

Mit Hilfe von `select()`können wir uns nun einzelne Häufigkeiten, wie oft ein feature mit einem anderen feature vorkommt, ausgeben lassen. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_fcm_df %>%
  filter(doc_id == "crime") %>% #Zeile
  select(america) #Spalte #Frage: Wie kann es sein, dass crime america 0 ist und america crime 2?
```

# 4 Kollokationen und N-gramme
Um herauszufinden, welche tokens oft hintereinander vorkommen, können wir die Funktion `textstat_collocations`aus dem quanteda.textstats Packet verwenden.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens %>%
  textstat_collocations(min_count = 10) %>%
  arrange(-lambda) %>%
  head(10)
```
In einem weiteren Schritt kann es manchmal sinnvoll sein, Kollokationen für die Analyse zu einem token zusammenzufassen (dieser Schritt wäre dann Teil des Preprocessings). Das lässt sich mit der `tokens_compound()`Funktion aus dem quanteda.textstats Packet umsetzen. Die Funktion verbindet die gegebenen tokens mit einem Unterstrich zu einem token.
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
ngramme <- c("los angel","new york citi", "serial killer", "high school", "best friend")
daten_tokens_ngramme <- tokens_compound(daten_tokens, pattern = phrase(ngramme))
```

Wie das nun in den Daten aussieht können wir mit Hilfe der `kwic()`Funktion aus dem quanteda Packet herausfinden (kwic steht für keywords in context). 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens_ngramme %>% 
  kwic(pattern = c("los angel","new york citi", "serial killer", "high school", "best friend")) %>% 
  head(n=30)
```
# 5 Part-of-speech tagging
Für das part of speech tagging nutzen wir das Packet UDPipe. Bevor wir unseren data frame in die `udipie()`Funktion geben, müssen wir ihn ins tibble Format umformen und eine ID Variable erstellen. Des Weiteren bennen wir die Spalte value in text um. Das ist notwendig, weil `as_tibble()`die ursprüngliche Spalte `Description` in `value` umbenannt hat. 
Am Ende lassen wir uns nur einen Teil der Variablen ausgeben, damit das Ergebnis übersichtlich bleibt. 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_df_udpipe <- daten_df$Description %>%

#Format für das udpipe Packet anpassen
  as_tibble() %>%
  mutate(doc_id = paste0("text", 1:n())) %>% 
  rename(text = value) %>%

#Der Einfachheit halber nur für einen Text
  slice(1) %>%

#part-of-speech tagging
  udpipe("english") 
```
Wenn wir jetzt beispielsweise herausfinden wollen, mit welchen Adjektiven das Wort "family" beschrieben wird, sieht der Code dafür wie folgt aus: 
```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#Dataframe nach Nomen mit dem lemma "family" filtern
adjectives_describing_family <- daten_df_udpipe %>%
  filter(upos == "NOUN" & lemma == "family") %>%

#Den gefilterten Dataframe mit dem ungefilterten Dataframe joinen
  inner_join(daten_df_udpipe, by = c("doc_id", "sentence_id")) %>%
  
#Großen Dataframe nach Adjektiven filtern, die bei ihrer head_token Variable die family token_id haben
  filter(upos.y == "ADJ" & head_token_id.y == token_id.x) %>%

#relevanten Variablen auswählen
  select(doc_id, sentence_id, token_id = token_id.y, token = token.y, upos = upos.y)

print(adjectives_describing_family)
```
# 6 Named entitiy recognition
 
