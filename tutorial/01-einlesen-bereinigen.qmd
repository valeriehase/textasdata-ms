---
title: "Sitzung 1: Einf√ºhrung & Preprocessing"
author: "Valerie Hase & Luisa Kutlar"
format:
  html:
    toc: true
    html-math-method: katex
    css: styles.css
---

# 1. Pakete laden und Daten einlesen

Zun√§chst installieren alle Pakete, die wir f√ºr diese Sitzung brauchten (z.B. `tidyverse`). Ihr braucht `install.packages()` nur, wenn ihr die Pakete im Methodencafe noch nicht installiert hattet.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#install.packages("tidyverse)
#install.packages("quanteda")
#install.packages("quanteda.textplots")
#install.packages("RCurl")

library("tidyverse")
library("quanteda")
library("quanteda.textplots")
library("RCurl")
```

## 1.1 Textdaten aus einer lokalen Datei einlesen

Zun√§chst k√∂nnt ihr die Text-Daten via der Tutorials-Webseite downloaden (s. "Materialien/Daten").

Anschliessen laden wir die Dateien in R. Wenn ihr via JupyterHub arbeitet, geht das via Click-and-Point. Andernfalls nutzt den `read.csv2()`-Befehl. Der Datensatz wird im Objekt `data` gespeichert.

```{r echo = TRUE, eval = FALSE, message = FALSE, error = FALSE}
data <- read.csv2("data_tvseries.csv")
```

## 1.2 Textdaten von einer URL downloaden

Oft wollen wir Dateien auch direkt von z. B. einer Webseite laden. Daf√ºr nutzen wir die Funktion `getURL()` aus dem package `Rcurl` und dann wieder die `read.csv2()`-Funktion.

```{r echo = TRUE, message = FALSE, error = FALSE}
url <-  getURL("https://raw.githubusercontent.com/valeriehase/textasdata-ms/main/data/data_tvseries.csv")
data <- read.csv2(text = url)
```

Nach dem Einlesen der Daten verschaffen wir uns einen √úberblick √ºber die Daten und kontrollieren, dass alles korrekt eingelesen wurde.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
head(data)
```

Sieht soweit gut aus!

# 2. Preprocessing

## 2.1 Bereinigung (z. B. Encoding-Probleme)

Nach dem Einlesen haben wir bereits einen Blick in den Datensatz geworfen. Nun schauen wir uns gezielt die Variable `Description` an, um zu √ºberpr√ºfen, ob der zu analysierende Text gut aussieht. Hier nutzen wir `slice()`, um uns nur den allerersten Text ausgeben zu lassen.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
data %>%
  
  # Auswahl der Variable "Description"
  select(Description) %>% 
  
  # Reduktion auf ersten Text
  slice(1)
```

Der Text sieht gut aus! Allerdings kann es, gerade bei Texten aus anderen Sprachen, zu sogenannten Encoding-Problemen kommen.

### 2.1.1 Encoding-Probleme

Schauen wir uns ein Beispiel an: deutsche Umlaute. Was passiert wenn wir hier das Encoding √§ndern?

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
# Beispiel-Satz
string <- "Sch√∂ne Gr√º√üe aus M√ºnchen"

# Encoding pr√ºfen
Encoding(string)

# Encoding testweise √§ndern
Encoding(string) <- "latin1"
string
```

Wie k√∂nnen wir Encoding-Probleme adressieren?

-   Beim Einlesen das richtige Encoding als Argument mitgeben (siehe z.B. `fileEncoding`-Argument in `read.csv2()`)
-   Mit Hilfe von regul√§ren Ausdr√ºcken bereinigen

Beim manuellen Bereinigen k√∂nnen regul√§re Ausdr√ºcke (mehr dazu [hier](https://bookdown.org/valerie_hase/TextasData_HS2021/tutorial-9-searching-manipulating-string-patterns.html)) und die Funktion `gsub()` helfen, mit der wir Zeichen ersetzen k√∂nnen. Zum Beispiel so:

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
# Mit Hilfe von regul√§ren Ausdr√ºcken bereinigen
string %>% 
  
  # Ersatz f√ºr falsches Encoding "√∂"
  gsub(pattern = "√É¬∂", replacement ="√∂") %>% 
  
  # Ersatz f√ºr falsches Encoding "√º"
  gsub(pattern = "√É¬º", replacement = "√º") %>% 
  
  # Ersatz f√ºr falsches Encoding "√ü"
  gsub(pattern = "√É≈∏", replacement = "√ü") 
```

### 2.1.2 Datenbereinigung mit regul√§ren Ausdr√ºcken

Probieren wir dies f√ºr unseren Datensatz aus. Wir wollen die Nummer, den Punkt und das Leerzeichen vor dem Titel der TV-Serie in der Variable `Title` mit `gsub()` entfernen:

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
data %>%
  select(Title) %>%
  head(5)
```

Das ginge mit folgenden Befehlen:

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}

# Entfernung der Zeichen vor dem Titel der TV-Serie
data <- data %>%
  mutate(Title = gsub("^[0-9]+[[:punct:]] ", "", Title))

# So sieht das Ergebnis aus:
data %>%
  select(Title) %>%
  head(5)
```

### 2.1.3 Datenfilterung mit regul√§ren Ausdr√ºcken

Mit regul√§ren Ausdr√ºcken k√∂nnen wir aber noch mehr machen, z.B. Daten filtern. Wir wollen nur TV-Serien behalten, die als "Drama" klassifiziert wurden.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
data %>%

  # filtern aller TV_Serien, die "Drama" in der Beschreibung beinhalten
  filter(grepl("[D|d]rama", Description)) %>%

  # Inspektion der ersten f√ºnf Titel
  select(Title) %>%
  head(5)
```

Und jetzt behalten wir solche, die als "Drama" oder "Crime" klassifiziert wurden.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
data %>%

  # filtern aller TV_Serien, die "Drama"und "Crime" in der Beschreibung beinhalten
  filter(grepl("[D|d]rama|[C|c]rime", Description)) %>%

  # Inspektion der ersten f√ºnf Titel
  select(Title) %>%
  head(5)
```

### 2.1.4 Aufgabe 1 üìå

#### Aufgabe 1.1 (Basis)

K√∂nnt ihr alle Serien identifizieren, die in Deutschland spielen?

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
data %>%
  filter(grepl("German", Description)) %>%
  head()
```

#### Aufgabe 1.2 (Fortgeschritten)
K√∂nnt ihr alle Serien identifizieren, in denen es um Superhelden geht und "*superhero/superheroes*‚Äù in der Variable `Description` mit "*fancy R programmers*‚Äú ersetzen?

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
data %>%
  filter(grepl("[S|s]uperhero[es]*", Description)) %>%
  mutate(Description = gsub("[S|s]uperhero[es]*", "fancy R programmers", Description)) %>%
  select(Description) %>%
  head(3)
```

## 2.2 Normalisierung

Die Funktionen `tokens()` bzw. entsprechende Subfunktionen von `quanteda` erm√∂glichen es uns, mehrere Preprocessing-Schritte in einer einzigen Pipe (d.h. Analyse-Pipeline auf Basis des `tidyverse`) durchzuf√ºhren:

-   Tokenisierung: `tokens()` (hier k√∂nnen Tokenisierung anwenden, aber bereits auch Sonderzeichen, wie Satzzeichen, entfernen)
-   Kleinschreibung: `tokens_tolower()`
-   Stoppw√∂rter entfernen: `tokens_remove_stopwords()`. 
-   Stemming: `tokens_wordstem()`

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
tokens <- tokens(data$Description,
                 what = "word", #Tokenisierung, hier zu W√∂rtern als Analyseeinheit
                 remove_punct = TRUE, #Entfernung von Satzzeichen
                 remove_numbers = TRUE) %>% #Entfernung von Zahlen
  
  # Kleinschreibung
  tokens_tolower() %>% 
  
  # Entfernung von Stoppw√∂rtern
  tokens_remove(stopwords("english")) %>% 
  
  # Stemming
  tokens_wordstem()


# So sah unser erster Text vor dem Preprocessing aus
data$Description[1]

# Und so danach
tokens[1]
```

### 2.2.1 Entfernung von Stoppw√∂rtern
Es gibt verschiedene M√∂glichkeiten, Stoppw√∂rter zu entfernen. Am einfachsten ist dies mithilfe der im `quanteda`-Paket integrierten Stoppwortlisten m√∂glich. Diese sind in mehreren Sprachen verf√ºgbar, darunter auch Deutsch. 

Je nach Forschungsfrage k√∂nnen diese Stoppwortlisten angepasst werden, indem eigene Stoppw√∂rter definiert und dann entfernt werden. Es ist aber auch m√∂glich, eigene Stoppwortlisten zu erstellen.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
# W√∂rter aus der quanteda Stoppwortliste entfernen
stoppw√∂rter <- stopwords("english")
stoppw√∂rter <- stoppw√∂rter[!stoppw√∂rter %in% c("i", "me")]

# Beispielhafte Anwendung 
tokens(data$Description,
                 what = "word", #Tokenisierung, hier zu W√∂rtern als Analyseeinheit
                 remove_punct = TRUE, #Entfernung von Satzzeichen
                 remove_numbers = TRUE) %>% #Entfernung von Zahlen
  
  # Kleinschreibung
  tokens_tolower() %>% 
  
  # Entfernung von Stoppw√∂rtern - hier z.B. reduzierte quanteda-Liste
  tokens_remove(stoppw√∂rter) %>% 
  
  # Stemming
  tokens_wordstem() %>%
  
  # Ausgabe des ersten Textes
  head(1)
```

### 2.2.2 Aufgabe 2 üìå

#### 2.2.2 Aufgabe 2.1 (Basis)
K√∂nnt ihr eine Liste mit 3-5 Stopw√∂rtern erstellen und diese als Teil des Preprocessings zus√§tzlich entfernen?

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
# Eigene Stopwortliste definieren
unique_stopwords = c("one", "two", "three", "four", "five")

# als Teil des Preprocessings aufnehmen
tokens <- tokens(data$Description,
                 what = "word", #Tokenisierung, hier zu W√∂rtern als Analyseeinheit
                 remove_punct = TRUE, #Entfernung von Satzzeichen
                 remove_numbers = TRUE) %>% #Entfernung von Zahlen
  
  # Kleinschreibung
  tokens_tolower() %>% 
  
  # Entfernung von Stoppw√∂rtern
  tokens_remove(stopwords("english")) %>% 
  
  # Entfernung der eigenen Stopw√∂rter
  tokens_remove(unique_stopwords) %>%
  
  # Stemming
  tokens_wordstem()
```

#### 2.2.2 Aufgabe 2.1 (Fortgeschritten)
K√∂nnt ihr daf√ºr sorgen, dass Namen von St√§dten (hier als Beispiel ‚ÄûNew York‚Äú) als ein einzelnes Feature beibehalten werden?

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
tokens <- tokens(data$Description,
                 what = "word", #Tokenisierung, hier zu W√∂rtern als Analyseeinheit
                 remove_punct = TRUE, #Entfernung von Satzzeichen
                 remove_numbers = TRUE) %>% #Entfernung von Zahlen
  
  # Kleinschreibung
  tokens_tolower() %>% 
  
  # Entfernung von Stoppw√∂rtern
  tokens_remove(stopwords("english")) %>% 
  
  # zus√§tzlicher Schritt, um New York als einen Begriff beizubehalten
  tokens_compound(pattern = phrase(c("new york*"))) %>%
  
  # Stemming
  tokens_wordstem()

# Beispieltext
tokens[81]
```

# 3. Text-as-Data-Repr√§sentation

## 3.1 Erstellung einer DFM

Um aus unseren tokens eine Document-Feature-Matrix zu machen, damit der Computer "Text-as-Data", d.h. als numerisches Datenformat, verarbeiten kann, nutzen wir die `dfm()`Funktion aus dem `quanteda`-Paket.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
# Wir erstellen eine Document-Feature matrix
dfm <- tokens %>%
  dfm()

# So sieht das Ergebnis aus
dfm
```

Wir sehen:

- Die DFM besteht aus 900 Dokumenten.
- Die DFM hat nach dem Preprocessing immer noch 4,246 individuelle Features, hier W√∂rter.
- Die DFM ist 99.66% "spare", d.h. mehr als 99% der Zellen enthalten eine 0, weil viele Features nur sehr selten vorkommen. 

## 3.2 Zus√§tzliche Normalisierung: Relative Pruning
Im letzten Schritt des Preprocessings entfernen wir h√§ufig und selten vorkommende Features aus der DFM. Das geht mit der Funktion `dfm_trim()`aus dem `quanteda`-Paket.

Es k√∂nnen unterschiedliche Grenzwerte gesetzt werden. Hier behalten wir nur Features, die in mindestens in 0.5% und h√∂chstens in 99% der Dokumente vorkommen. Das Argument `docfreq_type = "prop"`berechnet den Anteil der Dokumente, die ein bestimmtes Feature beinhalten relativ zur Gesamtzahl der Dokumente. `verbose = TRUE`printed w√§hrend der Ausf√ºhrung der Funktion Informationen √ºber den Rechenvorgang in die Konsole.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
# Anzahl Features vor relative pruning
dfm

# Anwendung des relative pruning
dfm  <- dfm  %>% 
  dfm_trim( min_docfreq = 0.005, 
            max_docfreq = 0.99, 
            docfreq_type = "prop", 
            verbose = TRUE) 

# Anzahl Features nach relative pruning
dfm
```
Wir sehen: Relative pruning kann unseren Datensatz noch einmal deutlich verkleinern (und damit z.B. Analysen beschleunigen): Wir haben nun 605 anstelle von 4,246 Features!

# 4. Erste Analysen

## 4.1 Top Features

F√ºr einen ersten Einblick in die Daten lassen wir uns mit der `topfeatures()`-Funktion aus dem `quanteda`-Packet die zehn am h√§ufigsten vorkommenden Features ausgeben.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
topfeatures(dfm, 10) %>%
  
  # Umwandlung in einen "sch√∂neren" Dataframe mit der Spalte "H√§ufigkeit"
  as.data.frame() %>%
  rename("H√§ufigkeit" = '.')
```

## 4.2. Die ber√ºhmt-ber√ºchtigte Word Cloud
Das Ergebnis k√∂nnen wir mit einer Word Cloud visualisieren. Hierf√ºr nutzen wir die `textplot_wordcloud()`Funktion aus dem `quanteda.textplots`-Paket. Dabei werden besonders "h√§ufige" Features gr√∂sser dargestellt.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
textplot_wordcloud(dfm, max_words = 100)
```
