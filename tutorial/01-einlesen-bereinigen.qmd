---
title: "Sitzung 1: Daten einlesen und bereinigen"
author: "Valerie Hase & Luisa Kutlar"
format:
  html:
    toc: true
    html-math-method: katex
    css: styles.css
---

# 1 Pakete laden und Daten einlesen

Zun√§chst installieren alle Packete, die wir f√ºr diese Sitzung brauchten (z.B. *tidyverse*). Ihr braucht *install.packages()* nur, wenn ihr die Pakete im Methodencafe noch nicht installiert hattet-

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#install.packages("tidyverse)
#install.packages("quanteda")
#install.packages("quanteda.textplots")
#install.packages("RCurl")

library("tidyverse")
library("quanteda")
library("quanteda.textplots")
library("RCurl")
```

## 1.1 Textdaten aus einer lokalen Datei einlesen

Zun√§chst k√∂nnt ihr die Text-Daten via diesem Link auf der Webseite downloaden:

![](images/clipboard-3533778068.png)

Anschliessen laden wir die Dateien in R. Wenn ihr via JupyterHub arbeitet, geht das via Click-and-Point. Andernfalls nutzt den `read.csv()`-Befehl. Der Datensatz wird im Objekt `data` gespeichert.

```{r echo = TRUE, eval = FALSE, message = FALSE, error = FALSE}
data <- read.csv2("data_tvseries.csv")
```

## 1.2 Textdaten von einer URL downloaden

Oft wollen wir Dateien auch direkt von z. B. einer Webseite laden. Daf√ºr nutzen wir die Funktion `getURL()` aus dem package `Rcurl` und dann wieder die `read.csv2()`-Funktion.

```{r echo = TRUE, message = FALSE, error = FALSE}
library("RCurl")
url <-  getURL("https://raw.githubusercontent.com/valeriehase/textasdata-ms/main/data/data_tvseries.csv")
data <- read.csv2(text = url)
```

Nach dem Einlesen der Daten verschaffen wir uns einen √úberblick √ºber die Daten und kontrollieren, dass alles korrekt eingelesen wurde.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
head(data)
```

# 2 Preprocessing

## 2.1 Bereinigung (z. B. Encoding-Probleme)

Nach dem Einlesen haben wir bereits einen Blick in den Datensatz geworfen. Nun schauen wir uns gezielt die Variable `Description` an, um zu √ºberpr√ºfen, ob der zu analysierende Text gut aussieht.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
data %>%
  select(Description) %>% 
  slice(1)
```

Der Text sieht gut aus! Allerdings kann es, gerade bei Texten aus anderen Sprachen, zu sogenannten Encoding Issues kommen.

Schauen wir uns ein Beispiel an: deutsche Umlaute. Was passiert wenn wir hier das Encoding √§ndern?

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#Beispiel-Satz
string <- "Sch√∂ne Gr√º√üe aus M√ºnchen"

#Encoding pr√ºfen
Encoding(string)

#Encoding testweise √§ndern
Encoding(string) <- "latin1"
string
```

Wie k√∂nnten wir Encoding-Probleme adressieren?

::: incremental
-   Beim Einlesen das richtige Encoding als Argument mitgeben (siehe z.B. `fileEncoding`-Argument in `read.csv2()`)
-   Mit Hilfe von regul√§ren Ausdr√ºcken bereinigen
:::

Beim manuellen Bereinigen kann die Funktion `gsub()` helfen, die Zeichenketten ersetzen kann. Zum Beispiel so:

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#Mit Hilfe von regul√§ren Ausdr√ºcken bereinigen
string_bereinigt <- string %>% 
  gsub(pattern = "√É¬∂", replacement ="√∂") %>% 
  gsub(pattern = "√É¬º", replacement = "√º") %>% 
  gsub(pattern = "√É≈∏", replacement = "√ü") 
string_bereinigt
```

Probieren wir das Ganze am Datensatz aus.

Wir wollen die Nummer, den Punkt und das Leerzeichen vor dem Titel der Tv-Serie in der Variable `Title` mit `gsub()` entfernen:

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
data %>%
  head()
```

Das ginge mit folgenden Befehlen:

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
data <- data %>%
  mutate(Title = gsub("^[0-9]+[[:punct:]] ", "", Title))

#So sieht das Ergebnis aus:
data %>%
  head(5)
```

Probieren wir das weiter aus:

Wir wollen nur TV-Serien behalten, die als "Drama" klassifiziert wurden.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
data %>%

  # filtern aller TV_Serien, die "Drama" in der Beschreibung beinhalten
  filter(grepl("[D|d]rama", Description)) %>%

  # Inspektion der ersten f√ºnf Zeilen
  head(5)
```

Und jetzt behalten wir solche, die als "Drama" oder "Crime" klassifiziert wurden.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
data %>%

  # filtern aller TV_Serien, die "Drama" in der Beschreibung beinhalten
  filter(grepl("[D|d]rama|[C|c]rime", Description)) %>%

  # Inspektion der ersten f√ºnf Zeilen
  head(5)
```

## üìåAufgabe 1

K√∂nnt ihr...

-   **Basis**: Alle Serien identifizieren, die in Deutschland spielen?

-   **Fortgeschritten**: Alle Serien identifizieren, in denen es um Superhelden geht und "*superhero/superheroes*‚Äù in der Variable `Description` mit "*fancy R programmers*‚Äú ersetzen?

## 2.2 Tokenisierung & Zahlen, URLs, etc. entfernen

## 2.2 Tokenisierung & Zahlen, URLs, etc. entfernen

Die Funktion `tokens()`von quanteda erm√∂glicht es uns bei der Aufteilung von Text in tokens direkt bestimmte Zeichen zu entfernen. Hier entfernen wir Punkte, Zahlen, URLs und Symbole.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens <- tokens(data$Description, what = "word", remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_symbols = TRUE) #wollen wir das alles entfernen?

daten_tokens %>% 
  head(n=3)
```

## 2.3 Anpassung auf Kleinschreibung

Mit der Funktion `tokens_tolower()`aus dem quanteda Packet k√∂nnen alle Buchstaben in Kleinbuchstaben umgeformt werden.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens <- tokens_tolower(daten_tokens)

daten_tokens %>% 
  head(n=3)
```

## 2.4 Stoppw√∂rter entfernen

Es gibt verschiedene M√∂glichkeiten, Stoppw√∂rter zu entfernen. Am einfachsten ist dies mithilfe der in quanteda integrierten Stoppwortlisten m√∂glich. Diese sind in mehreren Sprachen verf√ºgbar, darunter auch Deutsch.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
stopwords("english")
daten_tokens <- tokens_remove(daten_tokens, stopwords("english"))
```

Je nach Forschungsfrage k√∂nnen Stoppwortlisten angepasst werden, indem W√∂rter entfernt oder hinzugef√ºgt werden. Es ist aber auch m√∂glich eine eigene Liste zu erstellen.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#W√∂rter aus der quanteda Stoppwortliste entfernen
stoppw√∂rter <- stopwords("english")
stoppw√∂rter <- stoppw√∂rter[!stoppw√∂rter %in% c("i", "me")]

#W√∂rter der quanteda Stoppwortliste hinzuf√ºgen
stoppw√∂rter <- c(stoppw√∂rter, "i", "me")

#Eigene Liste erstellen
eigene_stoppw√∂rter <- c("hier", "eigene", "stoppw√∂rter")
```

## 2.5 Vereinheitlichung

Oft gibt es W√∂rter, die unterschiedliche Abk√ºrzungen oder Schreibweisen haben. Nehmen wir das Beispiel der Europ√§ischen Union, die auch mit EU oder E.U. abgek√ºrzt wird. Mit Hilfe der Funktion `gsub()` k√∂nnen wir strings mit anderen strings ersetzen.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
string <- "Bei den EU Wahlen k√∂nnen alle B√ºrger*innen der Europ√§ischen Union w√§hlen gehen."
string <- gsub("Europ√§ischen Union", "EU", string)
print(string)
```

## 2.6 Stemming

Mit der Funktion `tokens_wordstem()`aus quanteda reduzieren wir alle tokens auf ihren Wortstamm.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
daten_tokens <- daten_tokens %>% 
  tokens_wordstem() 

daten_tokens %>% 
  head(n=3)
```

## 2.7 Document-Feature-Matrix

Um aus unseren tokens eine dfm zu machen nutzen wir die `dfm()`Funktion aus dem quanteda package.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
datam <- daten_tokens %>% 
  dfm()
```

## 2.8 Seltene/h√§ufige features entfernen

Im letzten Schritt des Preprocessings entfernen wir h√§ufig und selten vorkommende features aus der dfm. Das geht mit der Funktion `dfm_trim()`aus dem quanteda Packet.

Es k√∂nnen unterschiedliche thresholds gesetzt werden - hier lassen wir nur features in der dfm die mindestens in 0.5% und h√∂chstens in 99% der Dokumente vorkommen. Das Argument `docfreq_type = "prop"`berechnet den Anteil der Dokumente, die ein bestimmtes feature beinhalten relativ zur Gesamtzahl der Dokumente. `verbose = TRUE`printed w√§hrend der Ausf√ºhrung der Funktion Informationen √ºber den Rechenvorgang in die Konsole.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
datam <- datam %>% 
  dfm_trim( min_docfreq = 0.005, 
            max_docfreq = 0.99, 
            docfreq_type = "prop", 
            verbose = TRUE) 
```

## 2.9 Word cloud: Erster Blick in die Daten

F√ºr einen ersten Einblick in die Daten lassen wir uns mit der `topfeatures()`Funktion aus dem quanteda Packet die 10 am h√§ufigsten vorkommenden features ausgeben.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
datam %>% 
  topfeatures(n = 10)
```

Das Ergebnis k√∂nnen wir mit einer word cloud visualisieren. Hierf√ºr nutzen wir die `textplot_wordcloud()`Funktion aus dem quanteda.textplots Packet.

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
word_cloud <- datam %>% 
  textplot_wordcloud(max_words = 100)
```

√úbung: mit emoji einleiten "test your knowledge" mit anderem Datensatz, der nicht zu gro√ü ist
